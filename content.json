[{"title":"AutoJs+mitmproxy App爬虫","date":"2020-05-19T12:39:34.075Z","path":"2020/05/19/AutoJs-mitmproxy-App爬虫/","text":"1这是崔斯特的第一百二十一篇原创文章 App爬虫 (๑• . •๑) #AutoJs 介绍官方文档：https://hyb1996.github.io/AutoJs-Docs/#/ Github：https://github.com/hyb1996/Auto.js 官方论坛：https://www.autojs.org/ 非官方文档：https://easydoc.xyz/doc/25791054/uw2FUUiw/3bEzXb4y AutoJs是一款不需要Root权限的JavaScript自动化软件，简单来说就是手机上的“脚本精灵”。 现在官方已经不在提供下载途径了，具体原因可以去这里参观：貌似Auto.js在酷安中已经被下架了 非官方文档中有下载地址 使用推荐使用AutoJsPro，本教程基于AutoJsPro 下载安装之后，需要打开几个权限： 无障碍 悬浮窗 远程调试 环境配置参考：https://easydoc.xyz/doc/25791054/uw2FUUiw/F30Lmx8U 远程链接参考：Auto.js Pro如何连接VS Code插件 基础操作打开应用12345678//启动APPif (currentPackage() != \"com.chaoxing.mobile\") &#123; toast(\"即将打开超星！\"); //直接打开学习通 app.launchApp(\"学习通\");&#125; else &#123; toast(\"已经在学习通中，即将开始进行下一步操作！\");&#125;; 获取当前activity方法，支付为例，com.zhihu.android就是currentPackage 1234567➜ ~ adb shell dumpsys window | grep mCurrent mCurrentAppOrientation=SCREEN_ORIENTATION_PORTRAIT mCurrentRotation=ROTATION_0 mCurrentUserId=0 mCurrentUserId=0 mCurrent=[0,72][1080,1920] mCurrentFocus=Window&#123;38aea79 u0 com.zhihu.android/com.zhihu.android.app.ui.activity.MainActivity&#125; 点击按钮打开悬浮窗的第三个按钮 比如我们现在点击发现按钮，会有弹窗，点击生成代码，如下图 就有代码了，可以直接拷贝到电脑中，所以如果你是在手机上编辑，会要命的。一定要把VSCode环境安装好。 如果你发现某个按钮是可以找到的，但是执行了id(&quot;hello&quot;).findOne().click()后，并没有点击动作，你可以实时这种方法，需要安卓7以上才行 如果一个控件本身无法通过click()点击，那么我们可以利用bounds()函数获取其坐标，再利用坐标点击。例如： 123var b = desc(\"打开侧拉菜单\").findOne().bounds();click(b.centerX(), b.centerY());// 如果使用root权限，则用 Tap(b.centerX(), b.centerY()); 有一些APP的组件，它们的id、text、desc等节点值是空值。className值又不是页面上唯一的。还有的组件id是动态变化的。可以尝试下面几种方法： 寻找父组件。如果该组件的父组件，或者爷组件存在固定的id、text、desc，就先定位到父组件，然后使用find()函数遍历父组件的子组件。最后通过className、depth、组件的索引等来判断是否为指定的组件。 寻找子组件。如果该组件的子组件，或者孙组件存在固定的id、text、desc，就先定位到子组件，然后使用parent()函数获取父组件对象。最后通过className、depth、组件的索引等来判断是否为指定的组件。 寻找兄弟组件。如果该组件的兄弟组件存在固定的id、text、desc，就先定位到兄弟组件，然后使用parent()函数获取父组件对象。接下来使用find()函数遍历父组件的子组件。最后通过className、depth、组件的索引等来判断是否为指定的组件。 mitmproxy使用AutoJs自动化操作后，是可以让手机自动化了，但是怎么抓取数据呢，这里使用mitmproxy是最快的。 环境配置安装：pip3 install mitmproxy 终端输入 mitmproxy后 12345➜ ~ cd ~/.mitmproxy➜ .mitmproxy lsconfig.yaml mitmproxy-ca-cert.pem mitmproxy-dhparam.pemmitmproxy-ca-cert.cer mitmproxy-ca.p12mitmproxy-ca-cert.p12 mitmproxy-ca.pem 可以在如下位置找到证书，然后安装。 手机在wifi处配置完成后打开mitm.it，下载安装信任证书即可。 数据获取编写脚本script.py 12345678910import jsonimport osfrom mitmproxy.http import flowdef response(flow: flow): print(flow.request.url, flow.response.text)if __name__ == &apos;__main__&apos;: os.system(&quot;mitmdump -s script.py&quot;) 安卓7问题出于网络安全的角度考虑，默认情况下，面向 Android 7.0 的应用开始只信任系统提供的证书(system)，且不再信任用户添加的证书(user)。 为了解决这个问题，需要Xposed+JustTrustMe去解决，否则网络不同。所以手机必须root。 详细教程，参考已root的安卓+XPosed+JustTrustMe破解ssl pinning","tags":[]},{"title":"数组知识","date":"2020-04-19T13:52:56.166Z","path":"2020/04/19/数组知识/","text":"这是崔斯特的第一百二十一篇原创文章 努力、奋斗 数组看起来简单基础，但是很多人没有理解这个数据结构的精髓。带着为什么数组要从0开始编号，而不是从1开始的问题，进入主题。 数组如何实现随机访问 数组是一种线性数据结构，用连续的存储空间存储相同类型数据 线性表：数组、链表、队列、栈；非线性表：树、图 连续的内存空间、相同的数据，所以数组可以随机访问，但对数组进行删除插入，为了保证数组的连续性，就要做大量的数据搬移工作 数组如何实现下标随机访问。引入数组再内存种的分配图，得出寻址公式 纠正数组和链表的错误认识。数组的查找操作时间复杂度并不是O(1)。即便是排好的数组，用二分查找，时间复杂度也是O（logn）。正确表述：数组支持随机访问，根据下标随机访问的时间复杂度为O（1） 低效的插入和删除 插入：从最好O(1) 最坏O(n) 平均O(n) 插入：数组若无序，插入新的元素时，可以将第K个位置元素移动到数组末尾，把心的元素，插入到第k个位置，此处复杂度为O(1)。作者举例说明 删除：从最好O(1) 最坏O(n) 平均O(n) 多次删除集中在一起，提高删除效率 记录下已经被删除的数据，每次的删除操作并不是搬移数据，只是记录数据已经被删除，当数组没有更多的存储空间时，再触发一次真正的删除操作。即JVM标记清除垃圾回收算法。 警惕数组的访问越界问题用C语言循环越界访问的例子说明访问越界的bug。此例在《C陷阱与缺陷》出现过，很惭愧，看过但是现在也只有一丢丢印象。翻了下书，替作者加上一句 话：如果用来编译这段程序的编译器按照内存地址递减的方式给变量分配内存，那么内存中的i将会被置为0，则为死循环永远出不去。 容器能否完全替代数组相比于数字，java中的ArrayList封装了数组的很多操作，并支持动态扩容。一旦超过村塾容量，扩容时比较耗内存，因为涉及到内存申请和数据搬移。 数组适合的场景： Java ArrayList 的使用涉及装箱拆箱，有一定的性能损耗，如果特别管柱性能，可以考虑数组 若数据大小事先已知，并且涉及的数据操作非常简单，可以使用数组 表示多维数组时，数组往往更加直观 业务开发容器即可；底层开发，如网络框架，性能优化，选择数组。 代码实现数组","tags":[]},{"title":"终于有人把“TCC分布式事务”实现原理讲明白了","date":"2020-04-14T02:59:55.852Z","path":"2020/04/14/终于有人把“TCC分布式事务”实现原理讲明白了/","text":"这是崔斯特的第一百二十篇原创文章 转载一篇非常优秀的博文 之前网上看到很多写分布式事务的文章，不过大多都是将分布式事务各种技术方案简单介绍一下。很多朋友看了还是不知道分布式事务到底怎么回事，在项目里到底如何使用。 所以这篇文章，就用大白话+手工绘图，并结合一个电商系统的案例实践，来给大家讲清楚到底什么是 TCC 分布式事务。 首先说一下，这里可能会牵扯到一些 Spring Cloud 的原理，如果有不太清楚的同学，可以参考之前的文章：《拜托，面试请不要再问我Spring Cloud底层原理！》。 业务场景介绍咱们先来看看业务场景，假设你现在有一个电商系统，里面有一个支付订单的场景。 那对一个订单支付之后，我们需要做下面的步骤： 更改订单的状态为“已支付” 扣减商品库存 给会员增加积分 创建销售出库单通知仓库发货 这是一系列比较真实的步骤，无论大家有没有做过电商系统，应该都能理解。 进一步思考好，业务场景有了，现在我们要更进一步，实现一个 TCC 分布式事务的效果。 什么意思呢？也就是说: 订单服务-修改订单状态， 库存服务-扣减库存， 积分服务-增加积分， 仓储服务-创建销售出库单。 上述这几个步骤，要么一起成功，要么一起失败，必须是一个整体性的事务。 举个例子，现在订单的状态都修改为“已支付”了，结果库存服务扣减库存失败。那个商品的库存原来是 100 件，现在卖掉了 2 件，本来应该是 98 件了。 结果呢？由于库存服务操作数据库异常，导致库存数量还是 100。这不是在坑人么，当然不能允许这种情况发生了！ 但是如果你不用 TCC 分布式事务方案的话，就用个 Spring Cloud 开发这么一个微服务系统，很有可能会干出这种事儿来。 我们来看看下面的这个图，直观的表达了上述的过程： 所以说，我们有必要使用 TCC 分布式事务机制来保证各个服务形成一个整体性的事务。 上面那几个步骤，要么全部成功，如果任何一个服务的操作失败了，就全部一起回滚，撤销已经完成的操作。 比如说库存服务要是扣减库存失败了，那么订单服务就得撤销那个修改订单状态的操作，然后得停止执行增加积分和通知出库两个操作。 说了那么多，老规矩，给大家上一张图，大伙儿顺着图来直观的感受一下： 落地实现 TCC 分布式事务那么现在到底要如何来实现一个 TCC 分布式事务，使得各个服务，要么一起成功？要么一起失败呢？ 大家稍安勿躁，我们这就来一步一步的分析一下。咱们就以一个 Spring Cloud 开发系统作为背景来解释。 TCC 实现阶段一：Try首先，订单服务那儿，它的代码大致来说应该是这样子的： 1234567891011121314151617181920212223242526272829public class OrderService &#123; // 库存服务 @Autowired private InventoryService inventoryService; // 积分服务 @Autowired private CreditService creditService; // 仓储服务 @Autowired private WmsService wmsService; // 对这个订单完成支付 public void pay()&#123; //对本地的的订单数据库修改订单状态为\"已支付\" orderDAO.updateStatus(OrderStatus.PAYED); //调用库存服务扣减库存 inventoryService.reduceStock(); //调用积分服务增加积分 creditService.addCredit(); //调用仓储服务通知发货 wmsService.saleDelivery(); &#125;&#125; 如果你之前看过 Spring Cloud 架构原理那篇文章，同时对 Spring Cloud 有一定的了解的话，应该是可以理解上面那段代码的。 其实就是订单服务完成本地数据库操作之后，通过 Spring Cloud 的 Feign 来调用其他的各个服务罢了。 但是光是凭借这段代码，是不足以实现 TCC 分布式事务的啊？！兄弟们，别着急，我们对这个订单服务修改点儿代码好不好。 首先，上面那个订单服务先把自己的状态修改为：OrderStatus.UPDATING。 这是啥意思呢？也就是说，在 pay() 那个方法里，你别直接把订单状态修改为已支付啊！你先把订单状态修改为 UPDATING，也就是修改中的意思。 这个状态是个没有任何含义的这么一个状态，代表有人正在修改这个状态罢了。 然后呢，库存服务直接提供的那个 reduceStock() 接口里，也别直接扣减库存啊，你可以是冻结掉库存。 举个例子，本来你的库存数量是 100，你别直接 100 - 2 = 98，扣减这个库存！ 你可以把可销售的库存：100 - 2 = 98，设置为 98 没问题，然后在一个单独的冻结库存的字段里，设置一个 2。也就是说，有 2 个库存是给冻结了。 积分服务的 addCredit() 接口也是同理，别直接给用户增加会员积分。你可以先在积分表里的一个预增加积分字段加入积分。 比如：用户积分原本是 1190，现在要增加 10 个积分，别直接 1190 + 10 = 1200 个积分啊！ 你可以保持积分为 1190 不变，在一个预增加字段里，比如说 prepare_add_credit 字段，设置一个 10，表示有 10 个积分准备增加。 仓储服务的 saleDelivery() 接口也是同理啊，你可以先创建一个销售出库单，但是这个销售出库单的状态是“UNKNOWN”。 也就是说，刚刚创建这个销售出库单，此时还不确定它的状态是什么呢！ 上面这套改造接口的过程，其实就是所谓的 TCC 分布式事务中的第一个 T 字母代表的阶段，也就是 Try 阶段。 总结上述过程，如果你要实现一个 TCC 分布式事务，首先你的业务的主流程以及各个接口提供的业务含义，不是说直接完成那个业务操作，而是完成一个 Try 的操作。 这个操作，一般都是锁定某个资源，设置一个预备类的状态，冻结部分数据，等等，大概都是这类操作。 咱们来一起看看下面这张图，结合上面的文字，再来捋一捋整个过程： TCC 实现阶段二：Confirm然后就分成两种情况了，第一种情况是比较理想的，那就是各个服务执行自己的那个 Try 操作，都执行成功了，Bingo！ 这个时候，就需要依靠 TCC 分布式事务框架来推动后续的执行了。这里简单提一句，如果你要玩儿 TCC 分布式事务，必须引入一款 TCC 分布式事务框架，比如国内开源的 ByteTCC、Himly、TCC-transaction。 否则的话，感知各个阶段的执行情况以及推进执行下一个阶段的这些事情，不太可能自己手写实现，太复杂了。 如果你在各个服务里引入了一个 TCC 分布式事务的框架，订单服务里内嵌的那个 TCC 分布式事务框架可以感知到，各个服务的 Try 操作都成功了。 此时，TCC 分布式事务框架会控制进入 TCC 下一个阶段，第一个 C 阶段，也就是 Confirm 阶段。 为了实现这个阶段，你需要在各个服务里再加入一些代码。比如说，订单服务里，你可以加入一个 Confirm 的逻辑，就是正式把订单的状态设置为“已支付”了，大概是类似下面这样子： 123456public class OrderServiceConfirm &#123; public void pay()&#123; orderDao.updateStatus(OrderStatus.PAYED); &#125;&#125; 库存服务也是类似的，你可以有一个 InventoryServiceConfirm 类，里面提供一个 reduceStock() 接口的 Confirm 逻辑，这里就是将之前冻结库存字段的 2 个库存扣掉变为 0。 这样的话，可销售库存之前就已经变为 98 了，现在冻结的 2 个库存也没了，那就正式完成了库存的扣减。 积分服务也是类似的，可以在积分服务里提供一个 CreditServiceConfirm 类，里面有一个 addCredit() 接口的 Confirm 逻辑，就是将预增加字段的 10 个积分扣掉，然后加入实际的会员积分字段中，从 1190 变为 1120。 仓储服务也是类似，可以在仓储服务中提供一个 WmsServiceConfirm 类，提供一个 saleDelivery() 接口的 Confirm 逻辑，将销售出库单的状态正式修改为“已创建”，可以供仓储管理人员查看和使用，而不是停留在之前的中间状态“UNKNOWN”了。 好了，上面各种服务的 Confirm 的逻辑都实现好了，一旦订单服务里面的 TCC 分布式事务框架感知到各个服务的 Try 阶段都成功了以后，就会执行各个服务的 Confirm 逻辑。 订单服务内的 TCC 事务框架会负责跟其他各个服务内的 TCC 事务框架进行通信，依次调用各个服务的 Confirm 逻辑。然后，正式完成各个服务的所有业务逻辑的执行。 同样，给大家来一张图，顺着图一起来看看整个过程： TCC 实现阶段三：Cancel好，这是比较正常的一种情况，那如果是异常的一种情况呢？ 举个例子：在 Try 阶段，比如积分服务吧，它执行出错了，此时会怎么样？ 那订单服务内的 TCC 事务框架是可以感知到的，然后它会决定对整个 TCC 分布式事务进行回滚。 也就是说，会执行各个服务的第二个 C 阶段，Cancel 阶段。同样，为了实现这个 Cancel 阶段，各个服务还得加一些代码。 首先订单服务，它得提供一个 OrderServiceCancel 的类，在里面有一个 pay() 接口的 Cancel 逻辑，就是可以将订单的状态设置为“CANCELED”，也就是这个订单的状态是已取消。 库存服务也是同理，可以提供 reduceStock() 的 Cancel 逻辑，就是将冻结库存扣减掉 2，加回到可销售库存里去，98 + 2 = 100。 积分服务也需要提供 addCredit() 接口的 Cancel 逻辑，将预增加积分字段的 10 个积分扣减掉。 仓储服务也需要提供一个 saleDelivery() 接口的 Cancel 逻辑，将销售出库单的状态修改为“CANCELED”设置为已取消。 然后这个时候，订单服务的 TCC 分布式事务框架只要感知到了任何一个服务的 Try 逻辑失败了，就会跟各个服务内的 TCC 分布式事务框架进行通信，然后调用各个服务的 Cancel 逻辑。 大家看看下面的图，直观的感受一下： 总结与思考好了，兄弟们，聊到这儿，基本上大家应该都知道 TCC 分布式事务具体是怎么回事了！ 总结一下，你要玩儿 TCC 分布式事务的话：首先需要选择某种 TCC 分布式事务框架，各个服务里就会有这个 TCC 分布式事务框架在运行。 然后你原本的一个接口，要改造为 3 个逻辑，Try-Confirm-Cancel： 先是服务调用链路依次执行 Try 逻辑。 如果都正常的话，TCC 分布式事务框架推进执行 Confirm 逻辑，完成整个事务。 如果某个服务的 Try 逻辑有问题，TCC 分布式事务框架感知到之后就会推进执行各个服务的 Cancel 逻辑，撤销之前执行的各种操作。 这就是所谓的 TCC 分布式事务。TCC 分布式事务的核心思想，说白了，就是当遇到下面这些情况时： 某个服务的数据库宕机了。 某个服务自己挂了。 那个服务的 Redis、Elasticsearch、MQ 等基础设施故障了。 某些资源不足了，比如说库存不够这些。 先来 Try 一下，不要把业务逻辑完成，先试试看，看各个服务能不能基本正常运转，能不能先冻结我需要的资源。 如果 Try 都 OK，也就是说，底层的数据库、Redis、Elasticsearch、MQ 都是可以写入数据的，并且你保留好了需要使用的一些资源（比如冻结了一部分库存）。 接着，再执行各个服务的 Confirm 逻辑，基本上 Confirm 就可以很大概率保证一个分布式事务的完成了。 那如果 Try 阶段某个服务就失败了，比如说底层的数据库挂了，或者 Redis 挂了，等等。 此时就自动执行各个服务的 Cancel 逻辑，把之前的 Try 逻辑都回滚，所有服务都不要执行任何设计的业务逻辑。保证大家要么一起成功，要么一起失败。 等一等，你有没有想到一个问题？如果有一些意外的情况发生了，比如说订单服务突然挂了，然后再次重启，TCC 分布式事务框架是如何保证之前没执行完的分布式事务继续执行的呢？ 所以，TCC 事务框架都是要记录一些分布式事务的活动日志的，可以在磁盘上的日志文件里记录，也可以在数据库里记录。保存下来分布式事务运行的各个阶段和状态。 问题还没完，万一某个服务的 Cancel 或者 Confirm 逻辑执行一直失败怎么办呢？ 那也很简单，TCC 事务框架会通过活动日志记录各个服务的状态。举个例子，比如发现某个服务的 Cancel 或者 Confirm 一直没成功，会不停的重试调用它的 Cancel 或者 Confirm 逻辑，务必要它成功！ 当然了，如果你的代码没有写什么 Bug，有充足的测试，而且 Try 阶段都基本尝试了一下，那么其实一般 Confirm、Cancel 都是可以成功的！ 最后，再给大家来一张图，来看看给我们的业务，加上分布式事务之后的整个执行流程： 不少大公司里，其实都是自己研发 TCC 分布式事务框架的，专门在公司内部使用，比如我们就是这样。 不过如果自己公司没有研发 TCC 分布式事务框架的话，那一般就会选用开源的框架。 这里笔者给大家推荐几个比较不错的框架，都是咱们国内自己开源出去的：ByteTCC，TCC-transaction，Himly。 大家有兴趣的可以去它们的 GitHub 地址，学习一下如何使用，以及如何跟 Spring Cloud、Dubbo 等服务框架整合使用。 只要把那些框架整合到你的系统里，很容易就可以实现上面那种奇妙的 TCC 分布式事务的效果了。 下面，我们来讲讲可靠消息最终一致性方案实现的分布式事务，同时聊聊在实际生产中遇到的运用该方案的高可用保障架构。 最终一致性分布式事务如何保障实际生产中 99.99% 高可用？上面咱们聊了聊 TCC 分布式事务，对于常见的微服务系统，大部分接口调用是同步的，也就是一个服务直接调用另外一个服务的接口。 这个时候，用 TCC 分布式事务方案来保证各个接口的调用，要么一起成功，要么一起回滚，是比较合适的。 但是在实际系统的开发过程中，可能服务间的调用是异步的。也就是说，一个服务发送一个消息给 MQ，即消息中间件，比如 RocketMQ、RabbitMQ、Kafka、ActiveMQ 等等。 然后，另外一个服务从 MQ 消费到一条消息后进行处理。这就成了基于 MQ 的异步调用了。 那么针对这种基于 MQ 的异步调用，如何保证各个服务间的分布式事务呢？也就是说，我希望的是基于 MQ 实现异步调用的多个服务的业务逻辑，要么一起成功，要么一起失败。 这个时候，就要用上可靠消息最终一致性方案，来实现分布式事务。 大家看上图，如果不考虑各种高并发、高可用等技术挑战的话，单从“可靠消息”以及“最终一致性”两个角度来考虑，这种分布式事务方案还是比较简单的。 可靠消息最终一致性方案的核心流程上游服务投递消息如果要实现可靠消息最终一致性方案，一般你可以自己写一个可靠消息服务，实现一些业务逻辑。 首先，上游服务需要发送一条消息给可靠消息服务。这条消息说白了，你可以认为是对下游服务一个接口的调用，里面包含了对应的一些请求参数。 然后，可靠消息服务就得把这条消息存储到自己的数据库里去，状态为“待确认”。 接着，上游服务就可以执行自己本地的数据库操作，根据自己的执行结果，再次调用可靠消息服务的接口。 如果本地数据库操作执行成功了，那么就找可靠消息服务确认那条消息。如果本地数据库操作失败了，那么就找可靠消息服务删除那条消息。 此时如果是确认消息，那么可靠消息服务就把数据库里的消息状态更新为“已发送”，同时将消息发送给 MQ。 这里有一个很关键的点，就是更新数据库里的消息状态和投递消息到 MQ。这俩操作，你得放在一个方法里，而且得开启本地事务。 啥意思呢？如果数据库里更新消息的状态失败了，那么就抛异常退出了，就别投递到 MQ；如果投递 MQ 失败报错了，那么就要抛异常让本地数据库事务回滚。这俩操作必须得一起成功，或者一起失败。 如果上游服务是通知删除消息，那么可靠消息服务就得删除这条消息。 下游服务接收消息下游服务就一直等着从 MQ 消费消息好了，如果消费到了消息，那么就操作自己本地数据库。 如果操作成功了，就反过来通知可靠消息服务，说自己处理成功了，然后可靠消息服务就会把消息的状态设置为“已完成”。 如何保证上游服务对消息的 100% 可靠投递？上面的核心流程大家都看完：一个很大的问题就是，如果在上述投递消息的过程中各个环节出现了问题该怎么办？ 我们如何保证消息 100% 的可靠投递，一定会从上游服务投递到下游服务？别着急，下面我们来逐一分析。 如果上游服务给可靠消息服务发送待确认消息的过程出错了，那没关系，上游服务可以感知到调用异常的，就不用执行下面的流程了，这是没问题的。 如果上游服务操作完本地数据库之后，通知可靠消息服务确认消息或者删除消息的时候，出现了问题。 比如：没通知成功，或者没执行成功，或者是可靠消息服务没成功的投递消息到 MQ。这一系列步骤出了问题怎么办？ 其实也没关系，因为在这些情况下，那条消息在可靠消息服务的数据库里的状态会一直是“待确认”。 此时，我们在可靠消息服务里开发一个后台定时运行的线程，不停的检查各个消息的状态。 如果一直是“待确认”状态，就认为这个消息出了点什么问题。此时的话，就可以回调上游服务提供的一个接口，问问说，兄弟，这个消息对应的数据库操作，你执行成功了没啊？ 如果上游服务答复说，我执行成功了，那么可靠消息服务将消息状态修改为“已发送”，同时投递消息到 MQ。 如果上游服务答复说，没执行成功，那么可靠消息服务将数据库中的消息删除即可。 通过这套机制，就可以保证，可靠消息服务一定会尝试完成消息到 MQ 的投递。 如何保证下游服务对消息的 100% 可靠接收？那如果下游服务消费消息出了问题，没消费到？或者是下游服务对消息的处理失败了，怎么办？ 其实也没关系，在可靠消息服务里开发一个后台线程，不断的检查消息状态。 如果消息状态一直是“已发送”，始终没有变成“已完成”，那么就说明下游服务始终没有处理成功。 此时可靠消息服务就可以再次尝试重新投递消息到 MQ，让下游服务来再次处理。 只要下游服务的接口逻辑实现幂等性，保证多次处理一个消息，不会插入重复数据即可。 如何基于 RocketMQ 来实现可靠消息最终一致性方案？在上面的通用方案设计里，完全依赖可靠消息服务的各种自检机制来确保： 如果上游服务的数据库操作没成功，下游服务是不会收到任何通知。如果上游服务的数据库操作成功了，可靠消息服务死活都会确保将一个调用消息投递给下游服务，而且一定会确保下游服务务必成功处理这条消息。通过这套机制，保证了基于 MQ 的异步调用/通知的服务间的分布式事务保障。其实阿里开源的 RocketMQ，就实现了可靠消息服务的所有功能，核心思想跟上面类似。 只不过 RocketMQ 为了保证高并发、高可用、高性能，做了较为复杂的架构实现，非常的优秀。有兴趣的同学，自己可以去查阅 RocketMQ 对分布式事务的支持。 可靠消息最终一致性方案的高可用保障生产实践背景引入上面那套方案和思想，很多同学应该都知道是怎么回事儿，我们也主要就是铺垫一下这套理论思想。 在实际落地生产的时候，如果没有高并发场景的，完全可以参照上面的思路自己基于某个 MQ 中间件开发一个可靠消息服务。 如果有高并发场景的，可以用 RocketMQ 的分布式事务支持上面的那套流程都可以实现。 今天给大家分享的一个核心主题，就是这套方案如何保证 99.99% 的高可用。 大家应该发现了这套方案里保障高可用性最大的一个依赖点，就是 MQ 的高可用性。 任何一种 MQ 中间件都有一整套的高可用保障机制，无论是 RabbitMQ、RocketMQ 还是 Kafka。 所以在大公司里使用可靠消息最终一致性方案的时候，我们通常对可用性的保障都是依赖于公司基础架构团队对 MQ 的高可用保障。 也就是说，大家应该相信兄弟团队，99.99% 可以保障 MQ 的高可用，绝对不会因为 MQ 集群整体宕机，而导致公司业务系统的分布式事务全部无法运行。 但是现实是很残酷的，很多中小型的公司，甚至是一些中大型公司，或多或少都遇到过 MQ 集群整体故障的场景。 MQ 一旦完全不可用，就会导致业务系统的各个服务之间无法通过 MQ 来投递消息，导致业务流程中断。 比如最近就有一个朋友的公司，也是做电商业务的，就遇到了 MQ 中间件在自己公司机器上部署的集群整体故障不可用，导致依赖 MQ 的分布式事务全部无法跑通，业务流程大量中断的情况。 这种情况，就需要针对这套分布式事务方案实现一套高可用保障机制。 基于 KV 存储的队列支持的高可用降级方案大家来看看下面这张图，这是我曾经指导过朋友的一个公司针对可靠消息最终一致性方案设计的一套高可用保障降级机制。 这套机制不算太复杂，可以非常简单有效的保证那位朋友公司的高可用保障场景，一旦 MQ 中间件出现故障，立马自动降级为备用方案。 ①自行封装 MQ 客户端组件与故障感知 首先第一点，你要做到自动感知 MQ 的故障接着自动完成降级，那么必须动手对 MQ 客户端进行封装，发布到公司 Nexus 私服上去。 然后公司需要支持 MQ 降级的业务服务都使用这个自己封装的组件来发送消息到 MQ，以及从 MQ 消费消息。 在你自己封装的 MQ 客户端组件里，你可以根据写入 MQ 的情况来判断 MQ 是否故障。 比如说，如果连续 10 次重新尝试投递消息到 MQ 都发现异常报错，网络无法联通等问题，说明 MQ 故障，此时就可以自动感知以及自动触发降级开关。 ②基于 KV 存储中队列的降级方案 如果 MQ 挂掉之后，要是希望继续投递消息，那么就必须得找一个 MQ 的替代品。 举个例子，比如我那位朋友的公司是没有高并发场景的，消息的量很少，只不过可用性要求高。此时就可以使用类似 Redis 的 KV 存储中的队列来进行替代。 由于 Redis 本身就支持队列的功能，还有类似队列的各种数据结构，所以你可以将消息写入 KV 存储格式的队列数据结构中去。 PS：关于 Redis 的数据存储格式、支持的数据结构等基础知识，请大家自行查阅了，网上一大堆。 但是，这里有几个大坑，一定要注意一下： 第一个，任何 KV 存储的集合类数据结构，建议不要往里面写入数据量过大，否则会导致大 Value 的情况发生，引发严重的后果。 因此绝不能在 Redis 里搞一个 Key，就拼命往这个数据结构中一直写入消息，这是肯定不行的。 第二个，绝对不能往少数 Key 对应的数据结构中持续写入数据，那样会导致热 Key 的产生，也就是某几个 Key 特别热。 大家要知道，一般 KV 集群，都是根据 Key 来 Hash 分配到各个机器上的，你要是老写少数几个 Key，会导致 KV 集群中的某台机器访问过高，负载过大。 基于以上考虑，下面是笔者当时设计的方案： 根据它们每天的消息量，在 KV 存储中固定划分上百个队列，有上百个 Key 对应。这样保证每个 Key 对应的数据结构中不会写入过多的消息，而且不会频繁的写少数几个 Key。一旦发生了 MQ 故障，可靠消息服务可以对每个消息通过 Hash 算法，均匀的写入固定好的上百个 Key 对应的 KV 存储的队列中。同时需要通过 ZK 触发一个降级开关，整个系统在 MQ 这块的读和写全部立马降级。 ③下游服务消费 MQ 的降级感知 下游服务消费 MQ 也是通过自行封装的组件来做的，此时那个组件如果从 ZK 感知到降级开关打开了，首先会判断自己是否还能继续从 MQ 消费到数据？ 如果不能了，就开启多个线程，并发的从 KV 存储的各个预设好的上百个队列中不断的获取数据。 每次获取到一条数据，就交给下游服务的业务逻辑来执行。通过这套机制，就实现了 MQ 故障时候的自动故障感知，以及自动降级。如果系统的负载和并发不是很高的话，用这套方案大致是没问题的。 因为在生产落地的过程中，包括大量的容灾演练以及生产实际故障发生时的表现来看，都是可以有效的保证 MQ 故障时，业务流程继续自动运行的。 ④故障的自动恢复 如果降级开关打开之后，自行封装的组件需要开启一个线程，每隔一段时间尝试给 MQ 投递一个消息看看是否恢复了。 如果 MQ 已经恢复可以正常投递消息了，此时就可以通过 ZK 关闭降级开关，然后可靠消息服务继续投递消息到 MQ，下游服务在确认 KV 存储的各个队列中已经没有数据之后，就可以重新切换为从 MQ 消费消息。 ⑤更多的业务细节 上面说的那套方案是一套通用的降级方案，但是具体的落地是要结合各个公司不同的业务细节来决定的，很多细节多没法在文章里体现。 比如说你们要不要保证消息的顺序性？是不是涉及到需要根据业务动态，生成大量的 Key？等等。 此外，这套方案实现起来还是有一定的成本的，所以建议大家尽可能还是 Push 公司的基础架构团队，保证 MQ 的 99.99% 可用性，不要宕机。 其次就是根据大家公司实际对高可用的需求来决定，如果感觉 MQ 偶尔宕机也没事，可以容忍的话，那么也不用实现这种降级方案。 但是如果公司领导认为 MQ 中间件宕机后，一定要保证业务系统流程继续运行，那么还是要考虑一些高可用的降级方案，比如本文提到的这种。 最后再说一句，真要是一些公司涉及到每秒几万几十万的高并发请求，那么对 MQ 的降级方案会设计的更加的复杂，那就远远不是这么简单可以做到的。 文章来源：终于有人把“TCC分布式事务”实现原理讲明白了","tags":[]},{"title":"分布式全站爬虫——以搜狗电视剧为例","date":"2020-04-09T11:40:39.526Z","path":"2020/04/09/分布式全站爬虫——以搜狗电视剧为例/","text":"全站抓取的技巧 1这是崔斯特的第一百一十九篇原创文章 分布式全站爬虫——以”搜狗电视剧”为例先看看robots协议 123User-agent: *Allow: / Sitemap: http://kan.sogou.com/sitemap/sitemap.xml 牛皮！ 分析打开一个具体的影视：http://kan.sogou.com/player/181171191/，网址中有具体数字ID，我们假设数字ID就是递增的，即从1开始，那么我们可以拼接url： http://kan.sogou.com/player/1/ http://kan.sogou.com/player/2/ http://kan.sogou.com/player/3/ http://kan.sogou.com/player/4/ … 有两个问题： ID上限是多少，我不可能永不止境的往上加吧 抓取效率问题，如果有10亿，那么我就需要发送10亿个HTTP请求，如果你开启了1000个线程，0.3秒能处理完一个ID，一天可以抓取：1000 * 0.3 * 60 * 60 * 24 = 25920000 个，需要约38天才能抓完，这肯定是不合乎要求的 针对这两个问题，可以用如下方法： 使用采样，比如我们确定间隔是1000，我们在1~1000中随机取数，在1001~2000中再随机取一个数，这样10亿数就被缩短为一百万了，这个数字就小多了 凭什么说上限是10亿呢，我们在真正爬虫之前还需要一次调研，调研的时候可以把间隔调大，比如5000，这次抓取只是为了评估ID分布范围，比如第一段是[1, 10000]，第二段是[1000000, 9000000]，最后一段是[10000000000, 10090000000]。确定ID分布范围后就可以在指定区间内采样抓取 代码核心代码参考：generate_uid.py，该函数是主节点开启的一个线程，该线程会监控redis中爬虫start_urls队列，如果小于预期，调用生成器生成ID列表，加入到队列中，同时更新此时的ID到redis。 运行爬虫，命令是：scrapy crawl sougou -a master=True，日志样例如下： 123456789102020-04-07 22:05:06 [scrapy.core.engine] INFO: Spider opened2020-04-07 22:05:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)2020-04-07 22:05:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:60232020-04-07 22:05:09 [sougou] INFO: spider sougou gen uid 400000012020-04-07 22:06:06 [scrapy.extensions.logstats] INFO: Crawled 3569 pages (at 3569 pages/min), scraped 0 items (at 0 items/min)2020-04-07 22:07:06 [scrapy.extensions.logstats] INFO: Crawled 7034 pages (at 3465 pages/min), scraped 0 items (at 0 items/min)2020-04-07 22:08:06 [scrapy.extensions.logstats] INFO: Crawled 10521 pages (at 3487 pages/min), scraped 0 items (at 0 items/min)2020-04-07 22:09:06 [scrapy.extensions.logstats] INFO: Crawled 13849 pages (at 3328 pages/min), scraped 0 items (at 0 items/min)2020-04-07 22:09:39 [sougou] INFO: success get useful id http://kan.sogou.com/updown.php?gid=181159677&amp;op=get2020-04-07 22:10:06 [scrapy.extensions.logstats] INFO: Crawled 17292 pages (at 3443 pages/min), scraped 0 items (at 0 items/min) 分布式这是MS设计，master负责去向队列里添加任务，slave负责消费。（这里的master同样有消费） 开启master只需要向启动命令添加额外参数，slave启动方式正常即可。 注意：master只能开启一个，否则会有重复任务，slave开启多少个取决于机器、网络、代理条件。 思考这种全量抓取方式只适合ID是数字的，这种网站还挺多的，淘宝、京东、知乎、美团等等。这些ID并不是递增，而是分布在一块块区域，先宏观上调查出大体范围，再缩小ID间隔。 但是有些网站，比如优酷的：https://v.youku.com/v_show/id_XNDU4OTM3NzM0NA==.html，id明显就是混淆过的，想要全量抓取只能通过分类接口去抓。这个有时间再聊聊。 线程安全我们想想，这样设计会不会有线程安全？ 先看看slave端，对redis的操作只有一个就是获取redis里的id，我们看看代码，scrapy_redis.spiders.RedisMixin.next_requests 123456789101112131415161718def next_requests(self): \"\"\"Returns a request to be scheduled or none.\"\"\" use_set = self.settings.getbool('REDIS_START_URLS_AS_SET', defaults.START_URLS_AS_SET) fetch_one = self.server.spop if use_set else self.server.lpop # XXX: Do we need to use a timeout here? found = 0 # TODO: Use redis pipeline execution. while found &lt; self.redis_batch_size: data = fetch_one(self.redis_key) if not data: # Queue empty. break req = self.make_request_from_data(data) if req: yield req found += 1 else: self.logger.debug(\"Request not made from data: %r\", data) scrapy-redis使用使用list结构，所以这里我们用到的是lpop命令，多次去redis中获取request，直到found = self.redis_batch_size，每次从redis中获取request的数量如果没有设置，默认就是settings中的CONCURRENT_REQUESTS，一般就是16。 if self.redis_batch_size is None: # TODO: Deprecate this setting (REDIS_START_URLS_BATCH_SIZE). self.redis_batch_size = settings.getint( 'REDIS_START_URLS_BATCH_SIZE', settings.getint('CONCURRENT_REQUESTS'), ) 每次从redis中获取到request后，会直接调用self.make_request_from_data(data)方法。作者在注释里也说到了这里的两个改进方法： Use timeout。redis的lpop是阻塞操作，所以理论上需要加上超时 Use redis pipeline。如果不了解的可以看看文档，Redis 管道（Pipelining）， 有心可以去提个pr，以后就可以吹牛说自己是scrapy-redis的contributors了，有人已经行动了，read start_urls use redis pipeline 因为lpop是原子操作，任何时候只会有单一线程从redis中拿到request，所以在获取request这一步是线程安全的。对这块不熟悉的可以阅读Redis 和 I/O 多路复用 再看看master端，有两个redis操作， 查询spider种子数量，使用llen 如果数量小于预期，生成任务ID，使用lpush插入数据 线程安全一般出现多线程之间的共享变量，这个场景下共享变量是什么，redis中的request列表吗，我仔细想了下，因为我们对redis的操作都保证原子性，并且插入的id保证不重复，所以不会出现问题。可以改进的地方，就是在master端使用redis pipeline操作。 欢迎交流想法。 实际应用这两天看到一个新闻： “卖空”研究公司Wolfpack Research今日在其网站上发布报告称，爱奇艺早在2018年IPO（首次公开招股）之前就存在欺诈行为，而且此后也一直在这样做。 该机构表示，我们估计爱奇艺将其2019年的营收夸大了约80―130亿元人民币（27%―44%），将其用户数量夸大了约42%―60%。 别的不说，就用户数，现在这个分布式爬虫就可以抓取用户数据。用户主页是：https://www.iqiyi.com/u/1416657079/feeds，也是数字的，同样可以使用采样的办法去抓取用户数据。","tags":[]},{"title":"IO模型","date":"2020-03-24T11:36:03.592Z","path":"2020/03/24/IO模型/","text":"IO模型介绍 (๑• . •๑) 1这是崔斯特的第一百一十八篇原创文章 基础 前言说到IO模型，都会牵扯到同步、异步、阻塞、非阻塞这几个词。从词的表面上看，很多人都觉得很容易理解。但是细细一想，却总会发现有点摸不着头脑。自己也曾被这几个词弄的迷迷糊糊的，每次看相关资料弄明白了，然后很快又给搞混了。 经历过这么几次之后，发现这东西必须得有所总结提炼才不至于再次混为一谈。尤其是最近看到好几篇讲这个的文章，很多都有谬误，很容易把本来就搞不清楚的人弄的更加迷糊。 最适合IO模型的例子应该是咱们平常生活中的去餐馆吃饭这个场景，下文就结合这个来讲解一下经典的几个IO模型。在此之前，先需要说明以下几点： IO有内存IO、网络IO和磁盘IO三种，通常我们说的IO指的是后两者。 阻塞和非阻塞，是函数/方法的实现方式，即在数据就绪之前是立刻返回还是等待，即发起IO请求是否会被阻塞。 以文件IO为例,一个IO读过程是文件数据从磁盘→内核缓冲区→用户内存的过程。同步与异步的区别主要在于数据从内核缓冲区→用户内存这个过程需不需要用户进程等待，即实际的IO读写是否阻塞请求进程。(网络IO把磁盘换做网卡即可) IO模型同步阻塞去餐馆吃饭，点一个自己最爱吃的盖浇饭，然后在原地等着一直到盖浇饭做好，自己端到餐桌就餐。这就是典型的同步阻塞。当厨师给你做饭的时候，你需要一直在那里等着。 网络编程中，读取客户端的数据需要调用recvfrom。在默认情况下，这个调用会一直阻塞直到数据接收完毕，就是一个同步阻塞的IO方式。这也是最简单的IO模型，在通常fd较少、就绪很快的情况下使用是没有问题的。 同步非阻塞接着上面的例子，你每次点完饭就在那里等着，突然有一天你发现自己真傻。于是，你点完之后，就回桌子那里坐着，然后估计差不多了，就问老板饭好了没，如果好了就去端，没好的话就等一会再去问，依次循环直到饭做好。这就是同步非阻塞。 这种方式在编程中对socket设置O_NONBLOCK即可。但此方式仅仅针对网络IO有效，对磁盘IO并没有作用。因为本地文件IO就没有被认为是阻塞，我们所说的网络IO的阻塞是因为网路IO有无限阻塞的可能，而本地文件除非是被锁住，否则是不可能无限阻塞的，因此只有锁这种情况下，O_NONBLOCK才会有作用。而且，磁盘IO时要么数据在内核缓冲区中直接可以返回，要么需要调用物理设备去读取，这时候进程的其他工作都需要等待。因此，后续的IO复用和信号驱动IO对文件IO也是没有意义的。 此外，需要说明的一点是nginx和node中对于本地文件的IO是用线程的方式模拟非阻塞的效果的，而对于静态文件的io，使用zero copy(例如sendfile、kafka、spark)的效率是非常高的。 IO复用接着上面的列子，你点一份饭然后循环的去问好没好显然有点得不偿失，还不如就等在那里直到准备好，但是当你点了好几样饭菜的时候，你每次都去问一下所有饭菜的状态(未做好/已做好)肯定比你每次阻塞在那里等着好多了。当然，你问的时候是需要阻塞的，一直到有准备好的饭菜或者你等的不耐烦(超时)。 这就引出了IO复用，也叫多路IO就绪通知。这是一种进程预先告知内核的能力，让内核发现进程指定的一个或多个IO条件就绪了，就通知进程。使得一个进程能在一连串的事件上等待。 IO复用的实现方式目前主要有select、poll和epoll。 select和poll的原理基本相同： 注册待侦听的fd(这里的fd创建时最好使用非阻塞) 每次调用都去检查这些fd的状态，当有一个或者多个fd就绪的时候返回 返回结果中包括已就绪和未就绪的fd 相比select，poll解决了单个进程能够打开的文件描述符数量有限制这个问题：select受限于FD_SIZE的限制，如果修改则需要修改这个宏重新编译内核；而poll通过一个pollfd数组向内核传递需要关注的事件，避开了文件描述符数量限制。 此外，select和poll共同具有的一个很大的缺点就是包含大量fd的数组被整体复制于用户态和内核态地址空间之间，开销会随着fd数量增多而线性增大。 select和poll就类似于上面说的就餐方式。但当你每次都去询问时，老板会把所有你点的饭菜都轮询一遍再告诉你情况，当大量饭菜很长时间都不能准备好的情况下是很低效的。于是，老板有些不耐烦了，就让厨师每做好一个菜就通知他。这样每次你再去问的时候，他会直接把已经准备好的菜告诉你，你再去端。这就是事件驱动IO就绪通知的方式-epoll。 epoll的出现，解决了select、poll的缺点： 基于事件驱动的方式，避免了每次都要把所有fd都扫描一遍。 epoll_wait只返回就绪的fd。 epoll使用mmap内存映射技术避免了内存复制的开销。 epoll的fd数量上限是操作系统的最大文件句柄数目,这个数目一般和内存有关，通常远大于1024。 目前，epoll是Linux2.6下最高效的IO复用方式，也是Nginx、Node的IO实现方式。而在freeBSD下，kqueue是另一种类似于epoll的IO复用方式。 此外，对于IO复用还有一个水平触发和边缘触发的概念： 水平触发：当就绪的fd未被用户进程处理后，下一次查询依旧会返回，这是select和poll的触发方式。 边缘触发：无论就绪的fd是否被处理，下一次不再返回。理论上性能更高，但是实现相当复杂，并且任何意外的丢失事件都会造成请求处理错误。epoll默认使用水平触发，通过相应选项可以使用边缘触发。 信号驱动上文的就餐方式还是需要你每次都去问一下饭菜状况。于是，你再次不耐烦了，就跟老板说，哪个饭菜好了就通知我一声吧。然后就自己坐在桌子那里干自己的事情。更甚者，你可以把手机号留给老板，自己出门，等饭菜好了直接发条短信给你。这就类似信号驱动的IO模型。 流程如下： 开启套接字信号驱动IO功能 系统调用sigaction执行信号处理函数（非阻塞，立刻返回） 数据就绪，生成sigio信号，通过信号回调通知应用来读取数据。 此种io方式存在的一个很大的问题：Linux中信号队列是有限制的，如果超过这个数字问题就无法读取数据。 异步非阻塞之前的就餐方式，到最后总是需要你自己去把饭菜端到餐桌。这下你也不耐烦了，于是就告诉老板，能不能饭好了直接端到你的面前或者送到你的家里(外卖)。这就是异步非阻塞IO了。 对比信号驱动IO，异步IO的主要区别在于：信号驱动由内核告诉我们何时可以开始一个IO操作(数据在内核缓冲区中)，而异步IO则由内核通知IO操作何时已经完成(数据已经在用户空间中)。 异步IO又叫做事件驱动IO，在Unix中，POSIX1003.1标准为异步方式访问文件定义了一套库函数，定义了AIO的一系列接口。使用aio_read或者aio_write发起异步IO操作，使用aio_error检查正在运行的IO操作的状态。但是其实现没有通过内核而是使用了多线程阻塞。此外，还有Linux自己实现的Native AIO，依赖两个函数：io_submit和io_getevents，虽然io是非阻塞的，但仍需要主动去获取读写的状态。 需要特别注意的是：AIO是I/O处理模式，是一种接口标准，各家操作系统可以实现也可以不实现。目前Linux中AIO的内核实现只对文件IO有效，如果要实现真正的AIO，需要用户自己来实现。 网络编程模型上文讲述了UNIX环境的五种IO模型。基于这五种模型，在Java中，随着NIO和NIO2.0(AIO)的引入，一般具有以下几种网络编程模型： BIO NIO AIO BIOBIO是一个典型的网络编程模型，是通常我们实现一个服务端程序的过程，步骤如下： 主线程accept请求阻塞 请求到达，创建新的线程来处理这个套接字，完成对客户端的响应。 主线程继续accept下一个请求 这种模型有一个很大的问题是：当客户端连接增多时，服务端创建的线程也会暴涨，系统性能会急剧下降。因此，在此模型的基础上，类似于 tomcat的bio connector，采用的是线程池来避免对于每一个客户端都创建一个线程。有些地方把这种方式叫做伪异步IO(把请求抛到线程池中异步等待处理)。 NIOJDK1.4开始引入了NIO类库，这里的NIO指的是Non-blcok IO，主要是使用Selector多路复用器来实现。Selector在Linux等主流操作系统上是通过epoll实现的。 NIO的实现流程，类似于select： 创建ServerSocketChannel监听客户端连接并绑定监听端口，设置为非阻塞模式。 创建Reactor线程，创建多路复用器(Selector)并启动线程。 将ServerSocketChannel注册到Reactor线程的Selector上。监听accept事件。 Selector在线程run方法中无线循环轮询准备就绪的Key。 Selector监听到新的客户端接入，处理新的请求，完成tcp三次握手，建立物理连接。 将新的客户端连接注册到Selector上，监听读操作。读取客户端发送的网络消息。 客户端发送的数据就绪则读取客户端请求，进行处理。 相比BIO，NIO的编程非常复杂。 AIOJDK1.7引入NIO2.0，提供了异步文件通道和异步套接字通道的实现。其底层在windows上是通过IOCP，在Linux上是通过epoll来实现的(LinuxAsynchronousChannelProvider.java,UnixAsynchronousServerSocketChannelImpl.java)。 创建AsynchronousServerSocketChannel，绑定监听端口 调用AsynchronousServerSocketChannel的accpet方法，传入自己实现的CompletionHandler。包括上一步，都是非阻塞的 连接传入，回调CompletionHandler的completed方法，在里面，调用AsynchronousSocketChannel的read方法，传入负责处理数据的CompletionHandler。 数据就绪，触发负责处理数据的CompletionHandler的completed方法。继续做下一步处理即可。 写入操作类似，也需要传入CompletionHandler。 其编程模型相比NIO有了不少的简化。 对比 . 同步阻塞IO 伪异步IO NIO AIO 客户端数目 ：IO线程 1 : 1 m : n m : 1 m : 0 IO模型 同步阻塞IO 同步阻塞IO 同步非阻塞IO 异步非阻塞IO 吞吐量 低 中 高 高 编程复杂度 简单 简单 非常复杂 复杂 参考资料 构建高性能Web站点 Netty权威指南","tags":[]},{"title":"Redis 和 I/O 多路复用","date":"2020-03-24T02:37:50.155Z","path":"2020/03/24/Redis-和-I-O-多路复用/","text":"1这是崔斯特的第一百一十七篇原创文章 面试必问 最近在看 UNIX 网络编程并研究了一下 Redis 的实现，感觉 Redis 的源代码十分适合阅读和分析，其中 I/O 多路复用（mutiplexing）部分的实现非常干净和优雅，在这里想对这部分的内容进行简单的整理。 几种 I/O 模型为什么 Redis 中要使用 I/O 多路复用这种技术呢？ 首先，Redis 是跑在单线程中的，所有的操作都是按照顺序线性执行的，但是由于读写操作等待用户输入或输出都是阻塞的，所以 I/O 操作在一般情况下往往不能直接返回，这会导致某一文件的 I/O 阻塞导致整个进程无法对其它客户提供服务，而 I/O 多路复用就是为了解决这个问题而出现的。 Blocking I/O先来看一下传统的阻塞 I/O 模型到底是如何工作的：当使用 read 或者 write 对某一个文件描述符（File Descriptor 以下简称 FD)进行读写时，如果当前 FD 不可读或不可写，整个 Redis 服务就不会对其它的操作作出响应，导致整个服务不可用。 这也就是传统意义上的，也就是我们在编程中使用最多的阻塞模型： 阻塞模型虽然开发中非常常见也非常易于理解，但是由于它会影响其他 FD 对应的服务，所以在需要处理多个客户端任务的时候，往往都不会使用阻塞模型。 I/O 多路复用虽然还有很多其它的 I/O 模型，但是在这里都不会具体介绍。 阻塞式的 I/O 模型并不能满足这里的需求，我们需要一种效率更高的 I/O 模型来支撑 Redis 的多个客户（redis-cli），这里涉及的就是 I/O 多路复用模型了： 在 I/O 多路复用模型中，最重要的函数调用就是 select，该方法的能够同时监控多个文件描述符的可读可写情况，当其中的某些文件描述符可读或者可写时，select 方法就会返回可读以及可写的文件描述符个数。 关于 select 的具体使用方法，在网络上资料很多，这里就不过多展开介绍了； 与此同时也有其它的 I/O 多路复用函数 epoll/kqueue/evport，它们相比 select 性能更优秀，同时也能支撑更多的服务。 Reactor 设计模式Redis 服务采用 Reactor 的方式来实现文件事件处理器（每一个网络连接其实都对应一个文件描述符） 文件事件处理器使用 I/O 多路复用模块同时监听多个 FD，当 accept、read、write 和 close 文件事件产生时，文件事件处理器就会回调 FD 绑定的事件处理器。 虽然整个文件事件处理器是在单线程上运行的，但是通过 I/O 多路复用模块的引入，实现了同时对多个 FD 读写的监控，提高了网络通信模型的性能，同时也可以保证整个 Redis 服务实现的简单。 I/O 多路复用模块I/O 多路复用模块封装了底层的 select、epoll、avport 以及 kqueue 这些 I/O 多路复用函数，为上层提供了相同的接口。 在这里我们简单介绍 Redis 是如何包装 select 和 epoll 的，简要了解该模块的功能，整个 I/O 多路复用模块抹平了不同平台上 I/O 多路复用函数的差异性，提供了相同的接口： static int aeApiCreate(aeEventLoop *eventLoop) static int aeApiResize(aeEventLoop *eventLoop, int setsize) static void aeApiFree(aeEventLoop *eventLoop) static int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) static void aeApiDelEvent(aeEventLoop *eventLoop, int fd, int mask) static int aeApiPoll(aeEventLoop eventLoop, struct timeval tvp) 同时，因为各个函数所需要的参数不同，我们在每一个子模块内部通过一个 aeApiState 来存储需要的上下文信息： 1234567891011// selecttypedef struct aeApiState &#123; fd_set rfds, wfds; fd_set _rfds, _wfds;&#125; aeApiState;// epolltypedef struct aeApiState &#123; int epfd; struct epoll_event *events;&#125; aeApiState; 这些上下文信息会存储在 eventLoop 的 void *state 中，不会暴露到上层，只在当前子模块中使用。 封装 select 函数select 可以监控 FD 的可读、可写以及出现错误的情况。 在介绍 I/O 多路复用模块如何对 select 函数封装之前，先来看一下 select 函数使用的大致流程： 123456789101112int fd = /* file descriptor */fd_set rfds;FD_ZERO(&amp;rfds);FD_SET(fd, &amp;rfds)for ( ; ; ) &#123; select(fd+1, &amp;rfds, NULL, NULL, NULL); if (FD_ISSET(fd, &amp;rfds)) &#123; /* file descriptor `fd` becomes readable */ &#125;&#125; 初始化一个可读的 fd_set 集合，保存需要监控可读性的 FD； 使用 FD_SET 将 fd 加入 rfds； 调用 select 方法监控 rfds 中的 FD 是否可读； 当 select 返回时，检查 FD 的状态并完成对应的操作。 而在 Redis 的 ae_select 文件中代码的组织顺序也是差不多的，首先在 aeApiCreate 函数中初始化 rfds 和 wfds： 12345678static int aeApiCreate(aeEventLoop *eventLoop) &#123; aeApiState *state = zmalloc(sizeof(aeApiState)); if (!state) return -1; FD_ZERO(&amp;state-&gt;rfds); FD_ZERO(&amp;state-&gt;wfds); eventLoop-&gt;apidata = state; return 0;&#125; 而 aeApiAddEvent 和 aeApiDelEvent 会通过 FD_SET 和 FD_CLR 修改 fd_set 中对应 FD 的标志位： 123456static int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) &#123; aeApiState *state = eventLoop-&gt;apidata; if (mask &amp; AE_READABLE) FD_SET(fd,&amp;state-&gt;rfds); if (mask &amp; AE_WRITABLE) FD_SET(fd,&amp;state-&gt;wfds); return 0;&#125; 整个 ae_select 子模块中最重要的函数就是 aeApiPoll，它是实际调用 select 函数的部分，其作用就是在 I/O 多路复用函数返回时，将对应的 FD 加入 aeEventLoop 的 fired 数组中，并返回事件的个数： 1234567891011121314151617181920212223242526static int aeApiPoll(aeEventLoop *eventLoop, struct timeval *tvp) &#123; aeApiState *state = eventLoop-&gt;apidata; int retval, j, numevents = 0; memcpy(&amp;state-&gt;_rfds,&amp;state-&gt;rfds,sizeof(fd_set)); memcpy(&amp;state-&gt;_wfds,&amp;state-&gt;wfds,sizeof(fd_set)); retval = select(eventLoop-&gt;maxfd+1, &amp;state-&gt;_rfds,&amp;state-&gt;_wfds,NULL,tvp); if (retval &gt; 0) &#123; for (j = 0; j &lt;= eventLoop-&gt;maxfd; j++) &#123; int mask = 0; aeFileEvent *fe = &amp;eventLoop-&gt;events[j]; if (fe-&gt;mask == AE_NONE) continue; if (fe-&gt;mask &amp; AE_READABLE &amp;&amp; FD_ISSET(j,&amp;state-&gt;_rfds)) mask |= AE_READABLE; if (fe-&gt;mask &amp; AE_WRITABLE &amp;&amp; FD_ISSET(j,&amp;state-&gt;_wfds)) mask |= AE_WRITABLE; eventLoop-&gt;fired[numevents].fd = j; eventLoop-&gt;fired[numevents].mask = mask; numevents++; &#125; &#125; return numevents;&#125; 封装 epoll 函数Redis 对 epoll 的封装其实也是类似的，使用 epoll_create 创建 epoll 中使用的 epfd： 123456789101112131415161718static int aeApiCreate(aeEventLoop *eventLoop) &#123; aeApiState *state = zmalloc(sizeof(aeApiState)); if (!state) return -1; state-&gt;events = zmalloc(sizeof(struct epoll_event)*eventLoop-&gt;setsize); if (!state-&gt;events) &#123; zfree(state); return -1; &#125; state-&gt;epfd = epoll_create(1024); /* 1024 is just a hint for the kernel */ if (state-&gt;epfd == -1) &#123; zfree(state-&gt;events); zfree(state); return -1; &#125; eventLoop-&gt;apidata = state; return 0;&#125; 在 aeApiAddEvent 中使用 epoll_ctl 向 epfd 中添加需要监控的 FD 以及监听的事件： 12345678910111213141516static int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) &#123; aeApiState *state = eventLoop-&gt;apidata; struct epoll_event ee = &#123;0&#125;; /* avoid valgrind warning */ /* If the fd was already monitored for some event, we need a MOD * operation. Otherwise we need an ADD operation. */ int op = eventLoop-&gt;events[fd].mask == AE_NONE ? EPOLL_CTL_ADD : EPOLL_CTL_MOD; ee.events = 0; mask |= eventLoop-&gt;events[fd].mask; /* Merge old events */ if (mask &amp; AE_READABLE) ee.events |= EPOLLIN; if (mask &amp; AE_WRITABLE) ee.events |= EPOLLOUT; ee.data.fd = fd; if (epoll_ctl(state-&gt;epfd,op,fd,&amp;ee) == -1) return -1; return 0;&#125; 由于 epoll 相比 select 机制略有不同，在 epoll_wait 函数返回时并不需要遍历所有的 FD 查看读写情况；在 epoll_wait 函数返回时会提供一个 epoll_event 数组： 1234567891011typedef union epoll_data &#123; void *ptr; int fd; /* 文件描述符 */ uint32_t u32; uint64_t u64;&#125; epoll_data_t;struct epoll_event &#123; uint32_t events; /* Epoll 事件 */ epoll_data_t data;&#125;; 其中保存了发生的 epoll 事件（EPOLLIN、EPOLLOUT、EPOLLERR 和 EPOLLHUP）以及发生该事件的 FD。 aeApiPoll 函数只需要将 epoll_event 数组中存储的信息加入 eventLoop 的 fired 数组中，将信息传递给上层模块： 123456789101112131415161718192021222324static int aeApiPoll(aeEventLoop *eventLoop, struct timeval *tvp) &#123; aeApiState *state = eventLoop-&gt;apidata; int retval, numevents = 0; retval = epoll_wait(state-&gt;epfd,state-&gt;events,eventLoop-&gt;setsize, tvp ? (tvp-&gt;tv_sec*1000 + tvp-&gt;tv_usec/1000) : -1); if (retval &gt; 0) &#123; int j; numevents = retval; for (j = 0; j &lt; numevents; j++) &#123; int mask = 0; struct epoll_event *e = state-&gt;events+j; if (e-&gt;events &amp; EPOLLIN) mask |= AE_READABLE; if (e-&gt;events &amp; EPOLLOUT) mask |= AE_WRITABLE; if (e-&gt;events &amp; EPOLLERR) mask |= AE_WRITABLE; if (e-&gt;events &amp; EPOLLHUP) mask |= AE_WRITABLE; eventLoop-&gt;fired[j].fd = e-&gt;data.fd; eventLoop-&gt;fired[j].mask = mask; &#125; &#125; return numevents;&#125; 子模块的选择因为 Redis 需要在多个平台上运行，同时为了最大化执行的效率与性能，所以会根据编译平台的不同选择不同的 I/O 多路复用函数作为子模块，提供给上层统一的接口；在 Redis 中，我们通过宏定义的使用，合理的选择不同的子模块： 12345678910111213#ifdef HAVE_EVPORT#include \"ae_evport.c\"#else #ifdef HAVE_EPOLL #include \"ae_epoll.c\" #else #ifdef HAVE_KQUEUE #include \"ae_kqueue.c\" #else #include \"ae_select.c\" #endif #endif#endif 因为 select 函数是作为 POSIX 标准中的系统调用，在不同版本的操作系统上都会实现，所以将其作为保底方案： Redis 会优先选择时间复杂度为 𝑂(1) 的 I/O 多路复用函数作为底层实现，包括 Solaries 10 中的 evport、Linux 中的 epoll 和 macOS/FreeBSD 中的 kqueue，上述的这些函数都使用了内核内部的结构，并且能够服务几十万的文件描述符。 但是如果当前编译环境没有上述函数，就会选择 select 作为备选方案，由于其在使用时会扫描全部监听的描述符，所以其时间复杂度较差 𝑂(𝑛)，并且只能同时服务 1024 个文件描述符，所以一般并不会以 select 作为第一方案使用。 总结Redis 对于 I/O 多路复用模块的设计非常简洁，通过宏保证了 I/O 多路复用模块在不同平台上都有着优异的性能，将不同的 I/O 多路复用函数封装成相同的 API 提供给上层使用。 整个模块使 Redis 能以单进程运行的同时服务成千上万个文件描述符，避免了由于多进程应用的引入导致代码实现复杂度的提升，减少了出错的可能性。 ReferenceSelect-Man-PagesReactor-Patternepoll vs kqueue 原文链接：Redis 和 I/O 多路复用 · 面向信仰编程 Follow: Draveness · GitHub","tags":[]},{"title":"我为什么很久没写博客了","date":"2020-03-13T12:08:29.000Z","path":"2020/03/13/我为什么很久没写博客了/","text":"看了下公众号记录，发现上次推文竟然还是去年双十二发的一篇薅羊毛的文章，有点小尴尬，这么久没发，不是因为密码完了","tags":[]},{"title":"Java并发编程实战-01-可见性、原子性和有序性问题：并发编程Bug的源头","date":"2020-03-10T08:09:10.548Z","path":"2020/03/10/Java并发编程实战-01-可见性、原子性和有序性问题：并发编程Bug的源头/","text":"Java 并发编程实战 (๑• . •๑) 1这是崔斯特的第一百一十六篇原创文章 如果你细心观察的话，你会发现，不管是哪一门编程语言，并发类的知识都是在高级篇里。换句话说，这块知识点其实对于程序员来说，是比较进阶的知识。我自己这么多年学习过来，也确实觉得并发是比较难的，因为它会涉及到很多的底层知识，比如若你对操作系统相关的知识一无所知的话，那去理解一些原理就会费些力气。这是我们整个专栏的第一篇文章，我说这些话的意思是如果你在中间遇到自己没想通的问题，可以去查阅资料，也可以在评论区找我，以保证你能够跟上学习进度。 你我都知道，编写正确的并发程序是一件极困难的事情，并发程序的 Bug 往往会诡异地出现，然后又诡异地消失，很难重现，也很难追踪，很多时候都让人很抓狂。但要快速而又精准地解决“并发”类的疑难杂症，你就要理解这件事情的本质，追本溯源，深入分析这些 Bug 的源头在哪里。 那为什么并发编程容易出问题呢？它是怎么出问题的？今天我们就重点聊聊这些 Bug 的源头。 并发程序幕后的故事这些年，我们的 CPU、内存、I/O 设备都在不断迭代，不断朝着更快的方向努力。但是，在这个快速发展的过程中，有一个核心矛盾一直存在，就是这三者的速度差异。CPU 和内存的速度差异可以形象地描述为：CPU 是天上一天，内存是地上一年（假设 CPU 执行一条普通指令需要一天，那么 CPU 读写内存得等待一年的时间）。内存和 I/O 设备的速度差异就更大了，内存是天上一天，I/O 设备是地上十年。 程序里大部分语句都要访问内存，有些还要访问 I/O，根据木桶理论（一只水桶能装多少水取决于它最短的那块木板），程序整体的性能取决于最慢的操作——读写 I/O 设备，也就是说单方面提高 CPU 性能是无效的。 为了合理利用 CPU 的高性能，平衡这三者的速度差异，计算机体系结构、操作系统、编译程序都做出了贡献，主要体现为： CPU 增加了缓存，以均衡与内存的速度差异； 操作系统增加了进程、线程，以分时复用 CPU，进而均衡 CPU 与 I/O 设备的速度差异； 编译程序优化指令执行次序，使得缓存能够得到更加合理地利用。 现在我们几乎所有的程序都默默地享受着这些成果，但是天下没有免费的午餐，并发程序很多诡异问题的根源也在这里。 源头之一：缓存导致的可见性问题在单核时代，所有的线程都是在一颗 CPU 上执行，CPU 缓存与内存的数据一致性容易解决。因为所有线程都是操作同一个 CPU 的缓存，一个线程对缓存的写，对另外一个线程来说一定是可见的。例如在下面的图中，线程 A 和线程 B 都是操作同一个 CPU 里面的缓存，所以线程 A 更新了变量 V 的值，那么线程 B 之后再访问变量 V，得到的一定是 V 的最新值（线程 A 写过的值）。 一个线程对共享变量的修改，另外一个线程能够立刻看到，我们称为可见性。 多核时代，每颗 CPU 都有自己的缓存，这时 CPU 缓存与内存的数据一致性就没那么容易解决了，当多个线程在不同的 CPU 上执行时，这些线程操作的是不同的 CPU 缓存。比如下图中，线程 A 操作的是 CPU-1 上的缓存，而线程 B 操作的是 CPU-2 上的缓存，很明显，这个时候线程 A 对变量 V 的操作对于线程 B 而言就不具备可见性了。这个就属于硬件程序员给软件程序员挖的“坑”。 下面我们再用一段代码来验证一下多核场景下的可见性问题。下面的代码，每执行一次 add10K()方法，都会循环 10000 次 count+=1 操作。在 calc()方法中我们创建了两个线程，每个线程调用一次 add10K()方法，我们来想一想执行 calc()方法得到的结果应该是多少呢？ 123456789101112131415161718192021222324252627public class Test &#123; private long count = 0; private void add10K() &#123; int idx = 0; while(idx++ &lt; 10000) &#123; count += 1; &#125; &#125; public static long calc() &#123; final Test test = new Test(); // 创建两个线程，执行add()操作 Thread th1 = new Thread(()-&gt;&#123; test.add10K(); &#125;); Thread th2 = new Thread(()-&gt;&#123; test.add10K(); &#125;); // 启动两个线程 th1.start(); th2.start(); // 等待两个线程执行结束 th1.join(); th2.join(); return count; &#125;&#125; 直觉告诉我们应该是 20000，因为在单线程里调用两次 add10K()方法，count 的值就是 20000，但实际上 calc()的执行结果是个 10000 到 20000 之间的随机数。为什么呢？ 我们假设线程 A 和线程 B 同时开始执行，那么第一次都会将 count=0 读到各自的 CPU 缓存里，执行完 count+=1 之后，各自 CPU 缓存里的值都是 1，同时写入内存后，我们会发现内存中是 1，而不是我们期望的 2。之后由于各自的 CPU 缓存里都有了 count 的值，两个线程都是基于 CPU 缓存里的 count 值来计算，所以导致最终 count 的值都是小于 20000 的。这就是缓存的可见性问题。 循环 10000 次 count+=1 操作如果改为循环 1 亿次，你会发现效果更明显，最终 count 的值接近 1 亿，而不是 2 亿。如果循环 10000 次，count 的值接近 20000，原因是两个线程不是同时启动的，有一个时差。 源头之二：线程切换带来的原子性问题由于 IO 太慢，早期的操作系统就发明了多进程，即便在单核的 CPU 上我们也可以一边听着歌，一边写 Bug，这个就是多进程的功劳。 操作系统允许某个进程执行一小段时间，例如 50 毫秒，过了 50 毫秒操作系统就会重新选择一个进程来执行（我们称为“任务切换”），这个 50 毫秒称为时间片。 在一个时间片内，如果一个进程进行一个 IO 操作，例如读个文件，这个时候该进程可以把自己标记为“休眠状态”并出让 CPU 的使用权，待文件读进内存，操作系统会把这个休眠的进程唤醒，唤醒后的进程就有机会重新获得 CPU 的使用权了。 这里的进程在等待 IO 时之所以会释放 CPU 使用权，是为了让 CPU 在这段等待时间里可以做别的事情，这样一来 CPU 的使用率就上来了；此外，如果这时有另外一个进程也读文件，读文件的操作就会排队，磁盘驱动在完成一个进程的读操作后，发现有排队的任务，就会立即启动下一个读操作，这样 IO 的使用率也上来了。 是不是很简单的逻辑？但是，虽然看似简单，支持多进程分时复用在操作系统的发展史上却具有里程碑意义，Unix 就是因为解决了这个问题而名噪天下的。 早期的操作系统基于进程来调度 CPU，不同进程间是不共享内存空间的，所以进程要做任务切换就要切换内存映射地址，而一个进程创建的所有线程，都是共享一个内存空间的，所以线程做任务切换成本就很低了。现代的操作系统都基于更轻量的线程来调度，现在我们提到的“任务切换”都是指“线程切换”。 Java 并发程序都是基于多线程的，自然也会涉及到任务切换，也许你想不到，任务切换竟然也是并发编程里诡异 Bug 的源头之一。任务切换的时机大多数是在时间片结束的时候，我们现在基本都使用高级语言编程，高级语言里一条语句往往需要多条 CPU 指令完成，例如上面代码中的count += 1，至少需要三条 CPU 指令。 指令 1：首先，需要把变量 count 从内存加载到 CPU 的寄存器； 指令 2：之后，在寄存器中执行+1 操作； 指令 3：最后，将结果写入内存（缓存机制导致可能写入的是 CPU 缓存而不是内存）。 操作系统做任务切换，可以发生在任何一条CPU 指令执行完，是的，是 CPU 指令，而不是高级语言里的一条语句。对于上面的三条指令来说，我们假设 count=0，如果线程 A 在指令 1 执行完后做线程切换，线程 A 和线程 B 按照下图的序列执行，那么我们会发现两个线程都执行了 count+=1 的操作，但是得到的结果不是我们期望的 2，而是 1。 我们潜意识里面觉得 count+=1 这个操作是一个不可分割的整体，就像一个原子一样，线程的切换可以发生在 count+=1 之前，也可以发生在 count+=1 之后，但就是不会发生在中间。我们把一个或者多个操作在 CPU 执行的过程中不被中断的特性称为原子性。CPU 能保证的原子操作是 CPU 指令级别的，而不是高级语言的操作符，这是违背我们直觉的地方。因此，很多时候我们需要在高级语言层面保证操作的原子性。 源头之三：编译优化带来的有序性问题那并发编程里还有没有其他有违直觉容易导致诡异 Bug 的技术呢？有的，就是有序性。顾名思义，有序性指的是程序按照代码的先后顺序执行。编译器为了优化性能，有时候会改变程序中语句的先后顺序，例如程序中：“a=6；b=7；”编译器优化后可能变成“b=7；a=6；”，在这个例子中，编译器调整了语句的顺序，但是不影响程序的最终结果。不过有时候编译器及解释器的优化可能导致意想不到的 Bug。 在 Java 领域一个经典的案例就是利用双重检查创建单例对象，例如下面的代码：在获取实例 getInstance()的方法中，我们首先判断 instance 是否为空，如果为空，则锁定 Singleton.class 并再次检查 instance 是否为空，如果还为空则创建 Singleton 的一个实例。 123456789101112public class Singleton &#123; static Singleton instance; static Singleton getInstance()&#123; if (instance == null) &#123; synchronized(Singleton.class) &#123; if (instance == null) instance = new Singleton(); &#125; &#125; return instance; &#125;&#125; 假设有两个线程 A、B 同时调用 getInstance()方法，他们会同时发现 instance == null ，于是同时对 Singleton.class 加锁，此时 JVM 保证只有一个线程能够加锁成功（假设是线程 A），另外一个线程则会处于等待状态（假设是线程 B）；线程 A 会创建一个 Singleton 实例，之后释放锁，锁释放后，线程 B 被唤醒，线程 B 再次尝试加锁，此时是可以加锁成功的，加锁成功后，线程 B 检查 instance == null 时会发现，已经创建过 Singleton 实例了，所以线程 B 不会再创建一个 Singleton 实例。 这看上去一切都很完美，无懈可击，但实际上这个 getInstance()方法并不完美。问题出在哪里呢？出在 new 操作上，我们以为的 new 操作应该是： 分配一块内存 M； 在内存 M 上初始化 Singleton 对象； 然后 M 的地址赋值给 instance 变量。 但是实际上优化后的执行路径却是这样的： 分配一块内存 M； 将 M 的地址赋值给 instance 变量； 最后在内存 M 上初始化 Singleton 对象。 优化后会导致什么问题呢？我们假设线程 A 先执行 getInstance()方法，当执行完指令 2 时恰好发生了线程切换，切换到了线程 B 上；如果此时线程 B 也执行 getInstance()方法，那么线程 B 在执行第一个判断时会发现 instance != null，所以直接返回 instance，而此时的 instance 是没有初始化过的，如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。 总结要写好并发程序，首先要知道并发程序的问题在哪里，只有确定了“靶子”，才有可能把问题解决，毕竟所有的解决方案都是针对问题的。并发程序经常出现的诡异问题看上去非常无厘头，但是深究的话，无外乎就是直觉欺骗了我们，只要我们能够深刻理解可见性、原子性、有序性在并发场景下的原理，很多并发 Bug 都是可以理解、可以诊断的. 在介绍可见性、原子性、有序性的时候，特意提到缓存导致的可见性问题，线程切换带来的原子性问题，编译优化带来的有序性问题，其实缓存、线程、编译优化的目的和我们写并发程序的目的是相同的，都是提高程序性能。但是技术在解决一个问题的同时，必然会带来另外一个问题，所以在采用一项技术的同时，一定要清楚它带来的问题是什么，以及如何规避。 我们这个专栏在讲解每项技术的时候，都会尽量将每项技术解决的问题以及产生的问题讲清楚，也希望你能够在这方面多思考、多总结。 课后思考常听人说，在 32 位的机器上对 long 型变量进行加减操作存在并发隐患，到底是不是这样呢？现在相信你一定能分析出来。 欢迎在留言区与我分享你的想法，也欢迎你在留言区记录你的思考过程。感谢阅读，如果你觉得这篇文章对你有帮助的话，也欢迎把它分享给更多的朋友。 优秀回复@Jialin对于双重锁的问题，我觉得任大鹏分析的蛮有道理，线程A进入第二个判空条件，进行初始化时，发生了时间片切换，即使没有释放锁，线程B刚要进入第一个判空条件时，发现条件不成立，直接返回instance引用，不用去获取锁。如果对instance进行volatile语义声明，就可以禁止指令重排序，避免该情况发生。对于有些同学对CPU缓存和内存的疑问，CPU缓存不存在于内存中的，它是一块比内存更小、读写速度更快的芯片，至于什么时候把数据从缓存写到内存，没有固定的时间，同样地，对于有volatile语义声明的变量，线程A执行完后会强制将值刷新到内存中，线程B进行相关操作时会强制重新把内存中的内容写入到自己的缓存，这就涉及到了volatile的写入屏障问题，当然也就是所谓happen-before问题。 @coderlong类型64位，所以在32位的机器上，对long类型的数据操作通常需要多条指令组合出来，无法保证原子性，所以并发的时候会出问题🌝🌝🌝 @嘎嘎针对阿根一世的问题，问题其实出现在new Singleton()这里。这一行分对于CPU来讲，有3个指令：1.分配内存空间2.初始化对象3.instance引用指向内存空间正常执行顺序1-2-3但是CPU重排序后执行顺序可能为1-3-2，那么问题就来了步骤如下：1.A、B线程同时进入了第一个if判断2.A首先进入synchronized块，由于instance为null，所以它执行instance = new Singleton();3.然后线程A执行1-&gt; JVM先画出了一些分配给Singleton实例的空白内存，并赋值给instance4.在还没有进行第三步（将instance引用指向内存空间）的时候，线程A离开了synchronized块5.线程B进入synchronized块，读取到了A线程返回的instance，此时这个instance并未进行物理地址指向，是一个空对象。有人说将对象设置成volatile，其实也不能完全解决问题。volatile只是保证可见性，并不保证原子性。 现行的比较通用的做法就是采用静态内部类的方式来实现。 12345678910111213public class MySingleton &#123; private static class MySingletonHandler&#123; private static MySingleton instanct = new MySingleton(); &#125; private MySingleton()&#123;&#125; public static MySingleton getInstance() &#123; return MySingletonHandler.instanct; &#125;&#125; @别皱眉周末了对留言问题总结一下 ——可见性问题——对于可见性那个例子我们先看下定义:可见性:一个线程对共享变量的修改，另外一个线程能够立刻看到 并发问题往往都是综合证，这里即使是单核CPU，只要出现线程切换就会有原子性问题。但老师的目的是为了让大家明白什么是可见性或许我们可以把线程对变量的读可写都看作时原子操作，也就是cpu对变量的操作中间状态不可见，这样就能更加理解什么是可见性了。 ——CPU缓存刷新到内存的时机——cpu将缓存写入内存的时机是不确定的。除非你调用cpu相关指令强刷 ——双重锁问题——如果A线程与B线程如果同时进入第一个分支，那么这个程序就没有问题 如果A线程先获取锁并出现指令重排序时，B线程未进入第一个分支，那么就可能出现空指针问题，这里说可能出现问题是因为当把内存地址赋值给共享变量后，CPU将数据写回缓存的时机是随机的 —— synchronized——线程在synchronized块中，发生线程切换，锁是不会释放的 ——指令优化——除了编译优化,有一部分可以通过看汇编代码来看，但是CPU和解释器在运行期也会做一部分优化，所以很多时候都是看不到的，也很难重现。 ——JMM模型和物理内存、缓存等关系——内存、cpu缓存是物理存在，jvm内存是软件存在的。关于线程的工作内存和寄存器、cpu缓存的关系 大家可以参考这篇文章https://blog.csdn.net/u013851082/article/details/70314778/ ——IO操作——io操作不占用cpu，读文件，是设备驱动干的事，cpu只管发命令。发完命令，就可以干别的事情了。 ——寄存器切换——寄存器是共用的，A线程切换到B线程的时候，寄存器会把操作A的相关内容会保存到内存里，切换回来的时候，会从内存把内容加载到寄存器。可以理解为每个线程有自己的寄存器 请老师帮忙看看，有没问题。希望我的总结能帮到更多人😄😄","tags":[]},{"title":"Java匠人手法-优雅的处理空值","date":"2020-03-04T12:34:00.939Z","path":"2020/03/04/Java匠人手法-优雅的处理空值/","text":"这是崔斯特的第一百一十五篇原创文章 努力、奋斗 导语 在笔者几年的开发经验中，经常看到项目中存在到处空值判断的情况，这些判断，会让人觉得摸不这头绪，它的出现很有可能和当前的业务逻辑并没有关系。但它会让你很头疼。有时候，更可怕的是系统因为这些空值的情况，会抛出空指针异常，导致业务系统发生问题。此篇文章，我总结了几种关于空值的处理手法，希望对读者有帮助。 业务中的空值场景存在一个UserSearchService用来提供用户查询的功能: 12345public interface UserSearchService&#123; List&lt;User&gt; listUser(); User get(Integer id);&#125; 问题现场对于面向对象语言来讲，抽象层级特别的重要。尤其是对接口的抽象，它在设计和开发中占很大的比重，我们在开发时希望尽量面向接口编程。对于以上描述的接口方法来看，大概可以推断出可能它包含了以下两个含义: listUser(): 查询用户列表 get(Integer id): 查询单个用户 在所有的开发中，XP推崇的TDD模式可以很好的引导我们对接口的定义，所以我们将TDD作为开发代码的”推动者”。对于以上的接口，当我们使用TDD进行测试用例先行时，发现了潜在的问题： listUser() 如果没有数据，那它是返回空集合还是null呢？ get(Integer id) 如果没有这个对象，是抛异常还是返回null呢？ 深入listUser研究我们先来讨论 1listUser() 这个接口，我经常看到如下实现: 1234567public List&lt;User&gt; listUser()&#123; List&lt;User&gt; userList = userListRepostity.selectByExample(new UserExample()); if(CollectionUtils.isEmpty(userList))&#123;//spring util工具类 return null; &#125; return userList;&#125; 这段代码返回是null,从我多年的开发经验来讲，对于集合这样返回值，最好不要返回null，因为如果返回了null，会给调用者带来很多麻烦。你将会把这种调用风险交给调用者来控制。 如果调用者是一个谨慎的人，他会进行是否为null的条件判断。如果他并非谨慎，或者他是一个面向接口编程的狂热分子(当然，面向接口编程是正确的方向)，他会按照自己的理解去调用接口，而不进行是否为null的条件判断，如果这样的话，是非常危险的，它很有可能出现空指针异常！ 根据墨菲定律来判断: “很有可能出现的问题，在将来一定会出现!” 基于此，我们将它进行优化: 1234567public List&lt;User&gt; listUser()&#123; List&lt;User&gt; userList = userListRepostity.selectByExample(new UserExample()); if(CollectionUtils.isEmpty(userList))&#123; return Lists.newArrayList();//guava类库提供的方式 &#125; return userList;&#125; 对于接口(List listUser())，它一定会返回List，即使没有数据，它仍然会返回List（集合中没有任何元素）; 通过以上的修改，我们成功的避免了有可能发生的空指针异常，这样的写法更安全！ 深入研究get方法对于接口 1User get(Integer id) 你能看到的现象是，我给出id，它一定会给我返回User。但事实真的很有可能不是这样的。 我看到过的实现: 123public User get(Integer id)&#123; return userRepository.selectByPrimaryKey(id);//从数据库中通过id直接获取实体对象&#125; 相信很多人也都会这样写。 通过代码的时候得知它的返回值很有可能是null! 但我们通过的接口是分辨不出来的!这个是个非常危险的事情。尤其对于调用者来说！ 我给出的建议是，需要在接口明明时补充文档,比如对于异常的说明,使用注解@exception: 1234567891011public interface UserSearchService&#123; /** * 根据用户id获取用户信息 * @param id 用户id * @return 用户实体 * @exception UserNotFoundException */ User get(Integer id);&#125; 我们把接口定义加上了说明之后，调用者会看到，如果调用此接口，很有可能抛出“UserNotFoundException(找不到用户)”这样的异常。 这种方式可以在调用者调用接口的时候看到接口的定义，但是，这种方式是”弱提示”的！如果调用者忽略了注释，有可能就对业务系统产生了风险，这个风险有可能导致一个亿！ 除了以上这种”弱提示”的方式，还有一种方式是，返回值是有可能为空的。那要怎么办呢？ 我认为我们需要增加一个接口，用来描述这种场景。引入jdk8的Optional,或者使用guava的Optional.看如下定义: 123456789public interface UserSearchService&#123; /** * 根据用户id获取用户信息 * @param id 用户id * @return 用户实体,此实体有可能是缺省值 */ Optional&lt;User&gt; getOptional(Integer id);&#125; Optional有两个含义: 存在 or 缺省。 那么通过阅读接口getOptional()，我们可以很快的了解返回值的意图，这个其实是我们想看到的，它去除了二义性。 它的实现可以写成: 123public Optional&lt;User&gt; getOptional(Integer id)&#123; return Optional.ofNullable(userRepository.selectByPrimaryKey(id));&#125; 深入入参通过上述的所有接口的描述，你能确定入参id一定是必传的吗？ 我觉得答案应该是：不能确定。除非接口的文档注释上加以说明。 那如何约束入参呢? 我给大家推荐两种方式： 强制约束 文档性约束（弱提示） 1.强制约束，我们可以通过jsr 303进行严格的约束声明: 12345678910111213141516public interface UserSearchService&#123; /** * 根据用户id获取用户信息 * @param id 用户id * @return 用户实体 * @exception UserNotFoundException */ User get(@NotNull Integer id); /** * 根据用户id获取用户信息 * @param id 用户id * @return 用户实体,此实体有可能是缺省值 */ Optional&lt;User&gt; getOptional(@NotNull Integer id);&#125; 当然，这样写，要配合AOP的操作进行验证，但让spring已经提供了很好的集成方案，在此我就不在赘述了。 2.文档性约束 在很多时候，我们会遇到遗留代码，对于遗留代码，整体性改造的可能性很小。我们更希望通过阅读接口的实现，来进行接口的说明。 jsr 305规范，给了我们一个描述接口入参的一个方式(需要引入库 com.google.code.findbugs:jsr305): 可以使用注解: @Nullable @Nonnull @CheckForNull 进行接口说明。比如: 1234567891011121314151617public interface UserSearchService&#123; /** * 根据用户id获取用户信息 * @param id 用户id * @return 用户实体 * @exception UserNotFoundException */ @CheckForNull User get(@NonNull Integer id); /** * 根据用户id获取用户信息 * @param id 用户id * @return 用户实体,此实体有可能是缺省值 */ Optional&lt;User&gt; getOptional(@NonNull Integer id);&#125; 小结通过 空集合返回值，Optional，jsr 303，jsr 305这几种方式，可以让我们的代码可读性更强，出错率更低！ 空集合返回值 ： 如果有集合这样返回值时，除非真的有说服自己的理由，否则，一定要返回空集合，而不是null Optional: 如果你的代码是jdk8，就引入它！ 如果不是，则使用Guava的Optional,或者升级jdk版本！ 它很大程度的能增加了接口的可读性！ jsr 303: 如果新的项目正在开发，不防加上这个试试！ 一定有一种特别爽的感觉! jsr 305: 如果老的项目在你的手上，你可以尝试的加上这种文档型注解，有助于你后期的重构，或者新功能增加了，对于老接口的理解! 空对象模式场景我们来看一个DTO转化的场景，对象: 1234567891011@Datastatic class PersonDTO&#123; private String dtoName; private String dtoAge;&#125;@Datastatic class Person&#123; private String name; private String age;&#125; 需求是将Person对象转化成PersonDTO，然后进行返回。 当然对于实际操作来讲，返回如果Person为空，将返回null,但是PersonDTO是不能返回null的（尤其Rest接口返回的这种DTO）。 在这里，我们只关注转化操作，看如下代码: 1234567891011121314@Testpublic void shouldConvertDTO()&#123; PersonDTO personDTO = new PersonDTO(); Person person = new Person(); if(!Objects.isNull(person))&#123; personDTO.setDtoAge(person.getAge()); personDTO.setDtoName(person.getName()); &#125;else&#123; personDTO.setDtoAge(\"\"); personDTO.setDtoName(\"\"); &#125;&#125; 优化修改这样的数据转化，我们认识可读性非常差，每个字段的判断，如果是空就设置为空字符串(“”) 换一种思维方式进行思考，我们是拿到Person这个类的数据，然后进行赋值操作(setXXX),其实是不关系Person的具体实现是谁的。 那我们可以创建一个Person子类: 1234567891011static class NullPerson extends Person&#123; @Override public String getAge() &#123; return \"\"; &#125; @Override public String getName() &#123; return \"\"; &#125;&#125; 它作为Person的一种特例而存在，如果当Person为空的时候，则返回一些get的默认行为. 所以代码可以修改为: 12345678910111213@Test public void shouldConvertDTO()&#123; PersonDTO personDTO = new PersonDTO(); Person person = getPerson(); personDTO.setDtoAge(person.getAge()); personDTO.setDtoName(person.getName()); &#125; private Person getPerson()&#123; return new NullPerson();//如果Person是null ,则返回空对象 &#125; 其中getPerson()方法，可以用来根据业务逻辑获取Person有可能的对象（对当前例子来讲，如果Person不存在，返回Person的的特例NUllPerson），如果修改成这样，代码的可读性就会变的很强了。 使用Optional可以进行优化空对象模式，它的弊端在于需要创建一个特例对象，但是如果特例的情况比较多，我们是不是需要创建多个特例对象呢，虽然我们也使用了面向对象的多态特性，但是，业务的复杂性如果真的让我们创建多个特例对象，我们还是要再三考虑一下这种模式，它可能会带来代码的复杂性。 对于上述代码，还可以使用Optional进行优化。 1234567891011121314@Test public void shouldConvertDTO()&#123; PersonDTO personDTO = new PersonDTO(); Optional.ofNullable(getPerson()).ifPresent(person -&gt; &#123; personDTO.setDtoAge(person.getAge()); personDTO.setDtoName(person.getName()); &#125;); &#125; private Person getPerson()&#123; return null; &#125; Optional对空值的使用，我觉得更为贴切，它只适用于”是否存在”的场景。 如果只对控制的存在判断，我建议使用Optional. Optioanl的正确使用Optional如此强大，它表达了计算机最原始的特性(0 or 1),那它如何正确的被使用呢! Optional不要作为参数如果你写了一个public方法，这个方法规定了一些输入参数，这些参数中有一些是可以传入null的，那这时候是否可以使用Optional呢？ 我给的建议是: 一定不要这样使用! 举个例子: 123public interface UserService&#123; List&lt;User&gt; listUser(Optional&lt;String&gt; username);&#125; 这个例子的方法 listUser,可能在告诉我们需要根据username查询所有数据集合，如果username是空，也要返回所有的用户集合. 当我们看到这个方法的时候，会觉得有一些歧义: “如果username是absent,是返回空集合吗？还是返回全部的用户数据集合？” Optioanl是一种分支的判断，那我们究竟是关注 Optional还是Optional.get()呢？ 我给大家的建议是，如果不想要这样的歧义，就不要使用它！ 如果你真的想表达两个含义，就給它拆分出两个接口: 1234public interface UserService&#123; List&lt;User&gt; listUser(String username); List&lt;User&gt; listUser();&#125; 我觉得这样的语义更强，并且更能满足 软件设计原则中的 “单一职责”。 如果你觉得你的入参真的有必要可能传null,那请使用jsr 303或者jsr 305进行说明和验证! 请记住! Optional不能作为入参的参数! Optional作为返回值当个实体的返回那Optioanl可以做为返回值吗？ 其实它是非常满足是否存在这个语义的。 你如说，你要根据id获取用户信息，这个用户有可能存在或者不存在。 你可以这样使用: 123public interface UserService&#123; Optional&lt;User&gt; get(Integer id);&#125; 当调用这个方法的时候，调用者很清楚get方法返回的数据，有可能不存在，这样可以做一些更合理的判断，更好的防止空指针的错误！ 当然，如果业务方真的需要根据id必须查询出User的话，就不要这样使用了，请说明，你要抛出的异常. 只有当考虑它返回null是合理的情况下，才进行Optional的返回 集合实体的返回不是所有的返回值都可以这样用的！ 如果你返回的是集合： 123public interface UserService&#123; Optional&lt;List&lt;User&gt;&gt; listUser();&#125; 这样的返回结果，会让调用者不知所措，是否我判断Optional之后，还用进行isEmpty的判断呢？ 这样带来的返回值歧义！ 我认为是没有必要的。 我们要约定，对于List这种集合返回值，如果集合真的是null的，请返回空集合(Lists.newArrayList); 使用Optional变量1Optional&lt;User&gt; userOpt = ... 如果有这样的变量userOpt,请记住 ： 一定不能直接使用get ，如果这样用，就丧失了Optional本身的含义 （ 比如userOp.get() ） 不要直接使用getOrThrow ,如果你有这样的需求：获取不到就抛异常。 那就要考虑，是否是调用的接口设计的是否合理 getter中的使用对于一个java bean,所有的属性都有可能返回null,那是否需要改写所有的getter成为Optional类型呢？ 我给大家的建议是，不要这样滥用Optional. 即便 我java bean中的getter是符合Optional的，但是因为java bean 太多了，这样会导致你的代码有50%以上进行Optinal的判断，这样便污染了代码。(我想说，其实你的实体中的字段应该都是由业务含义的，会认真的思考过它存在的价值的，不能因为Optional的存在而滥用) 我们应该更关注于业务，而不只是空值的判断。 请不要在getter中滥用Optional. 小结可以这样总结Optional的使用： 当使用值为空的情况，并非源于错误时，可以使用Optional! Optional不要用于集合操作! 不要滥用Optional,比如在java bean的getter中!","tags":[]},{"title":"Leetcode-136-只出现一次的数字","date":"2020-02-23T06:28:19.620Z","path":"2020/02/23/Leetcode-136-只出现一次的数字/","text":"这是崔斯特的第一百一十四篇原创文章 努力、奋斗 题目描述给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。 说明： 你的算法应该具有线性时间复杂度。 你可以不使用额外空间来实现吗？ 示例 1: 输入: [2,2,1]输出: 1示例 2: 输入: [4,1,2,1,2]输出: 4 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/single-number 解题思路粗暴解法使用map保存每个元素在数组中出现的次数，key是元素，value是出现次数。 这种方法显然不符合题目需求，不使用额外空间。 位运算按位异或：参与运算的两个值，如果两个相应bit位相同，则结果为0，否则为1。 如： 12340^0 = 0，1^0 = 1，0^1 = 1，1^1 = 0 异或运算的符号是^，其定义如下： 1a xor b = (a &amp; ^b) | (^a &amp; b) 真值表： a b a xor b 0 0 0 0 1 1 1 0 1 1 1 0 因为有定律a ^ b ^ a = b，所以我们这里就可以使用异或来做。 12345def single_number(numbers): tmp = 0 for n in numbers: tmp ^= n return tmp 1234567public int singleNumber(int[] nums) &#123; int result = 0; for (int num : nums) &#123; result ^= num; &#125; return result;&#125; 可以看到非常简单。 这些全都是计算机基础知识，但是自己不知道，只能说明自己基础差，以后补得起来吗。","tags":[]},{"title":"Kafka 消费者 Java 实现","date":"2020-01-09T13:01:06.553Z","path":"2020/01/09/Kafka-消费者-Java-实现/","text":"这是崔斯特的第一百一十三篇原创文章 努力、奋斗 (๑• . •๑) 应用程序使用 KafkaConsumer向 Kafka 订阅 Topic 接收消息，首先理解 Kafka 中消费者（consumer）和消费者组（consumer group）的概念和特性。 KafkaConsumer消费者和消费者组当生产者向 Topic 写入消息的速度超过了消费者（consumer）的处理速度，导致大量的消息在 Kafka 中淤积，此时需要对消费者进行横向伸缩，用多个消费者从同一个主题读取消息，对消息进行分流。 Kafka 的消费者都属于消费者组（consumer group）。一个组中的 consumer 订阅同样的 topic，每个 consumer 接收 topic 一些分区（partition）中的消息。同一个分区不能被一个组中的多个 consumer 消费。 假设现在有一个 Topic 有4个分区，有一个消费者组订阅了这个 Topic，随着组中的消费者数量从1个增加到5个时，Topic 中分区被读取的情况： 如果组中 consumer 的数量超过分区数，多出的 consumer 会被闲置。因此，如果想提高消费者的并行处理能力，需要设置足够多的 partition 数量。 除了通过增加 consumer 来横向伸缩单个应用程序外，还会出现多个应用程序从同一个 Topic 读取数据的情况。这也是 Kafka 设计的主要目标之一：让 Topic 中的数据能够满足各种应用场景的需求。 如果要每个应用程序都可以获取到所有的消息，而不只是其中的一部分，只要保证每个应用程序有自己的 consumer group，就可以获取到 Topic 所有的消息： 横向伸缩 Kafka 消费者和消费者群组并不会对性能造成负面影响。 分区再均衡一个消费者组内的 consumer 共同读取 Topic 的分区。 当一个 consumer 加入组时，读取的是原本由其他 consumer 读取的分区。 当一个 consumer 离开组时（被关闭或发生崩溃），原本由它读取的分区将由组里的其他 consumer 来读取。 当 Topic 发生变化时，比如添加了新的分区，会发生分区重分配。 分区的所有权从一个消费者转移到另一个消费者，这样的行为被称为再均衡（rebalance）。再均衡非常重要，为消费者组带来了高可用性和伸缩性，可以放心的增加或移除消费者。 再均衡期间，消费者无法读取消息，造成整个 consumer group 一小段时间的不可用。另外，当分区被重新分配给另一个消费者时，当前的读取状态会丢失。 消费者通过向作为组协调器（GroupCoordinator）的 broker（不同的组可以有不同的协调器）发送心跳来维持和群组以及分区的关系。心跳表明消费者在读取分区里的消息。消费者会在轮询消息或提交偏移量（offset）时发送心跳。如果消费者停止发送心跳的时间足够长，会话就会过期，组协调器认为消费者已经死亡，会触发一次再均衡。 在 Kafka 0.10.1 的版本中，对心跳行为进行了修改，由一个独立的线程负责心跳。 消费 Kafka创建 Kafka 消费者在读取消息之前，需要先创建一个 KafkaConsumer 对象。创建 KafkaConsumer 对象与创建 KafkaProducer 非常相似，创建 KafkaConsumer 示例： 12345678Properties props = new Properties();props.put(\"bootstrap.servers\", \"broker1:9092, broker2:9092\");// group.id，指定了消费者所属群组props.put(\"group.id\", \"CountryCounter\");props.put(\"key.deserializer\", \"org.apache.kafka.common.serializaiton.StrignDeserializer\");props.put(\"value.deserializer\", \"org.apache.kafka.common.serializaiton.StrignDeserializer\");KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); 订阅主题创建了消费者之后，需要订阅 Topic，subscribe() 方法接受一个主题列表作为参数： 12// topic name is “customerCountries”consumer.subscribe(Collections.singletonList(\"customerCountries\")); subscribe() 也可以接收一个正则表达式，匹配多个主题（如果有新的名称匹配的主题创建，会立即触发一次再均衡，消费者就可以读取新添加的主题）。在 Kafka 和其他系统之间复制数据时，使用正则表达式的方式订阅多个主题是很常见的做法。 12// 订阅所有 test 前缀的 Topic：consumer.subscribe(\"test.*\"); 消息轮询消息轮询是消费者的核心，通过轮询向服务器请求数据。消息轮询 API 会处理所有的细节，包括群组协调、分区再均衡、发送心跳和获取数据，开发者只需要处理从分区返回的数据。消费者代码的主要部分如下所示： 12345678910111213141516try &#123; while (true) &#123; // 100 是超时时间（ms），在该时间内 poll 会等待服务器返回数据 ConsumerReccords&lt;String, String&gt; records = consumer.poll(100); // poll 返回一个记录列表。 // 每条记录都包含了记录所属主题的信息、记录所在分区的信息、记录在分区里的偏移量，以及记录的键值对。 for (ConsumerReccord&lt;String, String&gt; record : records) &#123; log.debug(\"topic=%s, partition=%s, offset=%d, customer=%s, country=%s\", record.topic(), record.partition(), record.offset(), record.key(), record.value()); &#125;&#125; finally &#123; // 关闭消费者,网络连接和 socket 也会随之关闭，并立即触发一次再均衡 consumer.close();&#125; 在第一次调用新消费者的 poll() 方法时，会负责查找 GroupCoordinator，然后加入群组，接受分配的分区。如果发生了再均衡，整个过程也是在轮询期间进行的。心跳也是从轮询里发送出去的。 消费者配置Kafka 与消费者相关的配置大部分参数都有合理的默认值，一般不需要修改，不过有一些参数与消费者的性能和可用性有很大关系。接下来介绍这些重要的属性。 fetch.min.bytes指定消费者从服务器获取记录的最小字节数。服务器在收到消费者的数据请求时，如果可用的数据量小于 fetch.min.bytes，那么会等到有足够的可用数据时才返回给消费者。 合理的设置可以降低消费者和 broker 的工作负载，在 Topic 消息生产不活跃时，减少处理消息次数。如果没有很多可用数据，但消费者的 CPU 使用率却很高，需要调高该属性的值。如果消费者的数量比较多，调高该属性的值也可以降低 broker 的工作负载。 fetch.max.wait.ms指定在 broker 中的等待时间，默认是500ms。如果没有足够的数据流入 Kafka，消费者获取的数据量的也没有达到 fetch.min.bytes，最终导致500ms的延迟。 如果要降低潜在的延迟（提高 SLA），可以调低该属性的值。fetch.max.wait.ms 和 fetch.min.bytes 有一个满足条件就会返回数据。 max.parition.fetch.bytes指定了服务器从每个分区里返回给消费者的最大字节数，默认值是1MB。也就是说 KafkaConsumer#poll() 方法从每个分区里返回的记录最多不超过 max.parition.fetch.bytes 指定的字节。 如果一个主题有20个分区和5个消费者（同一个组内），那么每个消费者需要至少4MB 的可用内存（每个消费者读取4个分区）来接收记录。如果组内有消费者发生崩溃，剩下的消费者需要处理更多的分区。 max.parition.fetch.bytes 必须比 broker 能够接收的最大消息的字节数（max.message.size）大，否则消费者可能无法读取这些消息，导致消费者一直重试。 另一个需要考虑的因素是消费者处理数据的时间。消费者需要频繁调用 poll() 方法来避免会话过期和发生分区再均衡，如果单次调用 poll() 返回的数据太多，消费者需要更多的时间来处理，可能无法及时进行下一个轮询来避免会话过期。如果出现这种情况，可以把 max.parition.fetch.bytes 值改小或者延长会话过期时间。 session.timeout.ms指定了消费者与服务器断开连接的最大时间，默认是3s。如果消费者没有在指定的时间内发送心跳给 GroupCoordinator，就被认为已经死亡，会触发再均衡，把它的分区分配给其他消费者。 该属性与 heartbeat.interval.ms 紧密相关，heartbeat.interval.ms 指定了 poll() 方法向协调器发送心跳的频率，session.timeout.ms 指定了消费者最长多久不发送心跳。所以，一般需要同时修改这两个属性，heartbeat.interval.ms 必须比 session.timeout.ms 小，一般是 session.timeout.ms 的三分之一，如果 session.timeout.ms 是 3s，那么 heartbeat.interval.ms 应该是 1s。 调低属性的值可以更快地检测和恢复崩溃的节点，不过长时间的轮询或垃圾收集可能导致非预期的再均衡。调高属性的值，可以减少意外的再均衡，不过检测节点崩溃需要更长的时间。 auto.offset.reset指定了消费者在读取一个没有偏移量（offset）的分区或者偏移量无效的情况下（因消费者长时间失效，包含偏移量的记录已经过时井被删除）该作何处理，默认值是 latest，表示在 offset 无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的记录）。 另一个值是 earliest，消费者将从起始位置读取分区的记录。 enable.auto.commit指定了消费者是否自动提交偏移量，默认值是 true，自动提交。 设为 false 可以程序自己控制何时提交偏移量。如果设为 true，需要通过配置 auto.commit.interval.ms 属性来控制提交的频率。 partition.assignment.strategy分区分配给组内消费者的策略，根据给定的消费者和 Topic，决定哪些分区应该被分配给哪个消费者。Kafka 有两个默认的分配策略： Range，把 Topic 的若干个连续的分区分配给消费者。假设 consumer1 和 consumer2（c1、c2 代替）订阅了 topic1 和 topic2（t1、t2 代替），每个 Topic 都有3个分区。那么 c1 可能分配到 t1-part-0、t1-part-1、t2-part-0 和 t2-part1，而 c2 可能分配到 t1-part-2 和 t2-part-2。只要使用了 Range 策略，而且分区数量无法被消费者数量整除，就会出现这种情况。 RoundRobin，把所有分区逐个分配给消费者。上面的例子如果使用 RoundRobin 策略，那么 c1 可能分配到 t1-part-0、t1-part-2 和 t2-part-1，c2 可能分配到 t1-part-1、t2-part-0 和 t2-part-2。一般来说，RoundRobin 策略会给所有消费者分配大致相同的分区数。 默认值是 org.apache.kafka.clients.consumer.RangeAssignor，这个类实现了 Range 策略，org.apache.kafka.clients.consumer.RoundRobinAssignor 是 RoundRobin 策略的实现类。还可以使用自定义策略，属性值设为自定义类的名字。 client.idbroker 用来标识从客户端发送过来的消息，可以是任意字符串，通常被用在日志、度量指标和配额中。 max.poll.records用于控制单次调用 call() 方法能够返回的记录数量，帮助控制在轮询里需要处理的数据量。 receive.buffer.bytes 和 send.buffer.bytes分别指定了 TCP socket 接收和发送数据包的缓冲区大小。如果设为-1就使用操作系统的默认值。如果生产者或消费者与 broker 处于不同的数据中心，那么可以适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。 消费与提交提交和偏移量每次调用 poll() 方法，总是返回 Kafka 中还没有被消费者读取过的记录，使用偏移量（offset）来记录消费者读取的分区的位置。 更新分区当前位置的操作叫做“提交（commit）”，消费者是如何提交偏移量的呢？ 消费者向一个特殊的 Topic：_consumer_offset 发送消息，消息包含每个分区的偏移量。偏移量只有在消费者发生崩溃或者有新的消费者加入群组触发再均衡时有用。完成再均衡之后，消费者可能分配到新的分区，为了能够继续之前的工作，消费者需要读取每个分区最后一次提交的 offset，然后从 offset 指定的地方继续处理。如果提交的 offset 大于客户端处理的最后一个消息偏移量，那么处于两个偏移量之间的消息会丢失。反之则会消息重复。 所以，处理偏移量的方式对应用程序会有很大的影响。KafkaConsumer API 提供了多种方式来提交偏移量。 自动提交最简单的方式是消费者自动提交偏移量。如果 enable.auto.commit 设为 true，那 么每过一定时间间隔，消费者会自动把从 poll() 方法接收到的最大偏移量提交上去。提交时间间隔由 auto.commit.interval.ms 控制，默认是5s。 自动提交是在轮询里进行的。消费者每次在进行轮询时会检查是否需要提交偏移量，如果是，那么会提交从上一次轮询返回的偏移量。 假设我们使用默认的5s提交时间间隔，在最近一次提交之后的3s发生了再均衡，再 均衡之后，消费者从最后一次提交的偏移量位置开始读取消息。这个时候偏移量已经落后了3s，这3s内的数据已经处理过，再次消费是还会获取到。通过调低提交时间间隔来更频繁地提交偏移量，减小可能出现重复消费的时间窗，不过这种情况是无法完全避免的。 在使用自动提交时，每次调用轮询方法都会把上一次调用返回的偏移量提交上去，并不 知道具体哪些消息已经被处理了，所以在再次调用之前最好确保所有当前调用返回的消息都已经处理完毕（在调用 close() 方法前也会进行自动提交）。 在处理异常或提前退出轮询时要格外小心。自动提交虽然方便，不过并没有为开发者留有余地来避免重复处理消息。 提交当前偏移量KafkaConsumer API 提供的另一种提交偏移量的方式，程序主动触发提交当前偏移量，而不是基于时间间隔自动提交。 把 auto.commit.offset 设为 false，使用 commitSync() 方法提交偏移量最简单也最可靠，该方法会提交由 poll() 方法返回的最新偏移量，提交成功后马上返回，如果提交失败就抛出异常。 需要注意，commitSync() 将会提交 poll() 返回的最新偏移量，在处理完所有记录后调用 commitSync()，否则还是会有丢失消息的风险。 commitSync() 提交偏移量的例子： 12345678910111213141516while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf(\"topic=%s, partition=%s, offset=%d, customer=%s, country=%s\", record.topic(), record.partition(), record.offset(), record.key(), record.value()); // 处理消息的逻辑省略 &#125; try &#123; // poll 的数据全部处理完提交 consumer.commitSync(); &#125; catch (CommitFailedException e) &#123; log.error(\"commit failed\", e) &#125;&#125; 只要没有发生不可恢复的错误，commitSync() 会一直尝试直至提交成功。如果提交 失败会抛出 CommitFailedException 异常。 异步提交手动提交有一个不足之处，在 broker 对提交请求作出回应之前，应用程序会阻塞，这会影响应用程序的吞吐量。可以使用异步提交的方式，不等待 broker 的响应。 1234567891011while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf(\"topic=%s, partition=%s, offset=%d, customer=%s, country=%s\", record.topic(), record.partition(), record.offset(), record.key(), record.value()); &#125; // 异步提交 consumer.commitAsync();&#125; 在成功提交或发生无法恢复的错误之前，commitSync() 会一直尝试直至提交成功，但是 commitAsync() 不会，这也是该方法的一个问题。之所以不进行重试，是因为在收到服务器响应之前，可能有一个更大的偏移量已经提交成功。 假设我们发出一个请求提交偏移量2000，这个时候发生了短暂的通信问题，服务器收不到请求，与此同时，程序处理了另外一批消息，并成功提交了偏移量3000。如果 commitAsync() 重新尝试提交偏移量2000，有可能将偏移量3000改为2000，这个时候如果发生再均衡，就会出现重复消息。 commitAsync() 支持回调，在 broker 作出响应时会执行回调。回调经常被用于记录提交错误或生成度量指标，如果要用它来进行重试，一定要注意提交的顺序。 commitAsync() 支持回调，在 broker 作出响应时会执行回调。回调经常被用于记录提交错误或生成度量指标，如果要用它来进行重试，一定要注意提交的顺序。 1234567891011121314151617while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf(\"topic=%s, partition=%s, offset=%d, customer=%s, country=%s\", record.topic(), record.partition(), record.offset(), record.key(), record.value()); &#125; consumer.commitAsync(new OffsetCommitCallback() &#123; // 提交完成时回回调此函数 public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception) &#123; if (e != null) log.error(\"Commit failed for offsets &#123;&#125;\", offsets, e); &#125; &#125;);&#125; 重试异步提交 可以使用一个单调递增的序列号来维护异步提交的顺序。在每次提交偏移量之后或在回调里提交偏移量时递增序列号。在进行重试前，先检查回调的序列号和即将提交的偏移量是否相等，如果相等，说明没有新的提交，那么可以安全地进行重试。如果序列号比较大，说明有一个新的提交已经发送出去了，放弃重试。 同步与异步混合提交一般情况下，针对偶尔出现的提交失败，不进行重试不会有太大问题，如果因为临时网络问题导致的，那么后续的提交总会有成功的。但如果这是发生在关闭消费者或再均衡前的最后一次提交，就要确保能够提交成功。 在消费者关闭前一般会组合使用 commitAsync() 和 commitSync()： 12345678910111213141516171819202122try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf(\"topic=%s, partition=%s, offset=%d, customer=%s, country=%s\", record.topic(), record.partition(), record.offset(), record.key(), record.value()); &#125; // 异步提交 consumer.commitAsync(); &#125;&#125; catch (Exception e) &#123; log.error(\"Unexpected error\", e);&#125; finally &#123; try &#123; // 同步提交 consumer.commitSync(); &#125; finally &#123; consumer.close(); &#125;&#125; 在正常处理流程中，使用异步提交来提高性能，最后使用同步提交来保证位移提交成功。 提交特定的偏移量一般提交偏移量的频率与处理消息批次的频率是一样的。如果想要更频繁地提交怎么办？如果 poll() 方法返回一大批数据，为了避免因再均衡引起的重复处理整批消息，想要在批次中间提交偏移量该怎么办？ 这种情况无法通过调用 commitSync() 或 commitAsync() 来实现，只会提交最后一个偏移量，而此时该批次里的消息还没有处理完。 KafkaConsumer API 允许在调用 commitSync() 和 commitAsync() 方法时传进去希望提交的分区和偏移量的 map。因为消费者可能不只读取一个分区，需要跟踪所有分区的偏移量，所以在这个层面上控制偏移量的提交会让代码变复杂。 1234567891011121314151617181920212223242526// 记录分区的 offset 信息Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets = new HashMap&lt;&gt;();int count = 0;while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf(\"topic=%s, partition=%s, offset=%d, customer=%s, country=%s\", record.topic(), record.partition(), record.offset(), record.key(), record.value()); // 省略消息处理逻辑 ... // 记录分区的 offset currentOffsets.put( new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1, \"no metadata\") ); // 最多每处理 1000 条记录就提交一次偏移量 if (count % 1000 == 0) consumer.commitAsync(currentOffsets, null); count++; &#125; &#125; 再均衡监听器消费者在退出和进行分区再均衡之前，如果消费者知道要失去对一个分区的所有权，它可能需要提交最后一个已处理记录的偏移量。KafakConsumer API 可以在消费者新增分区或者失去分区时进行处理，在调用 subscribe() 方法时传入 ConsumerRebalanceListener 对象，该对象有两个方法： public void onPartitionRevoked(Collection partitions): 在消费者停止消费消费后，在再均衡开始前调用。 public void onPartitionAssigned(Collection partitions): 在分区分配给消费者后，在消费者开始读取消息前调用。 下面来看一个的例子，在消费者失去某个分区时提交 offset，以便其他消费者可以接着消费消息并处理： 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 记录分区的 offset 信息Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets = new HashMap&lt;&gt;();class HandleRebalance implements ConsumerRebalanceListener &#123; public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; &#125; // 如果发生再均衡，即将失去分区所有权时提交偏移量。 // 提交的是最近处理过的偏移量，而不是批次中还在处理的最后一个偏移量。 public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; System.out.println(\"Lost partitions in rebalance. Committing current offsets:\" + currentOffsets); consumer.commitSync(currentOffsets); &#125;&#125;// ...try &#123; // 把 ConsumerRebalanceListener 对象传给 subscribe() 方法 consumer.subscribe(topics, new HandleRebalance()); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; currentOffsets.put( new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset()+1, \"no metadata\") ); &#125; consumer.commitAsync(currentOffsets, null); &#125;&#125; catch (WakeupException e) &#123; // ignore&#125; catch (Exception e) &#123; log.error(\"Unexpected error\", e);&#125; finally &#123; try &#123; consumer.commitSync(currentOffsets); &#125; finally &#123; consumer.close(); &#125;&#125; 从指定位移开始消费除了读取最近一次提交的位置开始消费数据，有时候也需要从特定的偏移量处开始读取消息。 如果想从分区起始位置开始消费，可以使用 seekToBeginning(TopicPartition tp)；如果想从分区的最末端消费最新的消息，可以使用 seekToEnd(TopicPartition tp)。Kafka 还支持从指定 offset 处开始消费。最典型的一个是：offset 维护在其他系统（例如数据库）中，并且以其他系统的值为准。 考虑下面的场景：从 Kafka 中读取消息进行处理，最后把结果写入数据库，可能会按如下逻辑处理： 12345678910while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; currentOffsets.put(new TopicPartition(record.topic(), record.partition()), record.offset()); processRecord(record); storeRecordInDB(record); consumer.commitAsync(currentOffsets); &#125;&#125; 看似正确的逻辑要注意的是，在持久化到数据库成功后，提交位移到 Kafka 可能会失败，出现不一致的情况，那么这可能会导致消息会重复处理。对于这种情况，我们需要将持久化到数据库与提交 offset 实现为原子性操作，最简单的做法，在保存记录到数据库的同时保存 offset 信息，在消费者开始消费时指定数据库的 offset 开始消费。 只需要通过 seek() 来指定分区位移开始消费即可： 123456789101112131415161718192021222324252627282930313233343536373839class SaveOffsetsOnRebalance implements ConsumerRebalanceListener &#123; public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; // 在分区被回收前提交数据库事务，保存消费的记录和位移 commitDBTransaction(); &#125; public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; // 在开始消费前，从数据库中获取分区的位移，使用 seek() 指定开始消费的偏移量 for(TopicPartition partition: partitions) consumer.seek(partition, getOffsetFromDB(partition)); &#125;&#125;// ...consumer.subscribe(topics, new SaveOffsetOnRebalance(consumer));// 调用一次 poll() 方怯，让消费者加入到消费者群组里，并获取分配到的分区consumer.poll(0);// 然后马上调用 seek() 方法定位分区的偏移量。 // seek() 方法只更新我们正在使用的位置，在下一次调用 poll() 时就可以获得正确的消息。// 如果 seek() 发生错误, poll() 就会抛出异常。for (TopicPartition partition: consumer.assignment()) consumer.seek(partition, getOffsetFromDB(partition));while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; processRecord(record); // 保存记录结果 storeRecordInDB(record); // 保存位移信息 storeOffsetInDB(record.topic(), record.partition(), record.offset()); &#125; // 提交数据库事务 commitDBTransaction();&#125; 优雅退出一般情况下，在主线程中循环 poll() 消息并进行处理。当需要退出循环时，使用另一个线程调用 consumer.wakeup()，会使得 poll() 抛出 WakeupException。如果主线程正在处理消息，那么在下一次主线程调用 poll() 时会抛出异常。样例代码： 1234567891011121314151617181920212223242526272829303132// 注册 JVM 关闭时的回调，当 JVM 关闭时调用Runtime.getRuntime().addShutdownHook(new Thread() &#123; public void run() &#123; System.out.println(\"Starting exit...\"); // 调用消费者的 wakeup 方法通知主线程退出 consumer.wakeup(); try &#123; // 等待主线程退出 mainThread.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;); ...// 消费主线程try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; // ... &#125; consumer.commitSync(); &#125;&#125; catch (WakeupException e) &#123; // ignore&#125; finally &#123; consumer.close();&#125; 消息序列化Kafka 生产者将对象序列化成字节数组并发送到服务器，消费者需要将字节数组转换成对象（反序列化）。序列化与反序列化需要匹配，与生产者类似，推荐使用 Avro 序列化方式。 使用 Avro 反序列化样例代码如下（与生产者实现类似）： 12345678910111213141516171819202122Properties props = new Properties();props.put(\"bootstrap.servers\", \"broker1:9092,broker2:9092\");props.put(\"group.id\", \"CountryCounter\");props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");props.put(\"value.deserializer\", \"io.confluent.kafka.serializers.KafkaAvroDeserializer\");props.put(\"schema.registry.url\", schemaUrl);String topic = \"customerContacts\"KafkaConsumer consumer = new KafkaConsumer(createConsumerConfig(brokers, groupId, url));consumer.subscribe(Collections.singletonList(topic));System.out.println(\"Reading topic:\" + topic);while (true) &#123; // 这里使用之前生产者使用的Avro生成的Customer类 ConsumerRecords&lt;String, Customer&gt; records = consumer.poll(1000); for (ConsumerRecord&lt;String, Customer&gt; record: records) &#123; System.out.println(\"Current customer name is: \" + record.value().getName()); &#125; consumer.commitSync();&#125; 独立消费者一般情况下都是使用消费者组（即使只有一个消费者）来消费消息的，这样可以在增加或减少消费者时自动进行分区重平衡，这种方式是推荐的。 在知道主题和分区的情况下，也可以使用单个消费者来进行消费，需要实现给消费者分配分区，而不是让消费者订阅主题。代码样例： 123456789101112131415161718// 获取主题下所有的分区List&lt;PartitionInfo&gt; partitionInfos = consumer.partitionsFor(\"topic\");if (partitionInfos != null) &#123; for (PartitionInfo partition : partitionInfos) partitions.add(new TopicPartition(partition.topic(), partition.partition())); // 为消费者指定分区 consumer.assign(partitions); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000); for (ConsumerRecord&lt;String, String&gt; record: records) &#123; // ... &#125; consumer.commitSync(); &#125;&#125; 除了需要主动获取分区以及没有分区重平衡，其他的处理逻辑是一样的。需要注意的是，如果添加了新的分区，这个消费者是感知不到的，需要通过 consumer.partitionsFor() 来重新获取分区。 《Kafka权威指南》","tags":[]},{"title":"从尾到头打印链表","date":"2020-01-05T09:19:25.036Z","path":"2020/01/05/从尾到头打印链表/","text":"这是崔斯特的第一百一十二篇原创文章 努力、奋斗 (๑• . •๑) 从尾到头打印链表题目描述输入一个链表，按链表值从尾到头的顺序返回一个 ArrayList。 思路链表从头到尾输出会比较简单，于是我们很自然的想到把链表中链接节点的指正翻转过来，改变链表的方向，然后就可以从头到尾的输出了。但是该方法会修改原来链表的结构，这应该不是一个好的选择。 在面试中，如果我们打算修改输入的数据，最好先问面试官是否允许修改。 这道题肯定需要遍历链表的，但是遍历肯定是顺序，可是最终需要的是从尾到头，这就是典型的”先进先出“，可以使用栈实现。遍历该链表时，没经过一个节点，便把该节点放入栈中。遍历结束之后，再从栈顶输出所有值。 解法解法一【推荐】遍历链表，每个链表结点值 push 进栈，最后将栈中元素依次 pop 到 list 中。 1234567891011121314151617181920212223242526272829303132333435363738class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;public class Solution &#123; public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; ArrayList&lt;Integer&gt; res = new ArrayList&lt;&gt;(); if (listNode == null) &#123; return res; &#125; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); while (listNode != null) &#123; stack.push(listNode.val); listNode = listNode.next; &#125; while (!stack.isEmpty()) &#123; res.add(stack.pop()); &#125; return res; &#125; public static void main(String[] args) &#123; ListNode listNode = new ListNode(1); listNode.next = new ListNode(2); listNode.next.next = new ListNode(3); listNode.next.next.next = new ListNode(4); listNode.next.next.next.next = new ListNode(5); System.out.println(new Solution().printListFromTailToHead(listNode)); &#125;&#125; 面试官看到使用了栈，会不会手动让你完成栈的实现呢。下面有栈的实现。 解法二【不推荐】利用递归方式： 若不是链表尾结点，继续递归； 若是，添加到 list 中。 这种方式不推荐，当递归层数过多时，容易发生 Stack Overflow。 123456789101112131415161718192021222324252627public class Solution &#123; public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; ArrayList&lt;Integer&gt; res = new ArrayList&lt;&gt;(); if (listNode == null) &#123; return res; &#125; addElement(listNode, res); return res; &#125; private void addElement(ListNode listNode, ArrayList&lt;Integer&gt; res) &#123; if (listNode.next != null) &#123; addElement(listNode.next, res); &#125; res.add(listNode.val); &#125; public static void main(String[] args) &#123; ListNode listNode = new ListNode(1); listNode.next = new ListNode(2); listNode.next.next = new ListNode(3); listNode.next.next.next = new ListNode(4); listNode.next.next.next.next = new ListNode(5); System.out.println(new Solution().printListFromTailToHead(listNode)); &#125;&#125; 解法三通过遍历实现 1234567891011121314151617181920212223242526public class Solution &#123; public void printListFromTailToHead(ListNode listNode) &#123; ListNode pre = null; ListNode next = null; while (listNode != null) &#123; next = listNode.next; listNode.next = pre; pre = listNode; listNode = next; &#125; while (pre != null) &#123; System.out.println(pre.val); pre = pre.next; &#125; &#125; public static void main(String[] args) &#123; ListNode listNode = new ListNode(1); listNode.next = new ListNode(2); listNode.next.next = new ListNode(3); listNode.next.next.next = new ListNode(4); listNode.next.next.next.next = new ListNode(5); new Solution().printListFromTailToHead(listNode); &#125;&#125; 栈研究了下源码，简单实现了下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119import java.util.Arrays;import java.util.EmptyStackException;/** * @author zhujian on 2020/1/5. */public class Stack&lt;E&gt; &#123; /** * 元素数量 */ private int elementCount; /** * 储存数据的数组 */ private Object[] elementData; /** * 自定义最大数组容量 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * 构造方法，默认大小为10 */ public Stack() &#123; this(10); &#125; public Stack(int initialCapacity) &#123; if (initialCapacity &lt; 0) &#123; throw new IllegalArgumentException(\"Illegal Capacity: \" + initialCapacity); &#125; this.elementData = new Object[initialCapacity]; &#125; /** * 入栈 * * @param item * @return */ public E push(E item) &#123; if (elementCount + 1 - elementData.length &gt; 0) &#123; grow(elementCount + 1); &#125; elementData[elementCount++] = item; return item; &#125; /** * 出栈 * * @return */ public E pop() &#123; E item; item = peek(); removeElementAt(elementCount - 1); return item; &#125; public boolean isEmpty() &#123; return elementCount == 0; &#125; /** * 取出数组最右边元素 * * @return */ public E peek() &#123; int len = elementCount; if (len == 0) &#123; throw new EmptyStackException(); &#125; return (E) elementData[len - 1]; &#125; /** * 移除数组最右边元素 * * @param index */ public void removeElementAt(int index) &#123; if (index &gt;= elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + \" &gt;= \" + elementCount); &#125; else if (index &lt; 0) &#123; throw new ArrayIndexOutOfBoundsException(index); &#125; int j = elementCount - index - 1; if (j &gt; 0) &#123; System.arraycopy(elementData, index + 1, elementData, index, j); &#125; elementCount--; elementData[elementCount] = null; &#125; /** * 数组长度不够时，为数组扩大空间 * * @param minCapacity */ private void grow(int minCapacity) &#123; int oldCapacity = elementData.length; int newCapacity = oldCapacity * 2; if (newCapacity &lt; minCapacity) &#123; newCapacity = minCapacity; &#125; if (newCapacity &gt; MAX_ARRAY_SIZE) &#123; if (minCapacity &lt; 0) &#123; throw new OutOfMemoryError(); &#125; newCapacity = (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; elementData = Arrays.copyOf(elementData, newCapacity); &#125;&#125; JDK源码中是这样的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package java.util;publicclass Stack&lt;E&gt; extends Vector&lt;E&gt; &#123; public Stack() &#123; &#125; public E push(E item) &#123; addElement(item); return item; &#125; public synchronized E pop() &#123; E obj; int len = size(); obj = peek(); removeElementAt(len - 1); return obj; &#125; public synchronized E peek() &#123; int len = size(); if (len == 0) throw new EmptyStackException(); return elementAt(len - 1); &#125; public boolean empty() &#123; return size() == 0; &#125; public synchronized int search(Object o) &#123; int i = lastIndexOf(o); if (i &gt;= 0) &#123; return size() - i; &#125; return -1; &#125; private static final long serialVersionUID = 1224463164541339165L;&#125; 细节满满，想要看更多实现就需要去看Vector的源码了。Vector日常使用的太少了，是线程安全的，但实现同步需要很高的花费，因此访问比ArrayList慢。","tags":[]},{"title":"替换空格","date":"2020-01-03T03:03:47.228Z","path":"2020/01/03/替换空格/","text":"这是崔斯特的第一百一十一篇原创文章 努力、奋斗 (๑• . •๑) 替换空格题目描述请实现一个函数，将一个字符串中的每个空格替换成 %20。例如，当字符串为 We Are Happy，则经过替换之后的字符串为 We%20Are%20Happy。 思路解法一其实看到这道题，我最开始想到的就是字符串的replace方法，这显然不是面试官想要的答案，我就去看了下实现源码。 1234public String replace(CharSequence target, CharSequence replacement) &#123; return Pattern.compile(target.toString(), Pattern.LITERAL).matcher( this).replaceAll(Matcher.quoteReplacement(replacement.toString()));&#125; 原来是用正则去匹配再替换的，再去看看正则替换的方法 1234567891011121314public String replaceAll(String replacement) &#123; reset(); boolean result = find(); if (result) &#123; StringBuffer sb = new StringBuffer(); do &#123; appendReplacement(sb, replacement); result = find(); &#125; while (result); appendTail(sb); return sb.toString(); &#125; return text.toString();&#125; 发现用的是StringBuffer，保证线程安全。 解法二好了，下面步入正题，创建 StringBuilder，遍历原字符串，遇到非空格，直接 append 到 StringBuilder 中，遇到空格则将 %20 append 到 StringBuilder 中。 123456789101112131415161718192021public class Solution &#123; public String replaceSpace(String str) &#123; if (str == None || str.length() == 0) &#123; return str; &#125; StringBuilder sb = new StringBuilder(); int len = str.length(); for (int i = 0; i &lt; len; ++i) &#123; char ch = str.charAt(i); sb.append(ch == ' ' ? \"%20\" : ch); &#125; return sb.toString(); &#125; public static void main(String[] args) &#123; String s = \"We Are Happy\"; System.out.println(new Solution().replaceSpace(s)); &#125;&#125; 解法三（推荐）先遍历原字符串，遇到空格，则在原字符串末尾 append 任意两个字符，如两个空格。 用指针 p 指向原字符串末尾，q 指向现字符串末尾，p, q 从后往前遍历，当 p 遇到空格，q 位置依次要 append ‘02%’，若不是空格，直接 append p 指向的字符。 🤔思路扩展在合并两个数组（包括字符串）时，如果从前往后复制每个数字（或字符）需要重复移动数字（或字符）多次，那么我们可以考虑从后往前复制，这样就能减少移动的次数，从而提高效率。 12345678910111213141516171819202122232425262728293031323334public class Solution &#123; public String replaceSpace(String str) &#123; if (str == null || str.length() == 0) &#123; return null; &#125; StringBuilder stringBuilder = new StringBuilder(str); int len = stringBuilder.length(); StringBuilder strBuilder = new StringBuilder(stringBuilder); for (int i = 0; i &lt; len; i++) &#123; if (strBuilder.charAt(i) == ' ') &#123; strBuilder.append(\" \"); &#125; &#125; int p = len - 1; int q = strBuilder.length() - 1; while (p &gt; 0) &#123; char ch = stringBuilder.charAt(p--); if (ch == ' ') &#123; strBuilder.setCharAt(q--, '0'); strBuilder.setCharAt(q--, '2'); strBuilder.setCharAt(q--, '%'); &#125; else &#123; strBuilder.setCharAt(q--, ch); &#125; &#125; return strBuilder.toString(); &#125; public static void main(String[] args) &#123; String s = \"We Are Happy\"; System.out.println(new Solution().replaceSpace(s)); &#125;&#125; Python123456789101112131415161718192021222324252627class Solution(object): def replace_space(self, s): if not isinstance(s, str) or len(s) &lt;= 0 or s is None: return '' space_num = 0 for i in s: if i == ' ': space_num += 1 new_str_len = len(s) + space_num * 2 new_str = new_str_len * [None] index_of_origin, index_of_new = len(s) - 1, new_str_len - 1 while index_of_new &gt;= 0 and index_of_origin &lt;= index_of_new: if s[index_of_origin] == ' ': new_str[index_of_new - 2: index_of_new + 1] = ['%', '2', '0'] index_of_new -= 3 else: new_str[index_of_new] = s[index_of_origin] index_of_new -= 1 index_of_origin -= 1 return ''.join(new_str)if __name__ == '__main__': s = Solution().replace_space('We Are Happy') print(s) 看着都头大","tags":[]},{"title":"MapReduce例子","date":"2019-12-30T12:13:16.755Z","path":"2019/12/30/MapReduce例子/","text":"这是崔斯特的第一百一十篇原创文章 努力、奋斗 最近在学习《Hive编程指南》，尝试动手了第一个MapReduce案例，记录下。 JAVA12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.StringTokenizer;/** * @author zhujian on 2019/12/29. */public class WorldCount &#123; public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; private final static IntWritable ONE = new IntWritable(1); private Text word = new Text(); @Override public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); StringTokenizer tokenizer = new StringTokenizer(line); while (tokenizer.hasMoreTokens()) &#123; word.set(tokenizer.nextToken()); context.write(value, ONE); &#125; &#125; &#125; public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val : values) &#123; sum += val.get(); &#125; context.write(key, new IntWritable(sum)); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = new Job(conf, \"world count\"); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); job.waitForCompletion(true); &#125;&#125; 分别继承Mapper和Reducer这两个方法，重写自己的处理逻辑，最后设置map和reduce。 HQL同样的功能用HQL来写会简单很多： 123456789101112CREATE TABLE docs (line STRING);LOAD DATA INPATH 'docs' OVERWRITE INTO TABLE docs;CREATE TABLE world_counts ASSELECT world, count(1) AS countFROM ( SELECT explode(split(line, '\\s')) AS word FROM docs) wGROUP BY wordORDER BY word; so simple","tags":[]},{"title":"二维数组中的查找","date":"2019-12-29T06:32:25.513Z","path":"2019/12/29/二维数组中的查找/","text":"这是崔斯特的第一百零九篇原创文章 努力、奋斗 (๑• . •๑) 二维数组中的查找题目描述在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 例如下面的二维数组，如果查找7，返回true，查找5，返回false。 1 2 8 9 2 4 9 12 4 7 10 13 6 8 11 15 分析寻找目标数字b，从数组中选一个数字a，如果a=b，结束查找；如果a&gt;b，那么接下来查找的范围就是a的左边或上边；如果a&lt;b，那么接下来搜索范围就是右边或下边。 当我们需要解决一个复杂的问题时，一个很有效的方法就是从一个具体的问题入手。本题可以从查找数字7入手，一步步分析过程。如果我们在二维数组中间随机选一个数字，那么下次搜索范围就是两个区域，而且有重叠。如果从数组的某一个角上开始搜索，就不会遇到这个问题。 比如从数组右上方开始， a=9，b=7，a&gt;b，接着查找左边和上边，由于没有上边，所以只能查找左边，也就是1、2、8这三列 a=8，a&gt;b，接着查找左边和上边，因为没有上边，只能接着找左边，也就是1，2这两列 a=2，a&lt;b，接着就是右边和下边，右边已经查找过，所以接下来就是一直往下查找，也就是1、2这两列的下方 a=4，a&lt;b，接着查找1、2这两列的下面两行 a=7，a=b，结束查找。 解法从二维数组的右上方开始查找： 若元素值等于 target，返回 true； 若元素值大于 target，砍掉这一列，即 --j； 若元素值小于 target，砍掉这一行，即 ++i。 也可以从二维数组的左下方开始查找，以下代码使用左下方作为查找的起点。 注意，不能选择左上方或者右下方的数字，因为这样无法缩小查找的范围。 123456789101112131415161718192021222324252627282930313233public class Solution &#123; public boolean find(int target, int[][] array) &#123; if (array == null) &#123; return false; &#125; int rows = array.length; int columns = array[0].length; int i = rows - 1; int j = 0; while (i &gt;= 0 &amp;&amp; j &lt; columns) &#123; if (array[i][j] == target) &#123; return true; &#125; if (array[i][j] &lt; target) &#123; ++j; &#125; else &#123; --i; &#125; &#125; return false; &#125; public static void main(String[] args) &#123; int[][] arr = new int[][]&#123; &#123;1, 2, 8, 9&#125;, &#123;2, 4, 9, 12&#125;, &#123;4, 7, 10, 13&#125;, &#123;6, 8, 11, 15&#125; &#125;; boolean res = new Solution().find(7, arr); System.out.println(res); &#125;&#125; 如果是我自己去写这道题的话，肯定想不出这样的方法，可能就是遍历所有数字，这肯定不是面试官要问的。看了书中介绍的方法，分析过程确实很重要，一步步去寻找规律。我想起了高中数学的数列，老师常说“数列就是列数，如果不知道规律，就多写几个例子”。对于现在的这种算法题，一定要多试几个例子，从中找到规律。","tags":[]},{"title":"找出数组中重复的数字","date":"2019-12-26T08:44:27.000Z","path":"2019/12/26/找出数组中重复的数字/","text":"这是崔斯特的第一百零八篇原创文章 努力、奋斗 (๑• . •๑) 找出数组中重复的数字题目描述在一个长度为 n 的数组里的所有数字都在 0 到 n-1 的范围内。数组中某些数字是重复的，但不知道有几个数字重复了，也不知道每个数字重复了几次。请找出数组中任意一个重复的数字。例如，如果输入长度为 7 的数组 {2, 3, 1, 0, 2, 5, 3}，那么对应的输出是重复的数字 2 或者 3。 解法解法一排序后，顺序扫描，判断是否有重复，时间复杂度为 O(n²)。 解法二利用哈希表，遍历数组，如果哈希表中没有该元素，则存入哈希表中，否则返回重复的元素。时间复杂度为 O(n)，空间复杂度为 O(n)。 解法三长度为 n，元素的数值范围也为 n，如果没有重复元素，那么数组每个下标对应的值与下标相等。 从头到尾遍历数组，当扫描到下标 i 的数字 nums[i]： 如果等于 i，继续向下扫描； 如果不等于 i，拿它与第 nums[i] 个数进行比较，如果相等，说明有重复值，返回 nums[i]。如果不相等，就把第 i 个数 和第 nums[i] 个数交换。重复这个比较交换的过程。 此算法时间复杂度为 O(n)，因为每个元素最多只要两次交换，就能确定位置。空间复杂度为 O(1)。 12345678910111213141516171819202122232425262728293031323334/** * @author zhujian on 2019/12/26. */public class Solution &#123; public int findDuplicate(int[] array) &#123; if (array.length == 0) &#123; return -1; &#125; for (int i = 0; i &lt; array.length; i++) &#123; while (array[i] != i) &#123; if (array[i] == array[array[i]]) &#123; return array[i]; &#125; swap(array, i, array[i]); System.out.println(Arrays.toString(array)); &#125; &#125; return -1; &#125; private void swap(int[] array, int i, int j) &#123; int t = array[i]; array[i] = array[j]; array[j] = t; &#125; public static void main(String[] args) &#123; int[] arr = new int[]&#123;2, 3, 1, 0, 2, 5, 3&#125;; int res = new Solution().findDuplicate(arr); System.out.println(res); &#125;&#125; 代码中有一个两重循环，但每个数字最多只要交换两次就可以找到属于自己的位置，因此时间复杂度是O(n)。不需要额外分配内存，空间复杂度是O(1)。 Output1234[1, 3, 2, 0, 2, 5, 3][3, 1, 2, 0, 2, 5, 3][0, 1, 2, 3, 2, 5, 3]2 以数组[2, 3, 1, 0, 2, 5, 3]为例来分析找到重复数字的步骤： 数组的第 0 个数字（从 0 开始计数，和数组的下标保持一致）是 2，与它的下标不相等，于是把它和下标为 2 的数字 1 交换。交换之后的数组是[1, 3, 2, 0, 2, 5, 3]。 此时第 0 个数字是 1，仍然与它的下标不相等，继续把它和下标为 1 的数字 3 交换，得到数组[3, 1, 2, 0, 2, 5, 3] 接下来继续交换第 0 个数字 3 和第 3 个数字 0，得到数组[0, 1, 2, 3, 2, 5, 3]。 此时第 0 个数字的数值为 0，接着扫描下一个数字。在接下来的几个数字中，下标为 1，2，3 的三个数字分别为 1，2，3 它们的下标和数值都分别相等，因此不需要做任何操作。 接下来扫描到下标为 4 的数字 2。由于它的数值与它的下标不相等，再比较它和下标为 2 的数字。注意到此时数组中下标为 2 的数字也是 2，也就是数字在下标为 2 和下标为 4 的两个位置都出现了，因此找到一个重复的数字。 本题关键是：在一个长度为 n 的数组里的所有数字都在 0 到 n-1 的范围内并且有重复，这说明数组是可以排序的，把每个数字交换到自己应该在的位置，排序时就能找到重复字段。理想状态下的情况：[0, 1, 2, 3, 4, x, x, x] Python解法更简单 12345678910111213def solution(arr): if len(arr) == 0: return -1 for i in range(len(arr)): while arr[i] != i: tmp = arr[i] if tmp == arr[tmp]: return tmp arr[i], arr[tmp] = arr[tmp], arr[i] print(arr)print(solution([3, 1, 2, 0, 2, 5, 3])) 不修改数组找出重复的数字题目描述在一个长度为 n+1 的数组里的所有数字都在 1 到 n 的范围内，数组中至少有一个数字是重复的，请找出数组中任意一个重复的数字，但不能修改输入的数组。例如，如果输入长度为 8 的数组 {2, 3, 5, 4, 3, 2, 6, 7}，那么对应的输出是重复的数字 2 或者 3。 解法解法一创建长度为 n+1 的辅助数组，把原数组的元素复制到辅助数组中。如果原数组被复制的数是 m，则放到辅助数组第 m 个位置。这样很容易找出重复元素。空间复杂度为 O(n)。 解法二数组元素的取值范围是 [1, n]，对该范围对半划分，分成 [1, middle], [middle+1, n]。计算数组中有多少个(count)元素落在 [1, middle] 区间内，如果 count 大于 middle，那么说明这个范围内有重复元素，否则在另一个范围内。继续对这个范围对半划分，继续统计区间内元素数量。 时间复杂度 O(n * log n)，空间复杂度 O(1)。 注意，此方法无法找出所有重复的元素。如果左右半区同时存在重复值，先算哪个半区就会先得到哪边的重复值。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * @author zhujian on 2019/12/26. */public class Solution &#123; public int findDuplicate(int[] array) &#123; if (array.length == 0) &#123; return -1; &#125; int start = 1; int end = array.length - 1; while (start &lt;= end) &#123; int middle = start + ((end - start) &gt;&gt; 1); int count = countRange(array, start, middle); if (end == start) &#123; if (count &gt; 1) &#123; return start; &#125; else &#123; break; &#125; &#125; else &#123; if (count &gt; (middle - start) + 1) &#123; end = middle; &#125; else &#123; start = middle + 1; &#125; &#125; &#125; return -1; &#125; private int countRange(int[] array, int start, int end) &#123; if (array.length == 0) &#123; return -1; &#125; int count = 0; for (int i : array) &#123; if (i &gt;= start &amp;&amp; i &lt;= end) &#123; count++; &#125; &#125; return count; &#125; public static void main(String[] args) &#123; int[] arr = new int[]&#123;2, 3, 5, 4, 3, 2, 6, 7&#125;; int res = new Solution().findDuplicate(arr); System.out.println(res); &#125;&#125; 按照二分法查找的思路，如果输出长度为n的数组，那么函数countRange将被调用O(log n)次，每次需要O(n)时间，因此总的时间复杂度是O(nlog n) ，空间复杂度是O(1)。","tags":[{"name":"算法","slug":"算法","permalink":"https://zhangslob.github.io/tags/算法/"},{"name":"剑指offer","slug":"剑指offer","permalink":"https://zhangslob.github.io/tags/剑指offer/"}]},{"title":"大数据学习路线","date":"2019-12-06T02:34:57.481Z","path":"2019/12/06/大数据学习路线/","text":"这是崔斯特的第一百零七篇原创文章 努力、奋斗 大数据处理流程 上图是一个简化的大数据处理流程图，大数据处理的主要流程包括数据收集、数据存储、数据处理、数据应用等主要环节。下面我们逐一对各个环节所需要的技术栈进行讲解： 1.1 数据收集大数据处理的第一步是数据的收集。现在的中大型项目通常采用微服务架构进行分布式部署，所以数据的采集需要在多台服务器上进行，且采集过程不能影响正常业务的开展。基于这种需求，就衍生了多种日志收集工具，如 Flume 、Logstash、Kibana 等，它们都能通过简单的配置完成复杂的数据收集和数据聚合。 1.2 数据存储收集到数据后，下一个问题就是：数据该如何进行存储？通常大家最为熟知是 MySQL、Oracle 等传统的关系型数据库，它们的优点是能够快速存储结构化的数据，并支持随机访问。但大数据的数据结构通常是半结构化（如日志数据）、甚至是非结构化的（如视频、音频数据），为了解决海量半结构化和非结构化数据的存储，衍生了 Hadoop HDFS 、KFS、GFS 等分布式文件系统，它们都能够支持结构化、半结构和非结构化数据的存储，并可以通过增加机器进行横向扩展。 分布式文件系统完美地解决了海量数据存储的问题，但是一个优秀的数据存储系统需要同时考虑数据存储和访问两方面的问题，比如你希望能够对数据进行随机访问，这是传统的关系型数据库所擅长的，但却不是分布式文件系统所擅长的，那么有没有一种存储方案能够同时兼具分布式文件系统和关系型数据库的优点，基于这种需求，就产生了 HBase、MongoDB。 1.3 数据分析大数据处理最重要的环节就是数据分析，数据分析通常分为两种：批处理和流处理。 批处理：对一段时间内海量的离线数据进行统一的处理，对应的处理框架有 Hadoop MapReduce、Spark、Flink 等； 流处理：对运动中的数据进行处理，即在接收数据的同时就对其进行处理，对应的处理框架有 Storm、Spark Streaming、Flink Streaming 等。 批处理和流处理各有其适用的场景，时间不敏感或者硬件资源有限，可以采用批处理；时间敏感和及时性要求高就可以采用流处理。随着服务器硬件的价格越来越低和大家对及时性的要求越来越高，流处理越来越普遍，如股票价格预测和电商运营数据分析等。 上面的框架都是需要通过编程来进行数据分析，那么如果你不是一个后台工程师，是不是就不能进行数据的分析了？当然不是，大数据是一个非常完善的生态圈，有需求就有解决方案。为了能够让熟悉 SQL 的人员也能够进行数据的分析，查询分析框架应运而生，常用的有 Hive 、Spark SQL 、Flink SQL、 Pig、Phoenix 等。这些框架都能够使用标准的 SQL 或者 类 SQL 语法灵活地进行数据的查询分析。这些 SQL 经过解析优化后转换为对应的作业程序来运行，如 Hive 本质上就是将 SQL 转换为 MapReduce 作业，Spark SQL 将 SQL 转换为一系列的 RDDs 和转换关系（transformations），Phoenix 将 SQL 查询转换为一个或多个 HBase Scan。 1.4 数据应用数据分析完成后，接下来就是数据应用的范畴，这取决于你实际的业务需求。比如你可以将数据进行可视化展现，或者将数据用于优化你的推荐算法，这种运用现在很普遍，比如短视频个性化推荐、电商商品推荐、头条新闻推荐等。当然你也可以将数据用于训练你的机器学习模型，这些都属于其他领域的范畴，都有着对应的框架和技术栈进行处理，这里就不一一赘述。 1.5 其他框架上面是一个标准的大数据处理流程所用到的技术框架。但是实际的大数据处理流程比上面复杂很多，针对大数据处理中的各种复杂问题分别衍生了各类框架： 单机的处理能力都是存在瓶颈的，所以大数据框架都是采用集群模式进行部署，为了更方便的进行集群的部署、监控和管理，衍生了 Ambari、Cloudera Manager 等集群管理工具； 想要保证集群高可用，需要用到 ZooKeeper ，ZooKeeper 是最常用的分布式协调服务，它能够解决大多数集群问题，包括首领选举、失败恢复、元数据存储及其一致性保证。同时针对集群资源管理的需求，又衍生了 Hadoop YARN ; 复杂大数据处理的另外一个显著的问题是，如何调度多个复杂的并且彼此之间存在依赖关系的作业？基于这种需求，产生了 Azkaban 和 Oozie 等工作流调度框架； 大数据流处理中使用的比较多的另外一个框架是 Kafka，它可以用于消峰，避免在秒杀等场景下并发数据对流处理程序造成冲击； 另一个常用的框架是 Sqoop ，主要是解决了数据迁移的问题，它能够通过简单的命令将关系型数据库中的数据导入到 HDFS 、Hive 或 HBase 中，或者从 HDFS 、Hive 导出到关系型数据库上。 学习路线介绍完大数据框架，接着就可以介绍其对应的学习路线了，主要分为以下几个方面： 2.1 语言基础1. Java大数据框架大多采用 Java 语言进行开发，并且几乎全部的框架都会提供 Java API 。Java 是目前比较主流的后台开发语言，所以网上免费的学习资源也比较多。如果你习惯通过书本进行学习，这里推荐以下入门书籍： 《Java 编程的逻辑》：这里一本国人编写的系统入门 Java 的书籍，深入浅出，内容全面； 《Java 核心技术》：目前最新的是第 10 版，有卷一 和卷二 两册，卷二可以选择性阅读，因为其中很多章节的内容在实际开发中很少用到。 目前大多数框架要求 Java 版本至少是 1.8，这是由于 Java 1.8 提供了函数式编程，使得可以用更精简的代码来实现之前同样的功能，比如你调用 Spark API，使用 1.8 可能比 1.7 少数倍的代码，所以这里额外推荐阅读 《Java 8 实战》 这本书籍。 2. ScalaScala 是一门综合了面向对象和函数式编程概念的静态类型的编程语言，它运行在 Java 虚拟机上，可以与所有的 Java 类库无缝协作，著名的 Kafka 就是采用 Scala 语言进行开发的。 为什么需要学习 Scala 语言 ？ 这是因为当前最火的计算框架 Flink 和 Spark 都提供了 Scala 语言的接口，使用它进行开发，比使用 Java 8 所需要的代码更少，且 Spark 就是使用 Scala 语言进行编写的，学习 Scala 可以帮助你更深入的理解 Spark。同样的，对于习惯书本学习的小伙伴，这里推荐两本入门书籍： 《快学 Scala(第 2 版)》 《Scala 编程 (第 3 版)》 这里说明一下，如果你的时间有限，不一定要学完 Scala 才去学习大数据框架。Scala 确实足够的精简和灵活，但其在语言复杂度上略大于 Java，例如隐式转换和隐式参数等概念在初次涉及时会比较难以理解，所以你可以在了解 Spark 后再去学习 Scala，因为类似隐式转换等概念在 Spark 源码中有大量的运用。 2.2 Linux 基础通常大数据框架都部署在 Linux 服务器上，所以需要具备一定的 Linux 知识。Linux 书籍当中比较著名的是 《鸟哥私房菜》系列，这个系列很全面也很经典。但如果你希望能够快速地入门，这里推荐《Linux 就该这么学》，其网站上有免费的电子书版本。 2.3 构建工具这里需要掌握的自动化构建工具主要是 Maven。Maven 在大数据场景中使用比较普遍，主要在以下三个方面： 管理项目 JAR 包，帮助你快速构建大数据应用程序； 不论你的项目是使用 Java 语言还是 Scala 语言进行开发，提交到集群环境运行时，都需要使用 Maven 进行编译打包； 大部分大数据框架使用 Maven 进行源码管理，当你需要从其源码编译出安装包时，就需要使用到 Maven。 2.4 框架学习1. 框架分类上面我们介绍了很多大数据框架，这里进行一下分类总结： 日志收集框架：Flume 、Logstash、Kibana 分布式文件存储系统：Hadoop HDFS 数据库系统：Mongodb、HBase 分布式计算框架： 批处理框架：Hadoop MapReduce 流处理框架：Storm 混合处理框架：Spark、Flink 查询分析框架：Hive 、Spark SQL 、Flink SQL、 Pig、Phoenix 集群资源管理器：Hadoop YARN 分布式协调服务：Zookeeper 数据迁移工具：Sqoop 任务调度框架：Azkaban、Oozie 集群部署和监控：Ambari、Cloudera Manager 上面列出的都是比较主流的大数据框架，社区都很活跃，学习资源也比较丰富。建议从 Hadoop 开始入门学习，因为它是整个大数据生态圈的基石，其它框架都直接或者间接依赖于 Hadoop 。接着就可以学习计算框架，Spark 和 Flink 都是比较主流的混合处理框架，Spark 出现得较早，所以其应用也比较广泛。 Flink 是当下最火热的新一代的混合处理框架，其凭借众多优异的特性得到了众多公司的青睐。两者可以按照你个人喜好或者实际工作需要进行学习。 图片引用自 ：https://www.edureka.co/blog/hadoop-ecosystem 至于其它框架，在学习上并没有特定的先后顺序，如果你的学习时间有限，建议初次学习时候，同一类型的框架掌握一种即可，比如日志收集框架就有很多种，初次学习时候只需要掌握一种，能够完成日志收集的任务即可，之后工作上有需要可以再进行针对性地学习。 2. 学习资料大数据最权威和最全面的学习资料就是官方文档。热门的大数据框架社区都比较活跃、版本更新迭代也比较快，所以其出版物都明显滞后于其实际版本，基于这个原因采用书本学习不是一个最好的方案。比较庆幸的是，大数据框架的官方文档都写的比较好，内容完善，重点突出，同时都采用了大量配图进行辅助讲解。当然也有一些优秀的书籍历经时间的检验，至今依然很经典，这里列出部分个人阅读过的经典书籍： 《hadoop 权威指南 (第四版)》 2017 年 《Kafka 权威指南》 2017 年 《从 Paxos 到 Zookeeper 分布式一致性原理与实践》 2015 年 《Spark 技术内幕 深入解析 Spark 内核架构设计与实现原理》 2015 年 《Spark.The.Definitive.Guide》 2018 年 《HBase 权威指南》 2012 年 《Hive 编程指南》 2013 年 3. 视频学习资料上面我推荐的都是书籍学习资料，很少推荐视频学习资料，这里说明一下原因：因为书籍历经时间的考验，能够再版的或者豆瓣等平台评价高的证明都是被大众所认可的，从概率的角度上来说，其必然更加优秀，不容易浪费大家的学习时间和精力，所以我个人更倾向于官方文档或者书本的学习方式，而不是视频。因为视频学习资料，缺少一个公共的评价平台和完善的评价机制，所以其质量良莠不齐。但是视频任然有其不可替代的好处，学习起来更直观、印象也更深刻，所以对于习惯视频学习的小伙伴，这里我各推荐一个免费的和付费的视频学习资源，大家按需选择： 免费学习资源：尚硅谷大数据学习路线 —— 下载链接 \\ 在线观看链接 付费学习资源：慕课网 Michael PK 的系列课程 开发工具这里推荐一些大数据常用的开发工具： Java IDE：IDEA 和 Eclipse 都可以。从个人使用习惯而言，更倾向于 IDEA ; VirtualBox：在学习过程中，你可能经常要在虚拟机上搭建服务和集群。VirtualBox 是一款开源、免费的虚拟机管理软件，虽然是轻量级软件，但功能很丰富，基本能够满足日常的使用需求； MobaXterm：大数据的框架通常都部署在服务器上，这里推荐使用 MobaXterm 进行连接。同样是免费开源的，支持多种连接协议，支持拖拽上传文件，支持使用插件扩展； Translate Man：一款浏览器上免费的翻译插件 (谷歌和火狐均支持)。它采用谷歌的翻译接口，准确性非常高，支持划词翻译，可以辅助进行官方文档的阅读。 结语以上就是个人关于大数据的学习心得和路线推荐。本片文章对大数据技术栈做了比较狭义的限定，随着学习的深入，大家也可以把 Python 语言、推荐系统、机器学习等逐步加入到自己的大数据技术栈中。","tags":[]},{"title":"Leetcode-394-字符串解码","date":"2019-11-28T03:46:18.174Z","path":"2019/11/28/Leetcode-394-字符串解码/","text":"这是崔斯特的第一百零六篇原创文章 努力、奋斗 问题给定一个经过编码的字符串，返回它解码后的字符串。 编码规则为: k[encoded_string]，表示其中方括号内部的 encoded_string 正好重复 k 次。注意 k 保证为正整数。 你可以认为输入字符串总是有效的；输入字符串中没有额外的空格，且输入的方括号总是符合格式要求的。 此外，你可以认为原始数据不包含数字，所有的数字只表示重复的次数 k ，例如不会出现像3a或2[4]的输入。 示例 123s = &quot;3[a]2[bc]&quot;, 返回 &quot;aaabcbc&quot;.s = &quot;3[a2[c]]&quot;, 返回 &quot;accaccacc&quot;.s = &quot;2[abc]3[cd]ef&quot;, 返回 &quot;abcabccdcdcdef&quot;. 思路1234567891011121314151617181920212223242526272829303132333435363738394041输入:'3[a2[c]]'初始化栈: 栈nums 存要重复的次数k,栈str 存字符串遍历字符串:指针指向字符'3',为数字num暂存数字3继续遍历,遇到字符'['循环次数num入栈nums，空字符串res入栈strnums: 3 res: ''num置为0，str置空继续遍历,遇到字符'a',为字母空字符串res拼接字母'a',res='a'继续遍历,遇到字符'2',为数字num暂存数字2继续遍历遇到字符'['num入栈nums,res入栈strnums: 3 -&gt; 2 str: '' -&gt; 'a'num置为0，str置空继续遍历,遇到字符'c',为字母空字符串res拼接字母'c',res='c'继续遍历遇到字符']'nums弹出栈顶元素：当前字符串重复次数2res = res*2 = 'cc'str弹出栈顶元素'a'与res拼接并入栈:res = 'a'+'cc'='acc'str: '' -&gt; 'acc'继续遍历遇到字符']'nums弹出栈顶元素：当前字符串重复次数3res = res*3 = 'accaccacc'str弹出栈顶元素空字符串''与res拼接并入栈:res=''+'accaccacc'='accaccacc'str: 'accaccacc'结束返回res 注意： 由于重复次数可能大于10，所以暂存数字时要适当处理，如 num*10+当前数字 在c++里可以直接修改拼接字符，但Java不支持运算符重载，可以借助 StringBuilder 或 StringBuffer 类。 用栈暂存的逻辑与递归基本一致，可以理解为用递归实现栈。 python没有栈这种数据结构，可以用 list() 数组或双端队列 deque() python可以只用一个栈以元组的形式重复次数和字符串，如(num,res) 利用栈JAVA1234567891011121314151617181920212223242526272829303132public class Solution &#123; public String decodeString(String s) &#123; Stack&lt;StringBuilder&gt; str = new Stack&lt;&gt;(); Stack&lt;Integer&gt; nums = new Stack&lt;&gt;(); StringBuilder res = new StringBuilder(); int num = 0; for (char c : s.toCharArray()) &#123; if (c == '[') &#123; str.push(res); nums.push(num); num = 0; res = new StringBuilder(); &#125; else if (c == ']') &#123; StringBuilder stringBuilder = new StringBuilder(); for (int i = nums.pop(); i &gt; 0; i--) &#123; stringBuilder.append(res); &#125; res = str.pop().append(stringBuilder); &#125; else if (c &gt;= '0' &amp;&amp; c &lt;= '9') &#123; num = num * 10 + (c - '0'); &#125; else &#123; res.append(c); &#125; &#125; return res.toString(); &#125; public static void main(String[] args) &#123; String s = \"3[a]2[bc]\"; System.out.println(new Solution().decodeString(s)); &#125;&#125; Python1234567891011121314151617181920class Solution: def decodeString(self, s: str) -&gt; str: stack, res, num = [], '', 0 for i in s: if i.isdigit(): num = num * 10 + int(i) elif i.isalpha(): res += i elif i == '[': stack.append((res, num)) res, num = '', 0 else: last_str, this_num = stack.pop() res = last_str + this_num * res return resc = Solution().decodeString('3[a2[c]]') # aaabcbcprint(c) Go发现一种解法：倒序遍历字符串s，如果不是[则直接入栈；遇到[时，先找出[前边的数字nums表示为k，然后找出编码字符串encodedStr，重复k次入栈，跳过数字继续遍历 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package mainimport ( \"fmt\" \"strconv\" \"strings\")func decodeString(s string) string &#123; stack := make([]string, 0) for i := len(s) - 1; i &gt;= 0; &#123; if s[i] == '[' &#123; nums := make([]string, 0) for j := i - 1; j &gt;= 0; j-- &#123; if s[j] &gt;= '0' &amp;&amp; s[j] &lt;= '9' &#123; nums = append(nums, string(s[j])) &#125; else &#123; break &#125; &#125; reverse(nums) k, _ := strconv.Atoi(strings.Join(nums, \"\")) encodedStr := make([]string, 0) for stack[len(stack)-1] != \"]\" &#123; encodedStr = append(encodedStr, stack[len(stack)-1]) stack = stack[:len(stack)-1] &#125; stack = stack[:len(stack)-1] reverse(encodedStr) for j := 0; j &lt; k; j++ &#123; stack = append(stack, encodedStr...) &#125; i -= len(nums) &#125; else &#123; stack = append(stack, string(s[i])) &#125; i -= 1 &#125; reverse(stack) return strings.Join(stack, \"\")&#125;func reverse(arr []string) &#123; length := len(arr) for i := 0; i &lt; length/2; i++ &#123; arr[i], arr[length-i-1] = arr[length-i-1], arr[i] &#125;&#125;func main() &#123; s := \"3[a]2[bc]\" r := decodeString(s) fmt.Println(r)&#125; 惊了惊了，这么牛逼 利用递归JAVA将 s.length() 的值以参数传递，减少重复调用 length() 造成的时间损耗 123456789101112131415161718192021222324252627282930313233public class Solution &#123; private int i = -1; public String decodeString(String s) &#123; return dfs(s.toCharArray(), s.length()).toString(); &#125; private StringBuilder dfs(char[] chars, int len) &#123; int num = 0; StringBuilder str = new StringBuilder(); while (++i &lt; len) &#123; if (chars[i] &gt;= '0' &amp;&amp; chars[i] &lt;= '9') &#123; num = num * 10 + (chars[i] - '0'); &#125; else if (chars[i] == '[') &#123; StringBuilder tmp = dfs(chars, len); while (--num &gt;= 0) &#123; str.append(tmp); &#125; num = 0; &#125; else if (chars[i] == ']') &#123; return str; &#125; else &#123; str.append(chars[i]); &#125; &#125; return str; &#125; public static void main(String[] args) &#123; String s = \"3[a]2[bc]\"; System.out.println(new Solution().decodeString(s)); &#125;&#125; Python1234567891011121314151617181920212223class Solution: i = -1 def decodeString(self, s: str) -&gt; str: res, num = '', 0 while self.i &lt; len(s) - 1: self.i += 1 if s[self.i].isdigit(): num = num * 10 + int(s[self.i]) elif s[self.i].isalpha(): res += s[self.i] elif s[self.i] == '[': res += self.decodeString(s) * num num = 0 else: return res return resif __name__ == '__main__': assert Solution().decodeString('3[a]2[bc]') == 'aaabcbc' assert Solution().decodeString('3[a2[c]]') == 'accaccacc' assert Solution().decodeString('2[abc]3[cd]ef') == 'abcabccdcdcdef'","tags":[]},{"title":"抓包：淘宝、支付宝","date":"2019-11-27T12:05:52.084Z","path":"2019/11/27/抓包：淘宝、支付宝/","text":"这是崔斯特的第一百零五篇原创文章 Frida抓包技巧 对于淘宝、支付宝之类的App，想要直接抓包是不可能的，可以使用 frida 实现抓包。 Frida介绍Frida是一款基于python + javascript 的hook框架，通杀android\\ios\\linux\\win\\osx等各平台，由于是基于脚本的交互，因此相比xposed和substrace cydia更加便捷。Frida的官网为：http://www.frida.re/ 首先要保证你的android手机已经root。通过pip安装frida: 1pip install frida adb连接手机，到 https://github.com/frida/frida/releases 下载frida-server，根据情况选择android-arm 版本，安卓手机一般是是下载android-arm和android-arm64这两种。 push到android手机上并且运行，需要取得手机root权限 123456adb push frida-server /data/local/tmp/adb shellsucd /data/local/tmp/chmod 777 frida-server./frida-server 到此为止环境已经搭建完毕。 编写抓包脚本12345678910111213141516171819202122232425262728293031323334import sysimport fridardev = frida.get_usb_device()# session = rdev.attach('com.taobao.etao')session = rdev.attach('com.taobao.taobao')# session = rdev.attach('com.tmall.wireless')scr = \"\"\"Java.perform(function () &#123; var SwitchConfig = Java.use('mtopsdk.mtop.global.SwitchConfig'); SwitchConfig.isGlobalSpdySwitchOpen.overload().implementation = function()&#123; var ret = this.isGlobalSpdySwitchOpen.apply(this, arguments); console.log(\"isGlobalSpdySwitchOpenl \"+ret) return false &#125; &#125;)\"\"\"script = session.create_script(scr)def on_message(message, data): if message['type'] == 'send': print(\"[*] &#123;0&#125;\".format(message['payload'])) else: print(message)script.on(\"message\", on_message)script.load()sys.stdin.read() 运行该脚本，打开淘宝就可以抓包了。要注入的进程名已经写好了： com.taobao.etao 是一淘 com.taobao.taobao是淘宝 com.tmall.wireless 是天猫 想要抓淘宝App数据还有很多步要走，还有x-sign和x-min-wua等着你。(^o^)/~","tags":[]},{"title":"Leetcode-581-最短无序连续子数组","date":"2019-11-26T02:41:36.925Z","path":"2019/11/26/Leetcode-581-最短无序连续子数组/","text":"这是崔斯特的第一百零四篇原创文章 努力、奋斗 题目详述给定一个整数数组，你需要寻找一个连续的子数组，如果对这个子数组进行升序排序，那么整个数组都会变为升序排序。 你找到的子数组应是最短的，请输出它的长度。 示例 1: 123输入: [2, 6, 4, 8, 10, 9, 15]输出: 5解释: 你只需要对 [6, 4, 8, 10, 9] 进行升序排序，那么整个表都会变为升序排序。 说明 : 输入的数组长度范围在 [1, 10,000]。 输入的数组可能包含重复元素 ，所以升序的意思是&lt;=。 题目详解方法一 将原数组拷贝一份，然后对拷贝后的数组进行排序。 对比原数组和排序后的数组，除去前面一部分和后面一部分相同的元素，剩余区间的长度就是结果。 时间复杂度为 O(nlogn)。 JAVA123456789101112131415161718192021public class Solution &#123; public int findUnsortedSubarray(int[] nums) &#123; int[] sortedNums = Arrays.copyOf(nums, nums.length); Arrays.sort(sortedNums); int i = 0; int j = sortedNums.length - 1; while (i &lt;= j &amp;&amp; nums[i] == sortedNums[i]) &#123; ++i; &#125; while (i &lt;= j &amp;&amp; nums[j] == sortedNums[j]) &#123; --j; &#125; return j - i + 1; &#125; public static void main(String[] args) &#123; int[] nums = new int[]&#123;2, 6, 4, 8, 10, 9, 15&#125;; int i = new Solution().findUnsortedSubarray(nums); System.out.println(i); &#125;&#125; Go1234567891011121314151617181920212223242526272829package mainimport ( \"fmt\" \"sort\")func findUnsortedSubarray(nums []int) int &#123; sortedNum := make([]int, len(nums)) copy(sortedNum, nums) sort.Ints(sortedNum) i, j := 0, len(nums)-1 for i &lt;= j &amp;&amp; nums[i] == sortedNum[i] &#123; i++ &#125; for i &lt;= j &amp;&amp; nums[j] == sortedNum[j] &#123; j-- &#125; if i &gt;= j &#123; return 0 &#125; return j - i + 1&#125;func main() &#123; nums := []int&#123;2, 1&#125; n := findUnsortedSubarray(nums) fmt.Println(n)&#125; Python1234567891011121314class Solution: def findUnsortedSubarray(self, nums): sortedarray = sorted(nums) i, j = 0, len(nums) - 1 while i &lt;= j and nums[i] == sortedarray[i]: i += 1 while i &lt;= j and nums[j] == sortedarray[j]: j -= 1 return j - i + 1if __name__ == '__main__': s = Solution().findUnsortedSubarray([2, 1]) print(s) 方法二 遍历数组，同时进行从头到尾，从尾到头的遍历，分别找出无序连续子数组区间的右边和左边节点； 右边点，是从左到右不递增的点， 左边点，是从右到左不递减的点， 两点之间的距离就是所求值 JAVA12345678910111213141516171819202122232425262728293031323334public class Solution &#123; public int findUnsortedSubarray(int[] nums) &#123; int n = nums.length; if (n == 0) &#123; return 0; &#125; int start = n - 1; int end = 0; int max = nums[0]; int min = nums[n - 1]; for (int i = 0; i &lt; n; i++) &#123; if (nums[i] &gt;= max) &#123; max = nums[i]; &#125; else &#123; end = i; &#125; if (nums[n - i - 1] &lt;= min) &#123; min = nums[n - i - 1]; &#125; else &#123; start = n - i - 1; &#125; &#125; if (start &gt;= end) &#123; return 0; &#125; return end - start + 1; &#125; public static void main(String[] args) &#123; int[] nums = new int[]&#123;2, 6, 4, 8, 10, 9, 15&#125;; int i = new Solution().findUnsortedSubarray(nums); System.out.println(i); &#125;&#125; Go12345678910111213141516171819202122232425262728293031323334package mainimport \"fmt\"func findUnsortedSubarray(nums []int) int &#123; n := len(nums) if n == 0 &#123; return 0 &#125; start, end := n-1, 0 max, min := nums[0], nums[n-1] for i := 0; i &lt; n; i++ &#123; if nums[i] &gt;= max &#123; max = nums[i] &#125; else &#123; end = i &#125; if nums[n-i-1] &lt;= min &#123; min = nums[n-i-1] &#125; else &#123; start = n - i - 1 &#125; &#125; if start &gt;= end &#123; return 0 &#125; return end - start + 1&#125;func main() &#123; nums := []int&#123;2, 1&#125; n := findUnsortedSubarray(nums) fmt.Println(n)&#125; Python12345678910111213141516171819202122class Solution: def findUnsortedSubarray(self, nums): n = len(nums) start, end = 0, n - 1 ma, mi = nums[start], nums[end] for i in range(n): if ma &lt;= nums[i]: ma = nums[i] else: start = i if mi &gt;= nums[n - i - 1]: mi = nums[n - i - 1] else: end = n - i - 1 if start &lt;= end: return 0 return start - end + 1if __name__ == '__main__': s = Solution().findUnsortedSubarray([2, 6, 4, 8, 10, 9, 15]) print(s) 等我提交之后，发现排在第一的解法比我快很多，我当时就呵呵呢 123456789101112131415161718class Solution: def findUnsortedSubarray(self, nums): n = len(nums) max_num = nums[0] right = 0 for i in range(n): if nums[i] &gt;= max_num: max_num = nums[i] else: right = i left = n min_num = nums[-1] for i in range(n - 1, -1, -1): if nums[i] &lt;= min_num: min_num = nums[i] else: left = i return right - left + 1 if right - left &gt; 0 else 0","tags":[]},{"title":"Flink介绍","date":"2019-11-24T09:17:09.814Z","path":"2019/11/24/Flink介绍/","text":"这是崔斯特的第一百零三篇原创文章 Flink读书笔记（一） 最近在看一本书，《Flink原理、实战与性能优化》，记录下重点。 Flink是什么Apache Flink 是一个框架和分布式处理引擎，用于在无边界和有边界数据流上进行有状态的计算。Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。 Flink优势 同时支持高吞吐、低延迟、高性能。Spark或Storm不能同时支持这三种。 支持事件事件（Event Time）概念。大多数框架仅支持系统时间（Process Time）。 支持有状态的计算。在流失计算中，将算子的中间计算结果保存在内存或磁盘中。 支持高度灵活的窗口（Window）操作。 基于轻量分布式快照（SnapShot）实现的容错。任务出现任务问题都可以从CheckPoints中自动恢复。 基于JVM实现独立的内存管理。Flink实现自身内存管理机制，通过序列化/反序列化将所有数据转化为二进制储存在内存中。 Save Points（保存点）。Flink将任务执行的快照保存在储存介质上。（和第5点类似）。 Flink应用场景 实时智能推荐 复杂事件处理 实时欺诈检测 实时数仓与ETL 流数据处理 实时报表分析 Flink组件栈Flink作为一个软件堆栈，是一个分层系统。堆栈的不同层相互叠加，并提高它们接受的程序表示的抽象级别: 运行时层以JobGraph的形式接收程序。JobGraph是一个通用的并行数据流，其中包含使用和生成数据流的任意任务。 DataStream API和DataSet API都通过单独的编译过程生成JobGraphs。数据集API使用优化器来确定程序的最佳计划，而DataStream API使用流构建器。 JobGraph是根据Flink中可用的各种部署选项执行的(例如，本地、远程、Yarn等) Connector层所能对接的技术更是丰富多样，将不同类型、不同来源的数据介入到Flink组件栈中。 Flink架构图当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。 Client 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境连通即可）。提交 Job 后，Client 可以结束进程（Streaming的任务），也可以不结束并等待结果返回。 JobManager 主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。 TaskManager 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理。 可以看到 Flink 的任务调度是多线程模型，并且不同Job/Task混合在一个 TaskManager 进程中。虽然这种方式可以有效提高 CPU 利用率，但是个人不太喜欢这种设计，因为不仅缺乏资源隔离机制，同时也不方便调试。类似 Storm 的进程模型，一个JVM 中只跑该 Job 的 Tasks 实际应用中更为合理。","tags":[]},{"title":"Leetcode 287. 寻找重复数","date":"2019-11-21T05:53:36.612Z","path":"2019/11/21/Leetcode-287-寻找重复数/","text":"这是崔斯特的第一百零二篇原创文章 努力、奋斗 题目描述给定一个包含 n + 1 个整数的数组 nums，其数字都在 1 到 n 之间（包括 1 和 n），可知至少存在一个重复的整数。假设只有一个重复的整数，找出这个重复的数。 示例1： 12输入: [1,3,4,2,2]输出: 2 示例2： 12输入: [3,1,3,4,2]输出: 3 说明： 1234不能更改原数组（假设数组是只读的）。只能使用额外的 O(1) 的空间。时间复杂度小于 O(n2) 。数组中只有一个重复的数字，但它可能不止重复出现一次。 解题思路二分法思路是采用了二分法+抽屉原理。首先解释一下为什么用二分法，因为O(n2)时间复杂度不能A，所以往下应该是n*logn，很容易联想到二分法，因为其复杂度为logn。 抽屉原理是说假设你有11个苹果，要放进10个抽屉，那么至少有一个抽屉里是有两个苹果的。 对应到这题，1~n的n+1个数字，有1个数字会至少重复两次。 比如取数组为｛1，2，2，3，4，5｝，一共6个数，范围是1~5，其中位数应该是（5+1）/2 = 3，那么，如果小于等于3的数的个数如果超过了3，那么重复的数字一定出现在[1，3]之间，否则出现在[4，5]之间。以该数组为例，中位数为3，小于等于3的数一共有4个，大于3的数有两个，所以重复的数字在[1,3]之间。 JAVA123456789101112131415161718192021public class Solution &#123; public int findDuplicate(int[] nums) &#123; int start = 1; int end = nums.length - 1; while (start &lt; end) &#123; int mid = (end + start) / 2; int count = 0; for (int i : nums) &#123; if (i &lt;= mid) &#123; count++; &#125; &#125; if (count &gt; mid) &#123; end = mid; &#125; else &#123; start = mid + 1; &#125; &#125; return end; &#125;&#125; Python1234567891011class Solution: def findDuplicate(self, nums: List[int]) -&gt; int: left, right = 1, len(nums) - 1 while left &lt; right: mid = (left + right) // 2 count = sum(i &lt;= mid for i in nums) if count &gt; mid: right = mid else: left = mid + 1 return right Go123456789101112131415161718func findDuplicate(nums []int) int &#123; start, end := 1, len(nums)-1 for start &lt; end &#123; mid := (start + end) / 2 count := 0 for _, v := range nums &#123; if v &lt;= mid &#123; count++ &#125; &#125; if count &gt; mid &#123; end = mid &#125; else &#123; start = mid + 1 &#125; &#125; return start&#125; 快慢指针数组的索引与存储的数值之间形成了特殊链表。 如果存在重复的数，因为数组大小是 n+1，数字范围是1~n，所以该链表存在环。 环的入口即为结果。 123456789101112131415161718class Solution &#123; public int findDuplicate(int[] nums) &#123; // 快慢指针 int fast = nums[0]; int low = nums[0]; do&#123; low = nums[low]; fast = nums[nums[fast]]; &#125;while(fast != low); int step = nums[0]; // 寻找环链表的入口，即为结果 while(step != low)&#123; step = nums[step]; low = nums[low]; &#125; return low; &#125;&#125; 现在还没看懂这种方法","tags":[]},{"title":"算法","date":"2019-11-12T13:47:02.238Z","path":"2019/11/12/算法/","text":"java基础 (๑• . •๑) 1这是崔斯特的第一百零一篇原创文章 算法 - Algorithms 排序算法：快速排序、归并排序、计数排序 搜索算法：回溯、递归、剪枝技巧 图论：最短路、最小生成树、网络流建模 动态规划：背包问题、最长子序列、计数问题 基础技巧：分治、倍增、二分、贪心 数据结构 - Data Structures 数组与链表：单 / 双向链表、跳舞链 栈与队列 树与图：最近公共祖先、并查集 哈希表 堆：大 / 小根堆、可并堆 字符串：字典树、后缀树","tags":[]},{"title":"Java集合框架常见面试题","date":"2019-11-11T08:21:46.065Z","path":"2019/11/11/Java集合框架常见面试题/","text":"java基础知识 (๑• . •๑) 1这是崔斯特的第一百零一篇原创文章 说说List,Set,Map三者的区别 List(对付顺序的好帮手)： List接口存储一组不唯一（可以有多个元素引用相同的对象），有序的对象 Set(注重独一无二的性质): 不允许重复的集合。不会有多个元素引用相同的对象。 Map(用Key来搜索的专家): 使用键值对存储。Map会维护与Key有关联的值。两个Key可以引用相同的对象，但Key不能重复，典型的Key是String类型，但也可以是任何对象。 Arraylist 与 LinkedList 区别 1. 是否保证线程安全： ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全； 2. 底层数据结构： Arraylist 底层使用的是 Object 数组；LinkedList 底层使用的是 双向链表 数据结构（JDK1.6之前为循环链表，JDK1.7取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！） 3. 插入和删除是否受元素位置的影响： ① ArrayList 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行add(E e)方法的时候， ArrayList 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element)）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。 ② LinkedList 采用链表存储，所以对于add(E e)方法的插入，删除元素时间复杂度不受元素位置的影响，近似 O（1），如果是要在指定位置i插入和删除元素的话（(add(int index, E element)） 时间复杂度应为o(n))因为需要新创立一个新的链表，复制前i-1个元素并在第i位加入新的元素，最后附上n-i个元素。 4. 是否支持快速随机访问： LinkedList 不支持高效的随机元素访问，而 ArrayList 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。 5. 内存空间占用： ArrayList的空间浪费主要体现在在list列表的结尾会预留一定的容量空间，而LinkedList的空间花费则体现在它的每一个元素都需要消耗比ArrayList更多的空间（因为要存放直接后继和直接前驱以及数据）。 补充内容:RandomAccess接口12public interface RandomAccess &#123;&#125; 查看源码我们发现实际上 RandomAccess 接口中什么都没有定义。所以，在我看来 RandomAccess 接口不过是一个标识罢了。标识什么？ 标识实现这个接口的类具有随机访问功能。 在 binarySearch（）方法中，它要判断传入的list 是否 RamdomAccess 的实例，如果是，调用indexedBinarySearch（）方法，如果不是，那么调用iteratorBinarySearch（）方法 1234567public static &lt;T&gt;int binarySearch(List&lt;? extends Comparable&lt;? super T&gt;&gt; list, T key) &#123; if (list instanceof RandomAccess || list.size()&lt;BINARYSEARCH_THRESHOLD) return Collections.indexedBinarySearch(list, key); else return Collections.iteratorBinarySearch(list, key);&#125; ArrayList 实现了 RandomAccess 接口， 而 LinkedList 没有实现。为什么呢？我觉得还是和底层数据结构有关！ArrayList 底层是数组，而 LinkedList 底层是链表。数组天然支持随机访问，时间复杂度为 O（1），所以称为快速随机访问。链表需要遍历到特定位置才能访问特定位置的元素，时间复杂度为 O（n），所以不支持快速随机访问。，ArrayList 实现了 RandomAccess 接口，就表明了他具有快速随机访问功能。 RandomAccess 接口只是标识，并不是说 ArrayList 实现 RandomAccess 接口才具有快速随机访问功能的！ 下面再总结一下 list 的遍历方式选择： 实现了 RandomAccess 接口的list，优先选择普通 for 循环 ，其次 foreach, 未实现 RandomAccess接口的list，优先选择iterator遍历（foreach遍历底层也是通过iterator实现的,），大size的数据，千万不要使用普通for循环 双向链表和双向循环链表双向链表： 包含两个指针，一个prev指向前一个节点，一个next指向后一个节点。 双向循环链表： 最后一个节点的 next 指向head，而 head 的prev指向最后一个节点，构成一个环。 ArrayList 与 Vector 区别Vector类的所有方法都是同步的。可以由两个线程安全地访问一个Vector对象、但是一个线程访问Vector的话代码要在同步操作上耗费大量的时间。 Arraylist不是同步的，所以在不需要保证线程安全时建议使用Arraylist。 ArrayList 的扩容机制详见笔主的这篇文章:通过源码一步一步分析ArrayList 扩容机制 #HashMap 和 Hashtable 的区别 线程是否安全： HashMap 是非线程安全的，HashTable 是线程安全的；HashTable 内部的方法基本都经过synchronized 修饰。（如果你要保证线程安全的话就使用 ConcurrentHashMap 吧！）； 效率： 因为线程安全的问题，HashMap 要比 HashTable 效率高一点。另外，HashTable 基本被淘汰，不要在代码中使用它； 对Null key 和Null value的支持： HashMap 中，null 可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为 null。。但是在 HashTable 中 put 进的键值只要有一个 null，直接抛出 NullPointerException。 初始容量大小和每次扩充容量大小的不同 ： ①创建时如果不指定容量初始值，Hashtable 默认的初始大小为11，之后每次扩充，容量变为原来的2n+1。HashMap 默认的初始化大小为16。之后每次扩充，容量变为原来的2倍。②创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为2的幂次方大小（HashMap 中的tableSizeFor()方法保证，下面给出了源代码）。也就是说 HashMap 总是使用2的幂作为哈希表的大小,后面会介绍到为什么是2的幂次方。 底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。 HashMap 中带有初始容量的构造函数： 123456789101112131415public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125; public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125; 下面这个方法保证了 HashMap 总是使用2的幂作为哈希表的大小。 123456789101112/** * Returns a power of two size for the given target capacity. */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; HashMap 和 HashSet区别如果你看过 HashSet 源码的话就应该知道：HashSet 底层就是基于 HashMap 实现的。（HashSet 的源码非常非常少，因为除了 clone()、writeObject()、readObject()是 HashSet 自己不得不实现之外，其他方法都是直接调用 HashMap 中的方法。 HashMap HashSet 实现了Map接口 实现Set接口 存储键值对 仅存储对象 调用 put（）向map中添加元素 调用 add（）方法向Set中添加元素 HashMap使用键（Key）计算Hashcode HashSet使用成员对象来计算hashcode值，对于两个对象来说hashcode可能相同，所以equals()方法用来判断对象的相等性， HashSet如何检查重复当你把对象加入HashSet时，HashSet会先计算对象的hashcode值来判断对象加入的位置，同时也会与其他加入的对象的hashcode值作比较，如果没有相符的hashcode，HashSet会假设对象没有重复出现。但是如果发现有相同hashcode值的对象，这时会调用equals（）方法来检查hashcode相等的对象是否真的相同。如果两者相同，HashSet就不会让加入操作成功。（摘自我的Java启蒙书《Head fist java》第二版） hashCode（）与equals（）的相关规定： 如果两个对象相等，则hashcode一定也是相同的 两个对象相等,对两个equals方法返回true 两个对象有相同的hashcode值，它们也不一定是相等的 综上，equals方法被覆盖过，则hashCode方法也必须被覆盖 hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写hashCode()，则该class的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）。 ==与equals的区别 ==是判断两个变量或实例是不是指向同一个内存空间 equals是判断两个变量或实例所指向的内存空间的值是不是相同 ==是指对内存地址进行比较 equals()是对字符串的内容进行比较 ==指引用是否相同 equals()指的是值是否相同 HashMap的底层实现JDK1.8之前JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) &amp; hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。 所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。 JDK 1.8 HashMap 的 hash 方法源码: JDK 1.8 的 hash方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。 1234567 static final int hash(Object key) &#123; int h; // key.hashCode()：返回散列值也就是hashcode // ^ ：按位异或 // &gt;&gt;&gt;:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 对比一下 JDK1.7的 HashMap 的 hash 方法源码. 12345678static int hash(int h) &#123; // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; 相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。 所谓 “拉链法” 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。 JDK1.8之后相比于之前的版本， JDK1.8之后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。 TreeMap、TreeSet以及JDK1.8之后的HashMap底层都用到了红黑树。红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。 推荐阅读： 《Java 8系列之重新认识HashMap》 ：https://zhuanlan.zhihu.com/p/21673805 HashMap 的长度为什么是2的幂次方为了能让 HashMap 存取高效，尽量较少碰撞，也就是要尽量把数据分配均匀。我们上面也讲到了过了，Hash 值的范围值-2147483648到2147483647，前后加起来大概40亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个40亿长度的数组，内存是放不下的。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。这个数组下标的计算方法是“ (n - 1) &amp; hash”。（n代表数组长度）。这也就解释了 HashMap 的长度为什么是2的幂次方。 这个算法应该如何设计呢？ 我们首先可能会想到采用%取余的操作来实现。但是，重点来了：“取余(%)操作中如果除数是2的幂次则等价于与其除数减一的与(&amp;)操作（也就是说 hash%length==hash&amp;(length-1)的前提是 length 是2的 n 次方；）。” 并且 采用二进制位操作 &amp;，相对于%能够提高运算效率，这就解释了 HashMap 的长度为什么是2的幂次方。 HashMap 多线程操作导致死循环问题主要原因在于 并发下的Rehash 会造成元素之间会形成一个循环链表。不过，jdk 1.8 后解决了这个问题，但是还是不建议在多线程下使用 HashMap,因为多线程下使用 HashMap 还是会存在其他问题比如数据丢失。并发环境下推荐使用 ConcurrentHashMap 。 详情请查看：https://coolshell.cn/articles/9606.html","tags":[]},{"title":"Java疑难点","date":"2019-11-11T05:57:44.804Z","path":"2019/11/11/Java疑难点/","text":"java基础知识 (๑• . •๑) 1这是崔斯特的第一百篇原创文章 正确使用 equals 方法Object的equals方法容易抛空指针异常，应使用常量或确定有值的对象来调用 equals。 举个例子： 1234567// 不能使用一个值为null的引用类型变量来调用非静态方法，否则会抛出异常String str = null;if (str.equals(\"SnailClimb\")) &#123; ...&#125; else &#123; ..&#125; 运行上面的程序会抛出空指针异常，但是我们把第二行的条件判断语句改为下面这样的话，就不会抛出空指针异常，else 语句块得到执行。： 1\"SnailClimb\".equals(str); // false 不过更推荐使用 java.util.Objects#equals(JDK7 引入的工具类)。 1Objects.equals(null,\"SnailClimb\");// false 我们看一下java.util.Objects#equals的源码就知道原因了。 1234public static boolean equals(Object a, Object b) &#123; // 可以避免空指针异常。如果a==null的话此时a.equals(b)就不会得到执行，避免出现空指针异常。 return (a == b) || (a != null &amp;&amp; a.equals(b)); &#125; 注意： Reference:Java中equals方法造成空指针异常的原因及解决方案 每种原始类型都有默认值一样，如int默认值为 0，boolean 的默认值为 false，null 是任何引用类型的默认值，不严格的说是所有 Object 类型的默认值。 可以使用 == 或者 != 操作来比较null值，但是不能使用其他算法或者逻辑操作。在Java中null == null将返回true。 不能使用一个值为null的引用类型变量来调用非静态方法，否则会抛出异常 整型包装类值的比较所有整型包装类对象值的比较必须使用equals方法。 先看下面这个例子： 1234567Integer x = 3;Integer y = 3;System.out.println(x == y);// trueInteger a = new Integer(3);Integer b = new Integer(3);System.out.println(a == b);//falseSystem.out.println(a.equals(b));//true 当使用自动装箱方式创建一个Integer对象时，当数值在-128 ~127时，会将创建的 Integer 对象缓存起来，当下次再出现该数值时，直接从缓存中取出对应的Integer对象。所以上述代码中，x和y引用的是相同的Integer对象。 BigDecimal 的用处《阿里巴巴Java开发手册》中提到：浮点数之间的等值判断，基本数据类型不能用==来比较，包装数据类型不能用 equals 来判断。 具体原理和浮点数的编码方式有关，这里就不多提了，我们下面直接上实例： 12345float a = 1.0f - 0.9f;float b = 0.9f - 0.8f;System.out.println(a);// 0.100000024System.out.println(b);// 0.099999964System.out.println(a == b);// false 具有基本数学知识的我们很清楚的知道输出并不是我们想要的结果（精度丢失），我们如何解决这个问题呢？一种很常用的方法是：使用使用 BigDecimal 来定义浮点数的值，再进行浮点数的运算操作。 123456BigDecimal a = new BigDecimal(\"1.0\");BigDecimal b = new BigDecimal(\"0.9\");BigDecimal c = new BigDecimal(\"0.8\");BigDecimal x = a.subtract(b);// 0.1BigDecimal y = b.subtract(c);// 0.1System.out.println(x.equals(y));// true BigDecimal 的大小比较a.compareTo(b) : 返回 -1 表示小于，0 表示 等于， 1表示 大于。 123BigDecimal a = new BigDecimal(\"1.0\");BigDecimal b = new BigDecimal(\"0.9\");System.out.println(a.compareTo(b));// 1 BigDecimal 保留几位小数通过 setScale方法设置保留几位小数以及保留规则。保留规则有挺多种，不需要记，IDEA会提示。 123BigDecimal m = new BigDecimal(\"1.255433\");BigDecimal n = m.setScale(3,BigDecimal.ROUND_HALF_DOWN);System.out.println(n);// 1.255 BigDecimal 的使用注意事项注意：我们在使用BigDecimal时，为了防止精度丢失，推荐使用它的 BigDecimal(String) 构造方法来创建对象。《阿里巴巴Java开发手册》对这部分内容也有提到如下图所示。 总结BigDecimal 主要用来操作（大）浮点数，BigInteger 主要用来操作大整数（超过 long 类型）。 BigDecimal 的实现利用到了 BigInteger, 所不同的是 BigDecimal 加入了小数位的概念 基本数据类型与包装数据类型的使用标准Reference:《阿里巴巴Java开发手册》 【强制】所有的 POJO 类属性必须使用包装数据类型。 【强制】RPC 方法的返回值和参数必须使用包装数据类型。 【推荐】所有的局部变量使用基本数据类型。 比如我们如果自定义了一个Student类,其中有一个属性是成绩score,如果用Integer而不用int定义,一次考试,学生可能没考,值是null,也可能考了,但考了0分,值是0,这两个表达的状态明显不一样. 说明 :POJO 类属性没有初值是提醒使用者在需要使用时，必须自己显式地进行赋值，任何 NPE 问题，或者入库检查，都由使用者来保证。 正例 : 数据库的查询结果可能是 null，因为自动拆箱，用基本数据类型接收有 NPE 风险。 反例 : 比如显示成交总额涨跌情况，即正负 x%，x 为基本数据类型，调用的 RPC 服务，调用不成功时，返回的是默认值，页面显示为 0%，这是不合理的，应该显示成中划线。所以包装数据类型的 null 值，能够表示额外的信息，如:远程调用失败，异常退出。 Arrays.asList()使用指南最近使用Arrays.asList()遇到了一些坑，然后在网上看到这篇文章：Java Array to List Examples 感觉挺不错的，但是还不是特别全面。所以，自己对于这块小知识点进行了简单的总结。 简介Arrays.asList()在平时开发中还是比较常见的，我们可以使用它将一个数组转换为一个List集合。 1234String[] myArray = &#123; \"Apple\", \"Banana\", \"Orange\" &#125;； List&lt;String&gt; myList = Arrays.asList(myArray);//上面两个语句等价于下面一条语句List&lt;String&gt; myList = Arrays.asList(\"Apple\",\"Banana\", \"Orange\"); JDK 源码对于这个方法的说明： 123456/** *返回由指定数组支持的固定大小的列表。此方法作为基于数组和基于集合的API之间的桥梁，与 Collection.toArray()结合使用。返回的List是可序列化并实现RandomAccess接口。 */ public static &lt;T&gt; List&lt;T&gt; asList(T... a) &#123; return new ArrayList&lt;&gt;(a);&#125; 阿里巴巴Java 开发手册》对其的描述Arrays.asList()将数组转换为集合后,底层其实还是数组，《阿里巴巴Java 开发手册》对于这个方法有如下描述： 使用时的注意事项总结传递的数组必须是对象数组，而不是基本类型。 Arrays.asList()是泛型方法，传入的对象必须是对象数组。 1234567int[] myArray = &#123; 1, 2, 3 &#125;;List myList = Arrays.asList(myArray);System.out.println(myList.size());//1System.out.println(myList.get(0));//数组地址值System.out.println(myList.get(1));//报错：ArrayIndexOutOfBoundsExceptionint [] array=(int[]) myList.get(0);System.out.println(array[0]);//1 当传入一个原生数据类型数组时，Arrays.asList() 的真正得到的参数就不是数组中的元素，而是数组对象本身！此时List 的唯一元素就是这个数组，这也就解释了上面的代码。 我们使用包装类型数组就可以解决这个问题。 1Integer[] myArray = &#123; 1, 2, 3 &#125;; 使用集合的修改方法:add()、remove()、clear()会抛出异常。 1234List myList = Arrays.asList(1, 2, 3);myList.add(4);//运行时报错：UnsupportedOperationExceptionmyList.remove(1);//运行时报错：UnsupportedOperationExceptionmyList.clear();//运行时报错：UnsupportedOperationException Arrays.asList() 方法返回的并不是 java.util.ArrayList ，而是 java.util.Arrays 的一个内部类,这个内部类并没有实现集合的修改方法或者说并没有重写这些方法。 12List myList = Arrays.asList(1, 2, 3);System.out.println(myList.getClass());//class java.util.Arrays$ArrayList 下图是java.util.Arrays$ArrayList的简易源码，我们可以看到这个类重写的方法有哪些。 12345678910111213141516171819202122232425262728293031323334353637383940private static class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements RandomAccess, java.io.Serializable &#123; ... @Override public E get(int index) &#123; ... &#125; @Override public E set(int index, E element) &#123; ... &#125; @Override public int indexOf(Object o) &#123; ... &#125; @Override public boolean contains(Object o) &#123; ... &#125; @Override public void forEach(Consumer&lt;? super E&gt; action) &#123; ... &#125; @Override public void replaceAll(UnaryOperator&lt;E&gt; operator) &#123; ... &#125; @Override public void sort(Comparator&lt;? super E&gt; c) &#123; ... &#125; &#125; 我们再看一下java.util.AbstractList的remove()方法，这样我们就明白为啥会抛出UnsupportedOperationException。 123public E remove(int index) &#123; throw new UnsupportedOperationException();&#125; 如何正确的将数组转换为ArrayListstackoverflow：https://dwz.cn/vcBkTiTW 1. 自己动手实现（教育目的） 1234567891011//JDK1.5+static &lt;T&gt; List&lt;T&gt; arrayToList(final T[] array) &#123; final List&lt;T&gt; l = new ArrayList&lt;T&gt;(array.length); for (final T s : array) &#123; l.add(s); &#125; return (l);&#125;Integer [] myArray = &#123; 1, 2, 3 &#125;;System.out.println(arrayToList(myArray).getClass());//class java.util.ArrayList 2. 最简便的方法(推荐) 1List list = new ArrayList&lt;&gt;(Arrays.asList(\"a\", \"b\", \"c\")) 3. 使用 Java8 的Stream(推荐) 12345Integer [] myArray = &#123; 1, 2, 3 &#125;;List myList = Arrays.stream(myArray).collect(Collectors.toList());//基本类型也可以实现转换（依赖boxed的装箱操作）int [] myArray2 = &#123; 1, 2, 3 &#125;;List myList = Arrays.stream(myArray2).boxed().collect(Collectors.toList()); 4. 使用 Guava(推荐) 对于不可变集合，你可以使用ImmutableList类及其of()与copyOf()工厂方法：（参数不能为空） 12List&lt;String&gt; il = ImmutableList.of(\"string\", \"elements\"); // from varargsList&lt;String&gt; il = ImmutableList.copyOf(aStringArray); // from array 对于可变集合，你可以使用Lists类及其newArrayList()工厂方法： 123List&lt;String&gt; l1 = Lists.newArrayList(anotherListOrCollection); // from collectionList&lt;String&gt; l2 = Lists.newArrayList(aStringArray); // from arrayList&lt;String&gt; l3 = Lists.newArrayList(\"or\", \"string\", \"elements\"); // from varargs 5. 使用 Apache Commons Collections 12List&lt;String&gt; list = new ArrayList&lt;String&gt;();CollectionUtils.addAll(list, str); Collection.toArray()方法使用的坑&amp;如何反转数组该方法是一个泛型方法：&lt;T&gt; T[] toArray(T[] a); 如果toArray方法中没有传递任何参数的话返回的是Object类型数组。 123456String[] s = new String[]&#123; \"dog\", \"lazy\", \"a\", \"over\", \"jumps\", \"fox\", \"brown\", \"quick\", \"A\"&#125;;List&lt;String&gt; list = Arrays.asList(s);Collections.reverse(list);s=list.toArray(new String[0]);//没有指定类型的话会报错 由于JVM优化，new String[0]作为Collection.toArray()方法的参数现在使用更好，new String[0]就是起一个模板的作用，指定了返回数组的类型，0是为了节省空间，因为它只是为了说明返回的类型。详见：https://shipilev.net/blog/2016/arrays-wisdom-ancients/ 不要在 foreach 循环里进行元素的 remove/add 操作如果要进行remove操作，可以调用迭代器的 remove方法而不是集合类的 remove 方法。因为如果列表在任何时间从结构上修改创建迭代器之后，以任何方式除非通过迭代器自身remove/add方法，迭代器都将抛出一个ConcurrentModificationException,这就是单线程状态下产生的 fail-fast 机制。 fail-fast 机制 ：多个线程对 fail-fast 集合进行修改的时，可能会抛出ConcurrentModificationException，单线程下也会出现这种情况，上面已经提到过。 java.util包下面的所有的集合类都是fail-fast的，而java.util.concurrent包下面的所有的类都是fail-safe的。","tags":[]},{"title":"Java基础知识","date":"2019-11-04T05:50:20.054Z","path":"2019/11/04/Java基础知识/","text":"java基础知识 (๑• . •๑) 1这是崔斯特的第九十九篇原创文章 面向对象(oop)与面向过程(opp)的区别 面向过程 ：面向过程性能比面向对象高。 因为类调用时需要实例化，开销比较大，比较消耗资源，所以当性能是最重要的考量因素的时候，比如单片机、嵌入式开发、Linux/Unix等一般采用面向过程开发。但是，面向过程没有面向对象易维护、易复用、易扩展。 面向对象 ：面向对象易维护、易复用、易扩展。 因为面向对象有封装、继承、多态性的特性，所以可以设计出低耦合的系统，使系统更加灵活、更加易于维护。但是，面向对象性能比面向过程低。 Python也是一门面向对象的编程语言 Java 语言有哪些特点 简单易学； 面向对象（封装，继承，多态）； 平台无关性（ Java 虚拟机实现平台无关性）； 可靠性； 安全性； 支持多线程（ C++ 语言没有内置的多线程机制，因此必须调用操作系统的多线程功能来进行多线程程序设计，而 Java 语言却提供了多线程支持）； 支持网络编程并且很方便（ Java 语言诞生本身就是为简化网络编程设计的，因此 Java 语言不仅支持网络编程而且很方便）； 编译与解释并存； 关于 JVM JDK 和 JRE 最详细通俗的解答JVMJava虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。 什么是字节码?采用字节码的好处是什么? 在 Java 中，JVM可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以 Java 程序运行时比较高效，而且，由于字节码并不针对一种特定的机器，因此，Java程序无须重新编译便可在多种不同操作系统的计算机上运行。 Java 程序从源代码到运行一般有下面3步： 我们需要格外注意的是 .class-&gt;机器码 这一步。在这一步 JVM 类加载器首先加载字节码文件，然后通过解释器逐行解释执行，这种方式的执行速度会相对比较慢。而且，有些方法和代码块是经常需要被调用的(也就是所谓的热点代码)，所以后面引进了 JIT 编译器，而JIT 属于运行时编译。当 JIT 编译器完成第一次编译后，其会将字节码对应的机器码保存下来，下次可以直接使用。而我们知道，机器码的运行效率肯定是高于 Java 解释器的。这也解释了我们为什么经常会说 Java 是编译与解释共存的语言。 HotSpot采用了惰性评估(Lazy Evaluation)的做法，根据二八定律，消耗大部分系统资源的只有那一小部分的代码（热点代码），而这也就是JIT所需要编译的部分。JVM会根据代码每次被执行的情况收集信息并相应地做出一些优化，因此执行的次数越多，它的速度就越快。JDK 9引入了一种新的编译模式AOT(Ahead of Time Compilation)，它是直接将字节码编译成机器码，这样就避免了JIT预热等各方面的开销。JDK支持分层编译和AOT协作使用。但是 ，AOT 编译器的编译质量是肯定比不上 JIT 编译器的。 总结： Java虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。字节码和不同系统的 JVM 实现是 Java 语言“一次编译，随处可以运行”的关键所在。 JDK 和 JREJDK是Java Development Kit，它是功能齐全的Java SDK。它拥有JRE所拥有的一切，还有编译器（javac）和工具（如javadoc和jdb）。它能够创建和编译程序。 JRE 是 Java运行时环境。它是运行已编译 Java 程序所需的所有内容的集合，包括 Java虚拟机（JVM），Java类库，java命令和其他的一些基础构件。但是，它不能用于创建新程序。 如果你只是为了运行一下 Java 程序的话，那么你只需要安装 JRE 就可以了。如果你需要进行一些 Java 编程方面的工作，那么你就需要安装JDK了。但是，这不是绝对的。有时，即使您不打算在计算机上进行任何Java开发，仍然需要安装JDK。例如，如果要使用JSP部署Web应用程序，那么从技术上讲，您只是在应用程序服务器中运行Java程序。那你为什么需要JDK呢？因为应用程序服务器会将 JSP 转换为 Java servlet，并且需要使用 JDK 来编译 servlet。 Java和C++的区别?我知道很多人没学过 C++，但是面试官就是没事喜欢拿咱们 Java 和 C++ 比呀！没办法！！！就算没学过C++，也要记下来！ 都是面向对象的语言，都支持封装、继承和多态 Java 不提供指针来直接访问内存，程序内存更加安全 Java 的类是单继承的，C++ 支持多重继承；虽然 Java 的类不可以多继承，但是接口可以多继承。 Java 有自动内存管理机制，不需要程序员手动释放无用内存 补充一下JAVA和Python的区别： python虚拟机没有java强，java虚拟机是java的核心，python的核心是可以很方便地使用c语言函数或c++库 Python是动态语言，JAVA是静态语言。JAVA在编写时可以检测变量类型 Python入门比JAVA简单 Python更多用来写一些简单脚本，JAVA用来做web、大数据方面很多 重载和重写的区别 重载： 发生在同一个类中，方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同，发生在编译时。 重写： 发生在父子类中，方法名、参数列表必须相同，返回值范围小于等于父类，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类；如果父类方法访问修饰符为 private 则子类就不能重写该方法。 Java 面向对象编程三大特性: 封装 继承 多态封装封装把一个对象的属性私有化，同时提供一些可以被外界访问的属性的方法，如果属性不想被外界访问，我们大可不必提供方法给外界访问。但是如果一个类没有提供给外界访问的方法，那么这个类也没有什么意义了。 继承继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承我们能够非常方便地复用以前的代码。 关于继承如下 3 点请记住： 子类拥有父类对象所有的属性和方法（包括私有属性和私有方法），但是父类中的私有属性和方法子类是无法访问，只是拥有。 子类可以拥有自己属性和方法，即子类可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。（以后介绍）。 多态所谓多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定，而是在程序运行期间才确定，即一个引用变量到底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。 在Java中有两种形式可以实现多态：继承（多个子类对同一方法的重写）和接口（实现接口并覆盖接口中同一方法）。 String StringBuffer 和 StringBuilder 的区别可变性 简单的来说：String 类中使用 final 关键字修饰字符数组来保存字符串，private final char value[]，所以 String 对象是不可变的。 而StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串char[]value 但是没有用 final 关键字修饰，所以这两种对象都是可变的。 StringBuilder 与 StringBuffer 的构造方法都是调用父类构造方法也就是 AbstractStringBuilder 实现的，大家可以自行查阅源码。 StringBuilder.java 12345678910public final class StringBuilder extends AbstractStringBuilder implements java.io.Serializable, CharSequence&#123; public StringBuilder() &#123; super(16); &#125; public StringBuilder(int capacity) &#123; super(capacity); &#125;&#125; AbstractStringBuilder.java 123456789abstract class AbstractStringBuilder implements Appendable, CharSequence &#123; char[] value; int count; AbstractStringBuilder() &#123; &#125; AbstractStringBuilder(int capacity) &#123; value = new char[capacity]; &#125;&#125; 线程安全性 String 中的对象是不可变的，也就可以理解为常量，线程安全。AbstractStringBuilder 是 StringBuilder 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如 expandCapacity、append、insert、indexOf 等公共方法。 StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。 StringBuffer.java 123456789101112@Overridepublic synchronized StringBuffer insert(int offset, String str) &#123; toStringCache = null; super.insert(offset, str); return this;&#125;@Overridepublic synchronized void setLength(int newLength) &#123; toStringCache = null; super.setLength(newLength);&#125; 性能 每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。 对于三者使用的总结： 操作少量的数据: 适用String 单线程操作字符串缓冲区下操作大量数据: 适用StringBuilder 多线程操作字符串缓冲区下操作大量数据: 适用StringBuffer #自动装箱与拆箱 装箱：将基本类型用它们对应的引用类型包装起来； 拆箱：将包装类型转换为基本数据类型； 在一个静态方法内调用一个非静态成员为什么是非法的?由于静态方法可以不通过对象进行调用，因此在静态方法里，不能调用其他非静态变量，也不可以访问非静态变量成员。 在 Java 中定义一个不做事且没有参数的构造方法的作用Java 程序在执行子类的构造方法之前，如果没有用 super()来调用父类特定的构造方法，则会调用父类中“没有参数的构造方法”。 因此，如果父类中只定义了有参数的构造方法，而在子类的构造方法中又没有用 super()来调用父类中特定的构造方法，则编译时将发生错误，因为 Java 程序在父类中找不到没有参数的构造方法可供执行。解决办法是在父类里加上一个不做事且没有参数的构造方法。 接口和抽象类的区别是什么？ 接口的方法默认是 public，所有方法在接口中不能有实现(Java 8 开始接口方法可以有默认实现），而抽象类可以有非抽象的方法。 接口中除了static、final变量，不能有其他变量，而抽象类中则不一定。 一个类可以实现多个接口，但只能实现一个抽象类。接口自己本身可以通过extends关键字扩展多个接口。 接口方法默认修饰符是public，抽象方法可以有public、protected和default这些修饰符（抽象方法就是为了被重写所以不能使用private关键字修饰！）。 从设计层面来说，抽象是对类的抽象，是一种模板设计，而接口是对行为的抽象，是一种行为的规范。 备注：在JDK8中，接口也可以定义静态方法，可以直接用接口名调用。实现类和实现是不可以调用的。如果同时实现两个接口，接口中定义了一样的默认方法，则必须重写，不然会报错。 成员变量与局部变量的区别有哪些？ 从语法形式上看：成员变量是属于类的，而局部变量是在方法中定义的变量或是方法的参数；成员变量可以被 public,private,static 等修饰符所修饰，而局部变量不能被访问控制修饰符及 static 所修饰；但是，成员变量和局部变量都能被 final 所修饰。 从变量在内存中的存储方式来看:如果成员变量是使用static修饰的，那么这个成员变量是属于类的，如果没有使用static修饰，这个成员变量是属于实例的。而对象存在于堆内存，局部变量则存在于栈内存。 从变量在内存中的生存时间上看:成员变量是对象的一部分，它随着对象的创建而存在，而局部变量随着方法的调用而自动消失。 成员变量如果没有被赋初值:则会自动以类型的默认值而赋值（一种情况例外:被 final 修饰的成员变量也必须显式地赋值），而局部变量则不会自动赋值。 构造方法有哪些特性？ 名字与类名相同。 没有返回值，但不能用void声明构造函数。 生成类的对象时自动执行，无需调用。 静态方法和实例方法有何不同 在外部调用静态方法时，可以使用”类名.方法名”的方式，也可以使用”对象名.方法名”的方式。而实例方法只有后面这种方式。也就是说，调用静态方法可以无需创建对象。 静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），而不允许访问实例成员变量和实例方法；实例方法则无此限制。 == 与 equals== : 它的作用是判断两个对象的地址是不是相等。即，判断两个对象是不是同一个对象(基本数据类型==比较的是值，引用数据类型==比较的是内存地址)。基本数据类型：byte、short、int、long、float、double、char、boolean引用类型：类(class)、接口(interface)、数组(array) equals() : 它的作用也是判断两个对象是否相等。但它一般有两种使用情况： 情况1：类没有覆盖 equals() 方法。则通过 equals() 比较该类的两个对象时，等价于通过“==”比较这两个对象。 情况2：类覆盖了 equals() 方法。一般，我们都覆盖 equals() 方法来比较两个对象的内容是否相等；若它们的内容相等，则返回 true (即，认为这两个对象相等)。 举个例子： 1234567891011121314151617public class test1 &#123; public static void main(String[] args) &#123; String a = new String(\"ab\"); // a 为一个引用 String b = new String(\"ab\"); // b为另一个引用,对象的内容一样 String aa = \"ab\"; // 放在常量池中 String bb = \"ab\"; // 从常量池中查找 if (aa == bb) // true System.out.println(\"aa==bb\"); if (a == b) // false，非同一对象 System.out.println(\"a==b\"); if (a.equals(b)) // true System.out.println(\"aEQb\"); if (42 == 42.0) &#123; // true System.out.println(\"true\"); &#125; &#125;&#125; 说明： String 中的 equals 方法是被重写过的，因为 object 的 equals 方法是比较的对象的内存地址，而 String 的 equals 方法比较的是对象的值。 当创建 String 类型的对象时，虚拟机会在常量池中查找有没有已经存在的值和要创建的值相同的对象，如果有就把它赋给当前引用。如果没有就在常量池中重新创建一个 String 对象。 一般情况下都用equals，肯定不会错 hashCode 与 equals面试官可能会问你：“你重写过 hashcode 和 equals 么，为什么重写equals时必须重写hashCode方法？” hashCode（）介绍hashCode() 的作用是获取哈希码，也称为散列码；它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode() 定义在JDK的Object.java中，这就意味着Java中的任何类都包含有hashCode() 函数。 散列表存储的是键值对(key-value)，它的特点是：能根据“键”快速的检索出对应的“值”。这其中就利用到了散列码！（可以快速找到所需要的对象） hashCode可能会返回负数 为什么要有 hashCode我们先以“HashSet 如何检查重复”为例子来说明为什么要有 hashCode： 当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashcode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashcode 值作比较，如果没有相符的hashcode，HashSet会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用 equals（）方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。（摘自我的Java启蒙书《Head first java》第二版）。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。 通过我们可以看出：hashCode() 的作用就是获取哈希码，也称为散列码；它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode()在散列表中才有用，在其它情况下没用。在散列表中hashCode() 的作用是获取对象的散列码，进而确定该对象在散列表中的位置。 hashCode（）与equals（）的相关规定 如果两个对象相等，则hashcode一定也是相同的 两个对象相等,对两个对象分别调用equals方法都返回true 两个对象有相同的hashcode值，它们也不一定是相等的 因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖 hashCode() 的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据） 推荐阅读：Java hashCode() 和 equals()的若干问题解答 为什么Java中只有值传递有时间好好看看这个：这一次，彻底解决Java的值传递和引用传递 简述线程、程序、进程的基本概念。以及他们之间关系是什么?线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。 程序是含有指令和数据的文件，被存储在磁盘或其他的数据存储设备中，也就是说程序是静态的代码。 进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。简单来说，一个进程就是一个执行中的程序，它在计算机中一个指令接着一个指令地执行着，同时，每个进程还占有某些系统资源如CPU时间，内存空间，文件，输入输出设备的使用权等等。换句话说，当程序在执行时，将会被操作系统载入内存中。 线程是进程划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。从另一角度来说，进程属于操作系统的范畴，主要是同一段时间内，可以同时执行一个以上的程序，而线程则是在同一程序内几乎同时执行一个以上的程序段。 线程有哪些基本状态Java 线程在运行的生命周期中的指定时刻只可能处于下面6种不同状态的其中一个状态（图源《Java 并发编程艺术》4.1.4节）。 线程在生命周期中并不是固定处于某一个状态而是随着代码的执行在不同状态之间切换。Java 线程状态变迁如下图所示（图源《Java 并发编程艺术》4.1.4节）： 由上图可以看出： 线程创建之后它将处于 NEW（新建） 状态，调用 start() 方法后开始运行，线程这时候处于 READY（可运行） 状态。可运行状态的线程获得了 cpu 时间片（timeslice）后就处于 RUNNING（运行） 状态。 操作系统隐藏 Java虚拟机（JVM）中的 READY 和 RUNNING 状态，它只能看到 RUNNABLE 状态（图源：HowToDoInJava：Java Thread Life Cycle and Thread States），所以 Java 系统一般将这两个状态统称为 RUNNABLE（运行中） 状态 。 当线程执行 wait()方法之后，线程进入 WAITING（等待）状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而 TIME_WAITING(超时等待) 状态相当于在等待状态的基础上增加了超时限制，比如通过 sleep（long millis）方法或 wait（long millis）方法可以将 Java 线程置于 TIMED WAITING 状态。当超时时间到达后 Java 线程将会返回到 RUNNABLE 状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 BLOCKED（阻塞） 状态。线程在执行 Runnable 的run()方法之后将会进入到 TERMINATED（终止） 状态。 关于 final 关键字的一些总结final关键字主要用在三个地方：变量、方法、类。 对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。 当用final修饰一个类时，表明这个类不能被继承。final类中的所有成员方法都会被隐式地指定为final方法。 使用final方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的Java实现版本中，会将final方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现在的Java版本已经不需要使用final方法进行这些优化了）。类中所有的private方法都隐式地指定为final。 Java 中的异常处理Java异常类层次结构图 在 Java 中，所有的异常都有一个共同的祖先java.lang包中的 Throwable类。Throwable： 有两个重要的子类：Exception（异常） 和 Error（错误） ，二者都是 Java 异常处理的重要子类，各自都包含大量子类。 Error（错误）:是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。 这些错误表示故障发生于虚拟机自身、或者发生在虚拟机试图执行应用时，如Java虚拟机运行错误（Virtual MachineError）、类定义错误（NoClassDefFoundError）等。这些错误是不可查的，因为它们在应用程序的控制和处理能力之 外，而且绝大多数是程序运行时不允许出现的状况。对于设计合理的应用程序来说，即使确实发生了错误，本质上也不应该试图去处理它所引起的异常状况。在 Java中，错误通过Error的子类描述。 Exception（异常）:是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException。RuntimeException 异常由Java虚拟机抛出。NullPointerException（要访问的变量没有引用任何对象时，抛出该异常）、ArithmeticException（算术运算异常，一个整数除以0时，抛出该异常）和 ArrayIndexOutOfBoundsException （下标越界异常）。 注意：异常和错误的区别：异常能被程序本身处理，错误是无法处理。 Throwable类常用方法 public string getMessage():返回异常发生时的简要描述 public string toString():返回异常发生时的详细信息 public string getLocalizedMessage():返回异常对象的本地化信息。使用Throwable的子类覆盖这个方法，可以生成本地化信息。如果子类没有覆盖该方法，则该方法返回的信息与getMessage（）返回的结果相同 public void printStackTrace():在控制台上打印Throwable对象封装的异常信息 异常处理总结 try 块： 用于捕获异常。其后可接零个或多个catch块，如果没有catch块，则必须跟一个finally块。 catch 块： 用于处理try捕获到的异常。 finally 块： 无论是否捕获或处理异常，finally块里的语句都会被执行。当在try块或catch块中遇到return 语句时，finally语句块将在方法返回之前被执行。 在以下4种特殊情况下，finally块不会被执行： 在finally语句块第一行发生了异常。 因为在其他行，finally块还是会得到执行 在前面的代码中用了System.exit(int)已退出程序。 exit是带参函数 ；若该语句在异常语句之后，finally会执行 程序所在的线程死亡。 关闭CPU。 下面这部分内容来自issue:https://github.com/Snailclimb/JavaGuide/issues/190。 注意： 当try语句和finally语句中都有return语句时，在方法返回之前，finally语句的内容将被执行，并且finally语句的返回值将会覆盖原始的返回值。如下： 123456789public static int f(int value) &#123; try &#123; return value * value; &#125; finally &#123; if (value == 2) &#123; return 0; &#125; &#125;&#125; 如果调用 f(2)，返回值将是0，因为finally语句的返回值覆盖了try语句块的返回值。 Java序列化中如果有些字段不想进行序列化，怎么办对于不想进行序列化的变量，使用transient关键字修饰。 transient关键字的作用是：阻止实例中那些用此关键字修饰的的变量序列化；当对象被反序列化时，被transient修饰的变量值不会被持久化和恢复。transient只能修饰变量，不能修饰类和方法。 Java 中 IO 流Java 中 IO 流分为几种? 按照流的流向分，可以分为输入流和输出流； 按照操作单元划分，可以划分为字节流和字符流； 按照流的角色划分为节点流和处理流。 Java Io流共涉及40多个类，这些类看上去很杂乱，但实际上很有规则，而且彼此之间存在非常紧密的联系， Java I0流的40多个类都是从如下4个抽象类基类中派生出来的。 InputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。 OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。 按操作方式分类结构图： 按操作对象分类结构图： 既然有了字节流,为什么还要有字符流?问题本质想问：不管是文件读写还是网络发送接收，信息的最小存储单元都是字节，那为什么 I/O 流操作要分为字节流操作和字符流操作呢？ 回答：字符流是由 Java 虚拟机将字节转换得到的，问题就出在这个过程还算是非常耗时，并且，如果我们不知道编码类型就很容易出现乱码问题。所以， I/O 流就干脆提供了一个直接操作字符的接口，方便我们平时对字符进行流操作。如果音频文件、图片等媒体文件用字节流比较好，如果涉及到字符的话使用字符流比较好。 BIO,NIO,AIO 有什么区别? BIO (Blocking I/O): 同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成。在活动连接数不是特别高（小于单机1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。 NIO (New I/O): NIO是一种同步非阻塞的I/O模型，在Java 1.4 中引入了NIO框架，对应 java.nio 包，提供了 Channel , Selector，Buffer等抽象。NIO中的N可以理解为Non-blocking，不单纯是New。它支持面向缓冲的，基于通道的I/O操作方法。 NIO提供了与传统BIO模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞I/O来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发 AIO (Asynchronous I/O): AIO 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的IO模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。AIO 是异步IO的缩写，虽然 NIO 在网络操作中，提供了非阻塞的方法，但是 NIO 的 IO 行为还是同步的。对于 NIO 来说，我们的业务线程是在 IO 操作准备好时，得到通知，接着就由这个线程自行进行 IO 操作，IO操作本身是同步的。查阅网上相关资料，我发现就目前来说 AIO 的应用还不是很广泛，Netty 之前也尝试使用过 AIO，不过又放弃了。 常见关键字总结:static,final,this,super详见笔主的这篇文章: final,static,this,super 关键字总结 Collections 工具类和 Arrays 工具类常见方法总结 Collections 工具类和 Arrays 工具类常见方法","tags":[]},{"title":"移动端真机调试指南","date":"2019-10-29T02:06:21.667Z","path":"2019/10/29/移动端真机调试指南/","text":"1这是崔斯特的第九十八篇原创文章 各种抓包方法 (๑• . •๑) 系统自带调试功能iOS 系统运行环境要求 Mac + Safari 浏览器 iPhone（iOS 6 +） + Safari 浏览器 调试步骤 1、使用 Lightning 数据线将 iPhone 与 Mac 相连 2、iPhone 开启 Web 检查器（设置 -&gt; Safari -&gt; 高级 -&gt; 开启 Web 检查器） 3、iPhone 使用 Safari 浏览器打开要调试的页面 4、Mac 打开 Safari 浏览器调试（菜单栏 —&gt; 开发 -&gt; iPhone 设备名 -&gt; 选择调试页面） 如果你的菜单栏没有“开发”选项，可以到左上角 Safari -&gt; 偏好设置 -&gt; 高级 -&gt; 在菜单栏中显示“开发”菜单。 5、在弹出的 Safari Developer Tools 中调试 经过如上步骤就可在 Mac 端调试 iPhone 上 Safari 运行的页面了，但对于 WebView 页面就不适用了，另外 Windows 系统不适用此方案。 当前测试环境： Safari 版本 10.0.2 iPhone 7（iOS 10.1.1） 没有 iPhone 设备可以在 App Store 安装 Xcode 使用其内置的 iOS 模拟器，安装完成后通过以下两种方式开启： 右键 Xcode 图标 -&gt; Open Developer Tool -&gt; Simulator 右键 Finder 图标 -&gt; 前往文件夹 -&gt; /应用程序/Xcode.app/Contents/Developer/Applications/ -&gt; 运行 Simulator.app 运行 iOS 模拟器后，在模拟器中打开调试页面，再通过 Mac Safari 开发功能就可以调试到。 如果我需要调试更低版本的 iOS 怎么办？实际使用的 iPhone 不可能去降版本，不必担心，Simulator 有。 点击左上角 Xcode -&gt; Preferences -&gt; Downloads 就可以看到提供了如下版本： Android 系统运行环境要求 Chrome 版本 &gt;= 32 Android 版本 4.0 + 调试步骤 使用 USB 数据线将手机与电脑相连 手机进入开发者模式，勾选 USB 调试，并允许调试 如何开启 USB 调试：索尼 Z5：设置 -&gt; 关于关机 -&gt; 多次点击软件版本开启 -&gt; 返回上一级 -&gt; 开发者选项 -&gt; USB 调试 魅蓝 Note：设置 -&gt; 辅助功能 -&gt; 开发者选项 -&gt; USB 调试 不同 Android 设备进入开发者模式的方式有稍稍不同，瞎捣鼓一下即可开启。 电脑打开 Chrome 浏览器，在地址栏输入：chrome://inspect/#devices 并勾选 Discover USB devices 选项 手机允许远程调试，并访问调试页面 官方的教程是想让你使用手机 Chrome 开启调试页面的，但实际需求更多的是调试一些 WebView 页面，在官方的 Remote Debugging WebViews 有说明是可以调试 WebView 页面的，Android 版本需要在 4.4 以上，并且 APP 需要有配置相应的启动调试代码。 WebView debugging must be enabled from within your application. To enable WebView debugging, call the static method setWebContentsDebuggingEnabled on the WebView class. 必须在 APP 内启动 WebView 调试。要启动 WebView 调试，需要调用 WebView 类上的静态方法 setWebContentsDebuggingEnabled。 123if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.KITKAT) &#123; WebView.setWebContentsDebuggingEnabled(true);&#125; This setting applies to all of the application’s WebViews. 此设置适用于所有 APP 的 WebView。 Tip: WebView debugging is not affected by the state of the debuggable flag in the application’s manifest. If you want to enable WebView debugging only when debuggable is true, test the flag at runtime. 提示：WebView 是否可调试状态不受 mainfest 标志变量 debuggable 的影响，如果你想在 debuggable 为 true 的时候启动 WebView 调试，请使用以下代码： 12345if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.KITKAT) &#123; if (0 != (getApplicationInfo().flags &amp; ApplicationInfo.FLAG_DEBUGGABLE)) &#123; WebView.setWebContentsDebuggingEnabled(true); &#125;&#125; 电脑点击 inspect 按钮 如果你出现无法识别到设备的情况，建议尝试以下几种方法： 使用原装数据线，不要使用山寨数据线或一线多头的数据线 重新插拔 USB 数据线，让手机处于充电状态 关闭电脑相关的应用助手 重启手机 Windows 系统下自动安装驱动失败，到 Android Studio 手动下载 注意：使用 Chrome Inspect 查看页面时，Chrome 需要从 https://chrome-devtools-frontend.appspot.com 加载资源，如果你得到的调试界面是一片空白，那你可能需要科学上网。 进入调试界面 当前测试环境： Chrome 版本 55.0.2883.95 索尼 Z5（Android 5.1.1） 魅蓝 Note（Android 5.1） 三星 Galaxy S7（Android 6.0.1） 这里顺带提一下 TBS Studio 调试工具，它在 Chrome DevTools 调试功能的基础上进行了一些功能扩展，特性如下： 1.3 TBS Studio 功能特性 自动检测手机与 PC 的连接； 自动检测网页是否可进行 Inspect 调试； 自动引导开发者打开 Inspector 调试开关； 一键开启 Inspector 调试，无需打开浏览器输入 URL，方便快捷； 扩展 X5 内核独有 Inspect 选项，方便页面分析和优化； 真机远程 Inspector 调试。 详细介绍和使用步骤可到开发者论坛查看，部分 Android 机型通过 USB 可能依旧无法识别到设备，可使用后面会讲到的其他方案。 关于 Android 虚拟机也是有的，这里推荐使用 Genymotion 软件，使用 Genymotion 前需要安装 VirtuaBox，并且注册登陆后才能显示所有的虚拟设备。 使用代理工具调试开发环境页面对于需要配 Hosts 才能访问的开发环境页面，手机在默认情况下是没有权限修改 Hosts 文件的，除非是 iOS 设备越狱后和 Android 设备 root 后，所以一般情况下手机是无法访问开发环境页面，这时需要使用到 Mac 系统的 Charles 代理工具，Windows 系统可使用 Fiddler 代理工具。 实现思路 Mac 作为代理服务器 手机通过 HTTP 代理连接到 Mac 电脑 手机上的请求都经过代理服务器 通过给 Mac 配 Hosts 实现目的 调试步骤 查看电脑 IP（菜单 -&gt; Help -&gt; Local IP Addresses） 查看端口（菜单 -&gt; Settings -&gt; Proxy Settings -&gt; Proxies） 默认端口为：8888，勾选 Enable transparent HTTP proxying 将 IP、端口号填入手机 HTTP 代理 iOS 系统：设置 -&gt; 无线局域网 -&gt; 点击叹号 -&gt; HTTP 代理 -&gt; 手动 Android 系统：设置 -&gt; 长按当前网络 -&gt; 修改网络 -&gt; 高级选项 -&gt; 手动 Charles 允许授权 每次有新设备首次连接都会提示是否授权，可以通过 Proxy -&gt; Access Control Settings 配置以下参数 0.0.0.0/0 来关闭。 使用 SwitchHosts! 软件给 Mac 电脑配 Hosts 手机访问开发环境页面 到这一步手机就可以访问到开发环境下的页面了，再结合前面所讲的方案来调试页面。 Charles 的调试功能 7.1 网络映射修改文件 除了结合前面的方案调试，可以使用 Map Local 网络映射功能来实现对文件的修改，在菜单 -&gt; Proxy -&gt; Start Recording 开启抓包后访问页面，找到抓取到的样式文件，点击右键 Map Local，在 Local path 中设置本地映射文件的路径，修改后刷新页面可以看到效果。 7.2 模拟网络速度 菜单 -&gt; Proxy -&gt; Throttle Settings -&gt; 勾选 Enable Throttling，在 Throttling preset 中可以选择需要模拟的网络速度。 7.3 抓取 HTTPS 请求 默认情况下，Charles 无法抓取到 HTTPS 的请求，解决步骤如下： Mac 端安装证书：菜单 -&gt; Help -&gt; SSL Proxying -&gt; Install Charles Root Certificate 然后导出 Charles SSL 证书安装到手机，菜单 -&gt; Help -&gt; SSL Proxying -&gt; Save Charles Root Certificate Android 设备导出的 Charles SSL 证书存储到手机中并安装。 iOS 设备用 Safari 打开 http://www.charlesproxy.com/getssl/ 页面，下载 Charles SSL 证书并安装。 证书安装完成后，还需要给 Charles SSL 代理配置域名和端口号，菜单 -&gt; Proxy -&gt; SSL Proxying Settings 勾选 Enable SSL Proxying 点击 Add 填入域名和端口号，经过以上步骤就可以抓取到 HTTPS 的请求了。 7.4 断点调试请求和响应内容 开启 Charles 断点 Proxy -&gt; Breakpoints Settings -&gt; Enable Breakpoints点击 Add 可设置断点条件或者单独对需要的文件右键 Breakpoints 设置断点。 访问页面后，即可编辑请求和响应的内容，点击 Execute 按钮完成。 Weinre 调试工具Weinre 是一款较老的远程调试工具，功能与 Chrome DevTools 相似，需要在页面中插入一段 JS 脚本来实时调试页面 DOM 结构、样式、JS 等，另外它使用的是代理的方式，所以兼容性很好，无论是新老设备系统通吃，但对于样式调试不友善，缺少断点调试及 Profiles 等常用功能。 调试步骤： 1、安装 Weinre 使用 NPM 全局安装 Weinre 1$ sudo npm -g install weinre 2、启动 Weinre 监听服务 12$ ipconfig getifaddr en0 // 查看本机 IP$ weinre --boundHost 10.14.217.14 --httpPort 8090 --boundHost 后填入你本机 IP 地址，--httpPort 后填入端口号，默认为 8080 3、进入 Weinre 管理页面 使用 Chrome 浏览器访问 http://10.14.217.14:8090，在管理页面你可以看到使用相关的说明，有进入客户端调试界面的地址、使用的文档、DEMO 页面等等，说明中要求将一段 JS 脚本 &lt;script src=&quot;http://10.14.217.14:8090/target/target-script-min.js#anonymous&quot;&gt;&lt;/script&gt; 插入到需要调试的页面中，插入代码后手机访问调试页面。 4、进入客户端调试界面 点击 debug client user interface：http://10.14.217.14:8090/client/#anonymous 的链接。 5、JS 脚本注入 手动加入 JS 脚本不优雅，这里可以结合我们前面提到的 Charles 代理工具实现动态 HTTP Script 注入。 打开菜单 -&gt; Rewrite -&gt; 勾选 Enable Rewrite 输入 Rewrite 的名字并且在 Rules 一项添加匹配的规则，Location 一项是用于指定的域名和端口添加规则用的，这里我们不填默认匹配所有请求。 Type 允许对需要匹配的请求进行 Rewrite，一共提供了 11 种： Add Header Modify Header Remove Header Host Path URL Add Query Param Modify Query Param Remove Query Param Response Status Body 这里我们需要使用到的是 Body，它的作用是对请求或响应内容进行匹配替换，按照下图的配置，通过将匹配到的响应内容 &lt;/body&gt; 标签替换成需要插入到页面中的 JS 脚本，从而实现动态插入。 另外，也有基于 Weinre 进行功能扩展的工具，比如早期版本的 微信 Web 开发者工具 v0.7.0 和 spy-debugger，都在 Weinre 的基础上简化了要给每个调试页面添加 JS 脚本的步骤，spy-debugger 还增加了对 HTTPS 的支持。 感谢你的阅读，如果你还有其他更为实用的调试方案，欢迎下方留言交流。 参考资料 Safari Web Inspector Guide Get Started with Remote Debugging Android Devices Remote Debugging WebViews weinre - Running Charles Web Debugging Proxy wuchangming/spy-debugger TBS 开发调试利器 —— TBS Studio - QQ 浏览器移动产品论坛 微信 web 开发者工具","tags":[]},{"title":"Redis SCAN的使用","date":"2019-09-25T02:09:09.679Z","path":"2019/09/25/Redis-SCAN的使用/","text":"1这是崔斯特的第九十八篇原创文章 了解下redis的更多用法 SCAN有时候需要从 Redis 实例成千上万的 key 中找出特定前缀的 key 列表来手动处理数据，可能是修改它的值，也可能是删除 key。这里就有一个问题，如何从海量的 key 中找出满足特定前缀的 key 列表来？ Redis 提供了一个简单暴力的指令 keys 用来列出所有满足特定正则字符串规则的 key。 12345678910$redis-cli keys key67* 1) \"key6764\" 2) \"key6738\" 3) \"key6774\" 4) \"key673\" 5) \"key6710\" 6) \"key6759\" 7) \"key6715\" 8) \"key6746\" 9) \"key6796\" 这个指令使用非常简单，提供一个简单的正则字符串即可，但是有很明显的两个缺点。 没有 offset、limit 参数，一次性吐出所有满足条件的 key，万一实例中有几百 w 个 key 满足条件， 当你看到满屏的字符串刷的没有尽头时，你就知道难受了。 keys 算法是遍历算法，复杂度是 O(n)，如果实例中有千万级以上的 key，这个指令就会导致 Redis 服务卡顿， 所有读写 Redis 的其它的指令都会被延后甚至会超时报错， 因为 Redis 是单线程程序，顺序执行所有指令，其它指令必须等到当前的 keys 指令执行完了才可以继续。 建议生产环境屏蔽keys命令 Redis 为了解决这个问题，它在 2.8 版本中加入了指令——scan。 scan 相比 keys 具备有以下特点: 复杂度虽然也是 O(n)，但是它是通过游标分步进行的，不会阻塞线程; 提供 limit 参数，可以控制每次返回结果的最大条数，limit 只是对增量式迭代命令的一种提示(hint)，返回的结果可多可少; 同 keys 一样，它也提供模式匹配功能; 服务器不需要为游标保存状态，游标的唯一状态就是 scan 返回给客户端的游标整数; 返回的结果可能会有重复，需要客户端去重复，这点非常重要; 遍历的过程中如果有数据修改，改动后的数据能不能遍历到是不确定的; 单次返回的结果是空的并不意味着遍历结束，而要看返回的游标值是否为零 scan 基础使用SCAN cursor [MATCH pattern] [COUNT count] 初始执行scan命令例如scan 0。SCAN命令是一个基于游标的迭代器。 这意味着命令每次被调用都需要使用上一次这个调用返回的游标作为该次调用的游标参数，以此来延续之前的迭代过程。 当SCAN命令的游标参数被设置为0时，服务器将开始一次新的迭代，而当redis服务器向用户返回值为0的游标时， 表示迭代已结束，这是唯一迭代结束的判定方式，而不能通过返回结果集是否为空判断迭代结束。 scan 参数提供了三个参数，第一个是 cursor 整数值，第二个是 key 的正则模式，第三个是遍历的 limit hint。 第一次遍历时，cursor 值为 0，然后将返回结果中第一个整数值作为下一次遍历的 cursor。 一直遍历到返回的 cursor 值为 0 时结束。 12345678910111213141516171819202122232425262728$redis-cli scan 0 match key99* count 10001) \"13912\"2) 1) \"key997\" 2) \"key9906\" 3) \"key9957\" 4) \"key9902\" 5) \"key9971\" 6) \"key9935\" 7) \"key9958\" 8) \"key9928\" 9) \"key9931\" 10) \"key9961\" 11) \"key9948\" 12) \"key9965\" 13) \"key9937\" $redis-cli scan 13912 match key99* count 10001) \"5292\"2) 1) \"key996\" 2) \"key9960\" 3) \"key9973\" 4) \"key9978\" 5) \"key9927\" 6) \"key995\" 7) \"key9992\" 8) \"key9993\" 9) \"key9964\" 10) \"key9934\" 返回结果分为两个部分：第一部分即 1) 就是下一次迭代游标，第二部分即 2) 就是本次迭代结果集。 从上面的过程可以看到虽然提供的 limit 是 1000，但是返回的结果只有 10 个左右。 因为这个 limit 不是限定返回结果的数量，而是限定服务器单次遍历的字典槽位数量(约等于)。 如果将 limit 设置为 10，你会发现返回结果是空的，但是游标值不为零，意味着遍历还没结束。 1234567$redis-cli scan 0 match key99* count 101) \"15360\"2) (empty list or set)$redis-cli scan 15360 match key99* count 101) \"2304\"2) (empty list or set) 更多的 scan 指令scan 指令是一系列指令，除了可以遍历所有的 key 之外，还可以对指定的容器集合进行遍历。 zscan 遍历 zset 集合元素， hscan 遍历 hash 字典的元素、 sscan 遍历 set 集合的元素。 注意点： SSCAN 命令、 HSCAN 命令和 ZSCAN 命令的第一个参数总是一个数据库键。 而 SCAN 命令则不需要在第一个参数提供任何数据库键 —— 因为它迭代的是当前数据库中的所有数据库键。 大 key 扫描有时候会因为业务人员使用不当，在 Redis 实例中会形成很大的对象，比如一个很大的 hash，一个很大的 zset 这都是经常出现的。 这样的对象对 Redis 的集群数据迁移带来了很大的问题，因为在集群环境下，如果某个 key 太大，会让数据导致迁移卡顿。 另外在内存分配上，如果一个 key 太大，那么当它需要扩容时，会一次性申请更大的一块内存，这也会导致卡顿。 如果这个大 key 被删除，内存会一次性回收，卡顿现象会再一次产生。 在平时的业务开发中，要尽量避免大 key 的产生。 如果你观察到 Redis 的内存大起大落，这极有可能是因为大 key 导致的，这时候你就需要定位出具体是那个 key， 进一步定位出具体的业务来源，然后再改进相关业务代码设计。 那如何定位大 key 呢？ 为了避免对线上 Redis 带来卡顿，这就要用到 scan 指令，对于扫描出来的每一个 key，使用 type 指令获得 key 的类型， 然后使用相应数据结构的 size 或者 len 方法来得到它的大小，对于每一种类型，保留大小的前 N 名作为扫描结果展示出来。 上面这样的过程需要编写脚本，比较繁琐，不过 Redis 官方已经在 redis-cli 指令中提供了这样的扫描功能，我们可以直接拿来即用。 12345678910111213141516171819202122232425262728293031$redis-cli --bigkeys# Scanning the entire keyspace to find biggest keys as well as# average sizes per key type. You can use -i 0.1 to sleep 0.1 sec# per 100 SCAN commands (not usually needed).[00.00%] Biggest string found so far 'key316' with 3 bytes[00.00%] Biggest string found so far 'key7806' with 4 bytes[12.79%] Biggest zset found so far 'salary' with 1 members[13.19%] Biggest string found so far 'counter:__rand_int__' with 6 bytes[13.50%] Biggest hash found so far 'websit' with 2 fields[14.37%] Biggest set found so far 'bbs' with 3 members[14.67%] Biggest hash found so far 'website' with 3 fields[30.41%] Biggest list found so far 'mylist' with 100000 items[95.53%] Biggest zset found so far 'page_rank' with 3 members-------- summary -------Sampled 10019 keys in the keyspace!Total key length in bytes is 68990 (avg len 6.89)Biggest string found 'counter:__rand_int__' has 6 bytesBiggest list found 'mylist' has 100000 itemsBiggest set found 'bbs' has 3 membersBiggest hash found 'website' has 3 fieldsBiggest zset found 'page_rank' has 3 members10011 strings with 38919 bytes (99.92% of keys, avg size 3.89)3 lists with 100003 items (00.03% of keys, avg size 33334.33)1 sets with 3 members (00.01% of keys, avg size 3.00)2 hashs with 5 fields (00.02% of keys, avg size 2.50)2 zsets with 4 members (00.02% of keys, avg size 2.00) 如果你担心这个指令会大幅抬升 Redis 的 ops 导致线上报警，还可以增加一个休眠参数。 12345678910111213141516171819202122232425262728293031$redis-cli --bigkeys -i 0.1# Scanning the entire keyspace to find biggest keys as well as# average sizes per key type. You can use -i 0.1 to sleep 0.1 sec# per 100 SCAN commands (not usually needed).[00.00%] Biggest string found so far 'key316' with 3 bytes[00.00%] Biggest string found so far 'key7806' with 4 bytes[12.79%] Biggest zset found so far 'salary' with 1 members[13.19%] Biggest string found so far 'counter:__rand_int__' with 6 bytes[13.50%] Biggest hash found so far 'websit' with 2 fields[14.37%] Biggest set found so far 'bbs' with 3 members[14.67%] Biggest hash found so far 'website' with 3 fields[30.41%] Biggest list found so far 'mylist' with 100000 items[95.53%] Biggest zset found so far 'page_rank' with 3 members-------- summary -------Sampled 10019 keys in the keyspace!Total key length in bytes is 68990 (avg len 6.89)Biggest string found 'counter:__rand_int__' has 6 bytesBiggest list found 'mylist' has 100000 itemsBiggest set found 'bbs' has 3 membersBiggest hash found 'website' has 3 fieldsBiggest zset found 'page_rank' has 3 members10011 strings with 38919 bytes (99.92% of keys, avg size 3.89)3 lists with 100003 items (00.03% of keys, avg size 33334.33)1 sets with 3 members (00.01% of keys, avg size 3.00)2 hashs with 5 fields (00.02% of keys, avg size 2.50)2 zsets with 4 members (00.02% of keys, avg size 2.00) 上面这个指令每隔 100 条 scan 指令就会休眠 0.1s，ops 就不会剧烈抬升，但是扫描的时间会变长。 需要注意的是，这个bigkeys得到的最大，不一定是最大。 说明原因前，首先说明bigkeys的原理，非常简单，通过scan命令遍历，各种不同数据结构的key，分别通过不同的命令得到最大的key： 如果是string结构，通过strlen判断； 如果是list结构，通过llen判断； 如果是hash结构，通过hlen判断； 如果是set结构，通过scard判断； 如果是sorted set结构，通过zcard判断。 正因为这样的判断方式，虽然string结构肯定可以正确的筛选出最占用缓存，也可以说最大的key。 但是list不一定，例如，现在有两个list类型的key，分别是：numberlist–[0,1,2]，stringlist–[“123456789123456789”]， 由于通过llen判断，所以numberlist要大于stringlist。 而事实上stringlist更占用内存。其他三种数据结构hash，set，sorted set都会存在这个问题。 使用bigkeys一定要注意这一点。 slowlog命令上面提到不能使用keys命令，如果就有开发这么做了呢，我们如何得知？ 与其他任意存储系统例如mysql，mongodb可以查看慢日志一样，redis也可以，即通过命令slowlog。 用法如下 SLOWLOG subcommand [argument] subcommand主要有： get，用法：slowlog get [argument]，获取argument参数指定数量的慢日志。 len，用法：slowlog len，总慢日志数量。 reset，用法：slowlog reset，清空慢日志。 12345678910111213141516171819202122232425262728$redis-cli slowlog get 51) 1) (integer) 2 2) (integer) 1537786953 3) (integer) 17980 4) 1) \"scan\" 2) \"0\" 3) \"match\" 4) \"key99*\" 5) \"count\" 6) \"1000\" 5) \"127.0.0.1:50129\" 6) \"\"2) 1) (integer) 1 2) (integer) 1537785886 3) (integer) 39537 4) 1) \"keys\" 2) \"*\" 5) \"127.0.0.1:49701\" 6) \"\"3) 1) (integer) 0 2) (integer) 1537681701 3) (integer) 18276 4) 1) \"ZADD\" 2) \"page_rank\" 3) \"10\" 4) \"google.com\" 5) \"127.0.0.1:52334\" 6) \"\" 命令耗时超过多少才会保存到slowlog中，可以通过命令config set slowlog-log-slower-than 2000配置并且不需要重启redis。 注意：单位是微妙，2000微妙即2毫秒。 rename-command为了防止把问题带到生产环境，我们可以通过配置文件重命名一些危险命令， 例如keys等一些高危命令。操作非常简单， 只需要在conf配置文件增加如下所示配置即可： rename-command flushdb flushddbb rename-command flushall flushallall rename-command keys keysys","tags":[]},{"title":"25岁","date":"2019-09-16T02:02:50.041Z","path":"2019/09/16/25岁/","text":"1这是崔斯特的第九十七篇原创文章 25岁总结 (๑• . •๑) Time Flies. 如果说我能活到100岁，那么现在我已经度过人生的四分之一；如果我只能活到75岁，那么我已经度过了人生的三分之一。 发现自己并写不下去这种文章，以后还是回归到技术吧。 工作来到拼多多我觉得过的还是挺好的，虽然下班晚了，但是上班也晚，现在已经可以找到平衡点了。身体应该问题不大， 没有什么问题，就是自己确实是没有去锻炼，这可能会成为一个隐患。还是需要引起注意的。😔 至于工作内容方面，学习是每天工作中都会遇到的。基本上都是再用JAVA，这是个机会，以后回到武汉也可以找到JAVA相关的工作。 JAVA在某些方面用起来真的比Python要爽，但如果是写一些快速的脚本，Python当然是最佳选择。很庆幸自己可以通过爬虫转型到JAVA。 在现在这个大环境下，拼多多至少不会裁员，而且每年都有涨薪，还是值得期待的，我觉得应该会在拼多多呆个三到五年。如果我还留在造数，现在的结果肯定不会是这样吧。 生活每周六休息一天。如果是双休，我肯定是在打游戏，倒还不如来加班，趁着年轻多赚点钱，她还有一屁股债呢。 我是一个不愿意出门的人，无论什么放假，都不想出门去看人，自己待在家里很舒服。 至于结婚这个事情，我觉得应该是2021年吧，给自己两年时间。太早结婚并不是一个明智选择，我觉得还不够了解她，也发现了很多缺点，我要学会去包容。感觉我们很难走到最后。","tags":[]},{"title":"来拼多多的第一个月","date":"2019-07-27T13:01:21.858Z","path":"2019/07/27/来拼多多的第一个月/","text":"1这是崔斯特的第九十六篇原创文章 七月总结 (๑• . •๑) 2019年下半年，也就是7月1日，我入职了拼多多。算起来到现在已经4周了，想记录下现在到这边来的感受。 工作先说下在这边的工作，爬虫只是一方面，数据下游还有很长的应用链，用同事的话说，爬虫是大动脉，挂了就完了。就目前我做的一个月来说，有开发爬虫，提供数据，也有改进一些业务代码。语言的话，大环境是JAVA，爬虫有一部分是Python，我有部分业务也需要使用JAVA，然后就开始 JAVA7天从入门到精通，照着别人的代码一步步写。如何声明一个数组、Map，如何添加变量，函数返回的是什么，JSON解析的类型是什么，这种基础问题，全是一步步Google下来的，就这样下来，也算是完成了其中一个小模块，这周上线了，很有成就感😯。 工作内容确实会比较多，每天都会有做不完的事情，只要自己在其中能感受到成就感，能发挥自己的价值，每天能学到新东西，感觉就还好。只是刚来会有些不适应，慢慢就好了。 团队就团队来说的话，我算是其中履历最差的，这里有阿里、腾讯、百度等等大厂来的，应届生也是985的，还有位5年经验的爬虫大佬带我。很多问题都可以问他们找到答案，是一个可以快速成长的地方。 拼多多现在好像有五千多号员工了，我入职那天来了几百号新员工。大佬们没有独立的办公室，所以经常会见到大佬，曾经每天上厕所都要路过一位创始人的工位。 工作时间这个在来之前就问能不能接受“11116”，也就是早上11点到晚上11点，一周上六天班，来之前觉得挺吓人的，但是现在觉得还是OK的。 自己本来就是一个夜猫子，我的一天作息基本是这样： 凌晨1点之前睡觉，早上9点起床，保证每天8个小时以上 10点到公司吃个早餐，学一个小时，现在是在看JAVA，11点正式开始工作 12点吃午饭，吃完会睡半小时，1点半开始工作 6点吃晚餐，吃完了会下去走走，7点半上来工作 晚上11点下班（有些团队是10点下班） 毕竟晚上11点才能下班，所以现在也没时间写博客了，也就每周六放假了可以写点。以后爬虫的可能会少一点，会写一写JAVA基础。最近也有朋友在问怎么学爬虫，我的建议是，如果爬虫只是你的副业，掌握多线程就可以了，如果你要转行学爬虫的话，并不建议，学JAVA前景会很好，Python岗位太少。国内的话，还是JAVA第一。 关于拼多多在我来之前，我也像你们一样，内心是非常抵触拼多多的，全是假货，坚决不会在上面买的。面了拼多多之后，下载了拼多多App，领了一张5元券，买了一个小风扇，最后花了0.09元。挺好用的，真香。 之后会经常在上面买一些小东西，确实很便宜，有个同事在拼多多上买的2019款的MBP，没啥问题。以一个内部员工的身份告诉你，拼多多里的“百亿补贴”和“品牌馆”是没有假货的，大家买买买。 最后，我们这个团队还在招人，爬虫、JAVA、算法都要，3~5年经验，这边社招的话会问很多底层的东西，不止要求你会用，还会要求你知道其中的原理。","tags":[]},{"title":"比你优秀的人不可怕，可怕的是比你优秀的人比你更努力","date":"2019-07-13T07:32:06.000Z","path":"2019/07/13/比你优秀的人不可怕，可怕的是比你优秀的人比你更努力/","text":"这是崔斯特的第九十五篇原创文章 比你优秀的人不可怕，可怕的是比你优秀的人比你更努力 (๑• . •๑) 来到拼多多有半个月了，自己总结下来，有一句话就是标题说的：比你优秀的人不可怕，可怕的是比你优秀的人比你更努力。 这句话是我的领导经常挂在嘴边的，他是一个很厉害的人，之前做C++的，来到这边开始各种学习，JAVA、Python、Node，这些都是他自学的，因为他基础好。他常常跟我说，你不是计算机专业出生，你要好好补下linux基础，然后推荐我去看《unix环境编程》。说你看完这本书之后，你和别人交谈就会有料。 这边一般是11116，我一般会10点40多过来打卡，晚上11点20下班，回到家基本就不会再学习了。他说他每天睡6个小时，晚上回去看慕课网、早上起来也看，每次我下班了他都还没下班。他跟我说他周六还会到公司这边来学习，看视频教程，学SpringBoot，给我看他周六的打卡记录。当时我是震惊的，因为大家都会觉得在拼多多加班多，工作很累，好不容易放假了，要好好休息。 就我自己来说，这边工作确实是很有挑战的。基础差是一方面，还有就是要学习新语言，这边基本上都是JAVA，之前我一直用Python。刚开始真的很难受，各种类型和跳转已经把我逼疯了，两周下来，我也能开始写一些SpringBoot。现在我把每天起床时间提前一小时，可以先来到公司学一些基础。 JAVA每次声明变量就需要给定类型，这一点刚开始很难适应，特别是处理JSON感觉很蛋疼，Python的话就根本什么都不用管，拿来就上。这一点上确实是Python简单，但是一些服务上肯定还是会选择JAVA，语言性质决定的。 还有一点感受，爬虫真的只是很小很小一部分，要更多地接触到爬虫的上下游，要把原始数据变为价值数据是一个很难的过程，这就是全新的挑战。 最后，打一个小广告，拼多多现在发展的很快，疯狂招人。这边工作时间是11116，新员工第一年是可以住宿舍的，包三餐，吃的还行（拼多多工资给的确实高），可以去Boss直聘上看拼多多的招聘岗位，有意向的可以发简历给我，然后校招的在这里拼多多校招内推","tags":[{"name":"学习","slug":"学习","permalink":"https://zhangslob.github.io/tags/学习/"},{"name":"思考","slug":"思考","permalink":"https://zhangslob.github.io/tags/思考/"}]},{"title":"【面试高频问题】线程、进程、协程","date":"2019-06-22T02:30:17.000Z","path":"2019/06/22/【面试高频问题】线程、进程、协程/","text":"1这是崔斯特的第九十四篇原创文章 一些面试题 (๑• . •๑) 需要先对 IO 的概念有一定的认识: IO在计算机中指Input/Output，也就是输入和输出。 并发与并行 并发：在操作系统中，某一时间段，几个程序在同一个CPU上运行，但在任意一个时间点上，只有一个程序在CPU上运行。 当有多个线程时，如果系统只有一个CPU，那么CPU不可能真正同时进行多个线程，CPU的运行时间会被划分成若干个时间段，每个时间段分配给各个线程去执行，一个时间段里某个线程运行时，其他线程处于挂起状态，这就是并发。并发解决了程序排队等待的问题，如果一个程序发生阻塞，其他程序仍然可以正常执行。 并行：当操作系统有多个CPU时，一个CPU处理A线程，另一个CPU处理B线程，两个线程互相不抢占CPU资源，可以同时进行，这种方式成为并行。 区别 并发只是在宏观上给人感觉有多个程序在同时运行，但在实际的单CPU系统中，每一时刻只有一个程序在运行，微观上这些程序是分时交替执行。 在多CPU系统中，将这些并发执行的程序分配到不同的CPU上处理，每个CPU用来处理一个程序，这样多个程序便可以实现同时执行。 知乎上高赞例子： 你吃饭吃到一半，电话来了，你一直到吃完了以后才去接，这就说明你不支持并发也不支持并行。 你吃饭吃到一半，电话来了，你停了下来接了电话，接完后继续吃饭，这说明你支持并发。 你吃饭吃到一半，电话来了，你一边打电话一边吃饭，这说明你支持并行。 并发的关键是你有处理多个任务的能力，不一定要同时。并行的关键是你有同时处理多个任务的能力。所以我认为它们最关键的点就是：是否是『同时』。 进程 一个进程好比是一个程序，它是 资源分配的最小单位 。同一时刻执行的进程数不会超过核心数。不过如果问单核CPU能否运行多进程？答案又是肯定的。单核CPU也可以运行多进程，只不过不是同时的，而是极快地在进程间来回切换实现的多进程。举个简单的例子，就算是十年前的单核CPU的电脑，也可以聊QQ的同时看视频。 电脑中有许多进程需要处于「同时」开启的状态，而利用CPU在进程间的快速切换，可以实现「同时」运行多个程序。而进程切换则意味着需要保留进程切换前的状态，以备切换回去的时候能够继续接着工作。所以进程拥有自己的地址空间，全局变量，文件描述符，各种硬件等等资源。操作系统通过调度CPU去执行进程的记录、回复、切换等等。 线程 如果说进程和进程之间相当于程序与程序之间的关系，那么线程与线程之间就相当于程序内的任务和任务之间的关系。所以线程是依赖于进程的，也称为 「微进程」 。它是 程序执行过程中的最小单元 。 一个程序内包含了多种任务。打个比方，用播放器看视频的时候，视频输出的画面和声音可以认为是两种任务。当你拖动进度条的时候又触发了另外一种任务。拖动进度条会导致画面和声音都发生变化，如果进程里没有线程的话，那么可能发生的情况就是： 拖动进度条-&gt;画面更新-&gt;声音更新。你会明显感到画面和声音和进度条不同步。 但是加上了线程之后，线程能够共享进程的大部分资源，并参与CPU的调度。意味着它能够在进程间进行切换，实现「并发」，从而反馈到使用上就是拖动进度条的同时，画面和声音都同步了。所以我们经常能听到的一个词是「多线程」，就是把一个程序分成多个任务去跑，让任务更快处理。不过线程和线程之间由于某些资源是独占的，会导致锁的问题。例如Python的GIL多线程锁。 进程与线程的区别 进程是CPU资源分配的基本单位，线程是独立运行和独立调度的基本单位（CPU上真正运行的是线程）。 进程拥有自己的资源空间，一个进程包含若干个线程，线程与CPU资源分配无关，多个线程共享同一进程内的资源。 线程的调度与切换比进程快很多。 CPU密集型代码(各种循环处理、计算等等)：使用多进程。IO密集型代码(文件处理、网络爬虫等)：使用多线程 阻塞与非阻塞 阻塞是指调用线程或者进程被操作系统挂起。 非阻塞是指调用线程或者进程不会被操作系统挂起。 同步与异步 同步是阻塞模式，异步是非阻塞模式。 同步就是指一个进程在执行某个请求的时候，若该请求需要一段时间才能返回信息，那么这个进程将会一直等待下去，知道收到返回信息才继续执行下去； 异步是指进程不需要一直等下去，而是继续执行下面的操作，不管其他进程的状态。当有消息返回式系统会通知进程进行处理，这样可以提高执行的效率。 由调用方盲目主动问询的方式是同步调用，由被调用方主动通知调用方任务已完成的方式是异步调用。看下图 协程 协程，又称微线程，纤程。英文名Coroutine。一句话说明什么是线程：协程是一种用户态的轻量级线程。 协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。因此： 协程能保留上一次调用时的状态（即所有局部状态的一个特定组合），每次过程重入时，就相当于进入上一次调用的状态，换种说法：进入上一次离开时所处逻辑流的位置。 协程的好处： 无需线程上下文切换的开销 无需原子操作锁定及同步的开销 方便切换控制流，简化编程模型 高并发+高扩展性+低成本：一个CPU支持上万的协程都不是问题。所以很适合用于高并发处理。 缺点： 无法利用多核资源：协程的本质是个单线程,它不能同时将 单个CPU 的多个核用上,协程需要和进程配合才能运行在多CPU上.当然我们日常所编写的绝大部分应用都没有这个必要，除非是cpu密集型应用。 进行阻塞（Blocking）操作（如IO时）会阻塞掉整个程序 最佳实践 线程和协程推荐在IO密集型的任务(比如网络调用)中使用，而在CPU密集型的任务中，表现较差。 对于CPU密集型的任务，则需要多个进程，绕开GIL的限制，利用所有可用的CPU核心，提高效率。 所以大并发下的最佳实践就是多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。 顺便一提，非常流行的一个爬虫框架Scrapy就是用到异步框架Twisted来进行任务的调度，这也是Scrapy框架高性能的原因之一。 最后推荐阅读：深入理解 Python 异步编程(上) 阅读原文","tags":[{"name":"面试","slug":"面试","permalink":"https://zhangslob.github.io/tags/面试/"}]},{"title":"Docker通过EFK（Elasticsearch + Fluentd + Kibana）查询日志","date":"2019-04-20T12:47:51.000Z","path":"2019/04/20/Docker通过EFK（Elasticsearch-Fluentd-Kibana）查询日志/","text":"这是崔斯特的第九十三篇原创文章 部署相关 (๑• . •๑) Docker通过EFK（Elasticsearch + Fluentd + Kibana）查询日志这篇文章主要是参考Docker Logging via EFK (Elasticsearch + Fluentd + Kibana) Stack with Docker Compose，并在其基础上做了一些修改。 Elasticsearch是一个开源搜索引擎，以易用性着称。kibana是一个图形界面，可以在上面条件检索存储在ElasticSearch里数据，相当于提供了ES的可视化操作管理器。截图如下： 这里基本的架构是这样的 这里主要解决的问题是日志查询，日志来源是docker。我们使用docker部署任务时，可以使用docker logs -f &lt;容器id&gt;查看日志，也可以去/var/lib/docker/containers/&lt;容器id&gt;/&lt;容器id&gt;-json.log查看日志文件。但是这都很难去做查询，本文介绍的EFK就可以解决这个问题。 我们会创建四个容器： httpd (发送日志给EFK) Fluentd Elasticsearch Kibana 环境准备请安装最新的docker及docker-compose，老版本会有些问题。 这里是我的版本 123456789101112131415161718192021222324➜ ~ docker versionClient: Docker Engine - Community Version: 18.09.2 API version: 1.39 Go version: go1.10.8 Git commit: 6247962 Built: Sun Feb 10 04:12:39 2019 OS/Arch: darwin/amd64 Experimental: falseServer: Docker Engine - Community Engine: Version: 18.09.2 API version: 1.39 (minimum version 1.12) Go version: go1.10.6 Git commit: 6247962 Built: Sun Feb 10 04:13:06 2019 OS/Arch: linux/amd64 Experimental: false➜ ~ docker-compose versiondocker-compose version 1.23.2, build 1110ad01docker-py version: 3.6.0CPython version: 3.6.6OpenSSL version: OpenSSL 1.1.0h 27 Mar 2018 编写docker-compose.ymlDocker Compose是一个用于定义和运行多容器Docker应用程序的工具。 12345678910111213141516171819202122232425262728293031323334353637version: '2'services: web: image: httpd ports: - \"1080:80\" #避免和默认的80端口冲突 links: - fluentd logging: driver: \"fluentd\" options: fluentd-address: localhost:24224 tag: httpd.access fluentd: build: ./fluentd volumes: - ./fluentd/conf:/fluentd/etc links: - \"elasticsearch\" ports: - \"24224:24224\" - \"24224:24224/udp\" elasticsearch: image: elasticsearch:5.3.0 expose: - 9200 ports: - \"9200:9200\" kibana: image: kibana:5.3.0 links: - \"elasticsearch\" ports: - \"5601:5601\" 所有web里的日志会自动发送到fluentd-address: localhost:24224，也就是fluentd容器。 Elasticsearch 和 Kibana并不支持最新的版本，这里选择的是5.3.0，如果想要选择更新的，可以去这里查看 Elasticsearch image tags in DockerHub Kibana image tags in DockerHub Fluentd的配置和插件新建文件fluentd/Dockerfile，使用官方镜像Fluentd’s official Docker image，安装需要的插件 123# fluentd/DockerfileFROM fluent/fluentd:v0.12-debianRUN [\"gem\", \"install\", \"fluent-plugin-elasticsearch\", \"--no-rdoc\", \"--no-ri\", \"--version\", \"1.9.7\"] 然后新建文件fluentd/conf/fluent.conf，编写Fluentd的配置文件 123456789101112131415161718192021222324# fluentd/conf/fluent.conf&lt;source&gt; @type forward port 24224 bind 0.0.0.0&lt;/source&gt;&lt;match *.**&gt; @type copy &lt;store&gt; @type elasticsearch host elasticsearch port 9200 logstash_format true logstash_prefix fluentd logstash_dateformat %Y%m%d include_tag_key true type_name access_log tag_key @log_name flush_interval 1s &lt;/store&gt; &lt;store&gt; @type stdout &lt;/store&gt;&lt;/match&gt; 官方设置文档config-file 启动容器在后台启动，使用docker-compose up -d 12345➜ docker-compose up -dRecreating temp_elasticsearch_1 ... doneRecreating temp_kibana_1 ... doneRecreating temp_fluentd_1 ... doneRecreating temp_web_1 ... done 查看所有容器 123456➜ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES03d589afc2fd httpd \"httpd-foreground\" About a minute ago Up About a minute 0.0.0.0:1080-&gt;80/tcp temp_web_1e84d0753ee58 kibana:5.3.0 \"/docker-entrypoint.…\" About a minute ago Up About a minute 0.0.0.0:5601-&gt;5601/tcp temp_kibana_10f069b8b138f temp_fluentd \"tini -- /bin/entryp…\" About a minute ago Up About a minute 5140/tcp, 0.0.0.0:24224-&gt;24224/tcp, 0.0.0.0:24224-&gt;24224/udp temp_fluentd_1fb4c6255e7ed elasticsearch:5.3.0 \"/docker-entrypoint.…\" 2 minutes ago Up About a minute 0.0.0.0:9200-&gt;9200/tcp, 9300/tcp temp_elasticsearch_1 产生日志原文是repeat 10 curl http://localhost:80/，但是我在docker-compose.yml中修改了端口，所以我这里是 1234567891011➜ repeat 10 curl http://localhost:1080/&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 查看日志打开http://localhost:5601，提示需要先建索引，输入fluentd-*刷新即可 去Discover页面，然后就可以看到之前的日志了。 如何接入其他docker日志这里是以docker-compose形式启动的一个服务，如果还有别的任务需要将日志发送到fluentd，需要这几个步骤。 默认情况下，docker-compose会为我们的应用创建一个网络，服务的每个容器都会加入该网络中。这样，容器就可被该网络中的其他容器访问，不仅如此，该容器还能以服务名称作为hostname被其他容器访问。 所以我们首先需要找到我们现在创建的EFK的网络名， 1234567891011➜ docker network lsNETWORK ID NAME DRIVER SCOPEfd428cfa7264 bridge bridge locale08505f7bfe6 docker-compose-efk_default bridge local650f2f690175 docker-efk_default bridge local35f17aeb61e9 docker-elk_elk bridge localdb29d28aa5cc esnet bridge local56ea915974c9 host host local4d0207065fb8 none null local3dd60d8ddfce temp_default bridge localcf1a06702ae8 web_default bridge local 我是在temp目录下创建的docker-compose.yml文件，所以这里默认的名字就是temp_default。 再看看之前web的设置 1234567891011web: image: httpd ports: - \"1080:80\" #避免和默认的80端口冲突 links: - fluentd logging: driver: \"fluentd\" options: fluentd-address: localhost:24224 tag: httpd.access 有几个关键设置是：links和logging，link 用于容器直接的互通，logging则是日志的输出设置。 那我们这里再启动一个新docker需要这些设置 1234567docker run --link temp_fluentd_1 \\ --net temp_default \\ --log-driver=fluentd \\ --log-opt fluentd-address=localhost:24224 \\ --log-opt tag=httpd.access \\ -d hello-world 我们去kibana看看，果然，日志已经发送到kibana了 将log添加到选择字段，日志阅读起来更佳 版本选择上面说到这里选择的是5.3.0，我们试试新版本6.7.1，再看看kibana。使用docker-compose stop停止服务，修改docker-compose文件再启动。 但是已启动es就挂了，最 后查看日志docker-compose logs | grep elasticsearch_1，发现如下错误： “Native controller process has stopped - no new native processes can be started” 最后在giuhub上找到答案：es crashes with “Native controller process has stopped - no new native processes can be started” 解决方法是，提高docker可用内存，mac上是这样设置，把内存从2G提高到4G，再启动就成功了。 这里是6.7.1的界面 最后想要做的就是如何在一台服务器上搜集所有的日志，理论上来说，只需要一台服务器部署上EFK，暴露端口，其他服务器去发送即可，实际上还没试过。 如果有好的意见，欢迎来提。","tags":[{"name":"Docker","slug":"Docker","permalink":"https://zhangslob.github.io/tags/Docker/"},{"name":"EFK","slug":"EFK","permalink":"https://zhangslob.github.io/tags/EFK/"}]},{"title":"对第一份工作的总结","date":"2019-04-07T05:07:30.000Z","path":"2019/04/07/对第一份工作的总结/","text":"这是崔斯特的第九十二篇原创文章 两年工作的总结 (๑• . •๑) 有段时间没写东西了，如果你在萌新群里，应该知道我最近换工作的事。简单说，就是辞去了北京的工作，在杭州找到了新工作。 看到手里的离职证明，想着还是写点什么，记录下我毕业后的第一份工作，在这里我做了什么，学到了什么。 到来17年6月分毕业户就来到了北京，我清楚的记得去北京是坐的普快，那天晚上是欧冠决赛，皇马是冠军。我去查了下，是2017年06月04，皇马赢了尤文图斯。(我是C罗球迷) 开始一个人来到北京，确实有很多不适应的地方。首先住的地方，朋友帮我在6号线的东夏园租了一个1000元的小房子，小区特别偏僻，吃饭的地方很少，而且北方煎饼果子和面食特别多，刚开始很不适应，后来慢慢好了点。 公司刚开始在青年路站，后来搬去了十里堡。所以出行还是很方便的，就在6号线，早上上班还有座位可以坐。造数之前的产品主要是数据采集工具，现在可以免费试用：造数 - 新一代智能云爬虫，虽然很多网站用不了。 我刚开始做的有一部分运营的工作、也有配合其他人录制了爬虫视频教程，现在回去简直没法看，感觉太傻了，你们千万别去看。如果要学爬虫的话，等我把这个写完：数据采集从入门到放弃 转岗我是在造数内部完成的转岗，成为爬虫工程师。一方面是我有爬虫方面的基础，并一方面业务需要招了需要很多数据。转岗确实不容易，需要学习很多东西，也需要一点运气，感觉最重要是就是坚持吧。 做的第一个项目是有人带，前前后后有一个多月才算完工，虽然最终的爬虫很简单，中间却踩了不少弯路，也在使用中对Scrapy越来越熟悉，也知道如何在Scrapy的基础上再次开发，也就是造轮子，这些轮子在后来此类的项目中基本是在一直使用。 再到后来基本上就是自己独立负责N个项目了，感觉每做一个项目，就遇到一些坑，然后就会新学到一些技能，成长会比较快。这也是一个很有趣的过程。 会有加班，最晚的一次是凌晨两点吧，其实我现在回想加班的原因主要是自己能力太差，如果当时水平再好点，就不至于加班这么晚。 学习我觉得自己比较大的优势在于学习吧，从最开始学习编程开始，就会把自己所学记录下来，刚开始是在知乎上，后来用hexo搭建了自己的博客，后来写公众号，打算以后再用tornado来写个博客。 关于学习这个东西，所学一定要有所用，不然一定会忘记。某个东西在需要的时候再去学习，然后将所学用到实际项目中，再有人指点一二，会很快掌握新的技能。 我一般遇到问题会有如下解决方法： 首先肯定是去Google，注意需要把你的中文先翻译为英文 根据搜索结果去尝试，stackoverflow实在是太好用，可以解决一半的问题 如果尝试失败，再去问可能知道这个问题的人。不建议在网上问，效率低下，直接去找公司大佬问。 在通过各种途径解决该问题后，我会记录下来，一般会发在博客中。所以你看我的博客里，大多是针对某一个具体问题的解决方案，很少有类似于”我爬了xx数据，研究出xx结论”此类文章。 以后再遇到此类问题，我会迅速想到自己写的，就会打开博客，找到那篇文章阅读。 造数有很多技术大牛，在某些领域上专攻，所以在遇到问题基本上都可以去问他们，这种方法比在微信上问别人好得多。 技能列举下自己在造数所学到的或者能使用的技能点，我也在简历中有写出来： Python3、Scala、Go、SQl Requests、Scrapy、Scrapy-Redis、Bloom filter IOS逆向（静态IDA、动态hook）、JS断点 Selenium、Appium、Airtest MongoDB、MySQL、Redis、Docker Xpath、bs4、css、jQuery、Regular Expression Fiddler、Charles、Wireshark、Mitmproxy Flask、Pandas、Spark、TensorFlow 以上这些技能点基本上都可以在历史文章中找到身影，这些也是做爬虫所需。当然，也还有一些我不大了解的，像aiohttp、rabbitmq、celery等，没有在实际项目中使用过，不敢说自己会用。在爬虫部署、框架开发、代理池设计等方面也需要提高。 离开今年开年后，我搬到沙河，而造数在朝阳，每天6点半起床，坐两小时地铁，距离原因，所以提了离职，在朋友内推下来到小黑鱼，在杭州。 总的来说，在造数真的学到了很多，作为毕业后的第一份工作，虽然是小公司，我觉得很开心的。看我的github仓库提交记录，我只想说：真实! 2019年4月4日来到杭州，那天还下着雨，所以记得很清楚。现在已经安定下来，在公司5分钟路程租到了房子。明天就会入职新公司，希望可以好好做吧。刚来到杭州没几天，我觉得杭州很不错，住着比北京舒服，环境比北京好。如果有杭州的小伙伴，欢迎面基。","tags":[{"name":"总结","slug":"总结","permalink":"https://zhangslob.github.io/tags/总结/"}]},{"title":"scrapy去重与scrapy_redis去重与布隆过滤器","date":"2019-03-26T07:35:53.000Z","path":"2019/03/26/scrapy与scrapy-redis的去重/","text":"1这是崔斯特的第九十一篇原创文章 在开始介绍scrapy的去重之前，先想想我们是怎么对requests对去重的。requests只是下载器，本身并没有提供去重功能。所以我们需要自己去做。很典型的做法是事先定义一个去重队列，判断抓取的url是否在其中，如下： 1234567crawled_urls = set()def check_url(url): if url not in crawled_urls: return True return False 此时的集合是保存在内存中的，随着爬虫抓取内容变多，该集合会越来越大，有什么办法呢？ 接着往下看，你会知道的。 scrapy的去重scrapy对request不做去重很简单，只需要在request对象中设置dont_filter为True，如 1yield scrapy.Request(url, callback=self.get_response, dont_filter=True) 看看源码是如何做的，位置 1234567891011121314151617181920_fingerprint_cache = weakref.WeakKeyDictionary()def request_fingerprint(request, include_headers=None): if include_headers: include_headers = tuple(to_bytes(h.lower()) for h in sorted(include_headers)) cache = _fingerprint_cache.setdefault(request, &#123;&#125;) if include_headers not in cache: fp = hashlib.sha1() fp.update(to_bytes(request.method)) fp.update(to_bytes(canonicalize_url(request.url))) fp.update(request.body or b'') if include_headers: for hdr in include_headers: if hdr in request.headers: fp.update(hdr) for v in request.headers.getlist(hdr): fp.update(v) cache[include_headers] = fp.hexdigest() return cache[include_headers] 注释过多，我就删掉了。谷歌翻译 + 人翻 12345678910111213141516返回请求指纹请求指纹是唯一标识请求指向的资源的哈希。 例如，请使用以下两个网址：http://www.example.com/query?id=111&amp;cat=222http://www.example.com/query?cat=222&amp;id=111即使这两个不同的URL都指向相同的资源并且是等价的（即，它们应该返回相同的响应）另一个例子是用于存储会话ID的cookie。 假设以下页面仅可供经过身份验证的用户访问：http://www.example.com/members/offers.html许多网站使用cookie来存储会话ID，这会随机添加字段到HTTP请求，因此在计算时应该被忽略指纹。因此，计算时默认会忽略request headers。 如果要包含特定headers，请使用include_headers参数，它是要计算Request headers的列表。 其实就是说：scrapy使用sha1算法，对每一个request对象加密，生成40为十六进制数，如：’fad8cefa4d6198af8cb1dcf46add2941b4d32d78’。 我们看源码，重点是一下三行 1234fp = hashlib.sha1()fp.update(to_bytes(request.method))fp.update(to_bytes(canonicalize_url(request.url)))fp.update(request.body or b'') 如果没有自定义headers，只计算method、url、和二进制body，我们来计算下，代码： 123print(request_fingerprint(scrapy.Request('http://www.example.com/query?id=111&amp;cat=222')))print(request_fingerprint(scrapy.Request('http://www.example.com/query?cat=222&amp;id=111')))print(request_fingerprint(scrapy.Request('http://www.example.com/query'))) 输出： 123fad8cefa4d6198af8cb1dcf46add2941b4d32d78fad8cefa4d6198af8cb1dcf46add2941b4d32d78b64c43a23f5e8b99e19990ce07b75c295165a923 可以看到第一条和第二条的密码是一样的，是因为调用了canonicalize_url方法，该方法返回如下 12345678910&gt;&gt;&gt; import w3lib.url&gt;&gt;&gt;&gt;&gt;&gt; # sorting query arguments&gt;&gt;&gt; w3lib.url.canonicalize_url('http://www.example.com/do?c=3&amp;b=5&amp;b=2&amp;a=50')'http://www.example.com/do?a=50&amp;b=2&amp;b=5&amp;c=3'&gt;&gt;&gt;&gt;&gt;&gt; # UTF-8 conversion + percent-encoding of non-ASCII characters&gt;&gt;&gt; w3lib.url.canonicalize_url(u'http://www.example.com/r\\u00e9sum\\u00e9')'http://www.example.com/r%C3%A9sum%C3%A9'&gt;&gt;&gt; scrapy的去重默认会保存到内存中，如果任务重启，会导致内存中所有去重队列消失 scrapy-redis的去重scrapy-redis重写了scrapy的调度器和去重队列，所以需要在settings中修改如下两列 12345# Enables scheduling storing requests queue in redis.SCHEDULER = \"scrapy_redis.scheduler.Scheduler\"# Ensure all spiders share same duplicates filter through redis.DUPEFILTER_CLASS = \"scrapy_redis.dupefilter.RFPDupeFilter\" 一般我们会在redis中看到这两个，分别是去重队列和种子链接 先看看代码：重要代码 123456789101112131415161718192021222324def request_seen(self, request): \"\"\"Returns True if request was already seen. Parameters ---------- request : scrapy.http.Request Returns ------- bool \"\"\" fp = self.request_fingerprint(request) # This returns the number of values added, zero if already exists. added = self.server.sadd(self.key, fp) return added == 0def request_fingerprint(self, request): \"\"\"Returns a fingerprint for a given request. Parameters ---------- request : scrapy.http.Request Returns ------- str \"\"\" return request_fingerprint(request) 首先拿到scrapy.http.Request会先调用self.request_fingerprint去计算，也就是scrapy的sha1算法去加密，然后会向redis中添加该指纹。 该函数的作用是：计算该请求指纹，添加到redis的去重队列，如果已经存在该指纹，返回True。 我们可以看到，只要有在settings中添加DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;，就会在redis中新加一列去重队列，说下这样做的优劣势： 优点：将内存中的去重队列序列化到redis中，及时爬虫重启或者关闭，也可以再次使用，你可以使用SCHEDULER_PERSIST来调整缓存 缺点：如果你需要去重的指纹过大，redis占用空间过大。8GB=8589934592Bytes，平均一个去重指纹40Bytes，约可以存储214,748,000个(2亿)。所以在做关系网络爬虫中，序列化到redis中可能并不是很好，保存在内存中也不好，所以就产生了布隆过滤器。 布隆过滤器它的原理是将一个元素通过 k 个哈希函数，将元素映射为 k 个比特位，在 bitmap 中把它们置为 1。在验证的时候只需要验证这些比特位是否都是 1 即可，如果其中有一个为 0，那么元素一定不在集合里，如果全为 1，则很可能在集合里。（因为可能会有其它的元素也映射到相应的比特位上） 同时这也导致不能从 Bloom filter 中删除某个元素，无法确定这个元素一定在集合中。以及带来了误报的问题，当里面的数据越来越多，这个可能在集合中的靠谱程度就越来越低。（由于哈希碰撞，可能导致把不属于集合内的元素认为属于该集合） 布隆过滤器的缺点是错判，就是说，不在的一定不在，在的不一定在，而且无法删除其中数据。 123456789&gt;&gt;&gt; import pybloomfilter&gt;&gt;&gt; fruit = pybloomfilter.BloomFilter(100000, 0.1, '/tmp/words.bloom')&gt;&gt;&gt; fruit.update(('apple', 'pear', 'orange', 'apple'))&gt;&gt;&gt; len(fruit)3&gt;&gt;&gt; 'mike' in fruitFalse&gt;&gt;&gt; 'apple' in fruitTrue python3使用pybloomfilter的例子。 那么如何在scrapy中使用布隆过滤器呢，崔大大已经写好了，地址：ScrapyRedisBloomFilter，已经打包好，可以直接安装 1pip install scrapy-redis-bloomfilter 在settings中这样配置： 1234567891011121314151617# Ensure use this SchedulerSCHEDULER = \"scrapy_redis_bloomfilter.scheduler.Scheduler\"# Ensure all spiders share same duplicates filter through redisDUPEFILTER_CLASS = \"scrapy_redis_bloomfilter.dupefilter.RFPDupeFilter\"# Redis URLREDIS_URL = 'redis://localhost:6379/0'# Number of Hash Functions to use, defaults to 6BLOOMFILTER_HASH_NUMBER = 6# Redis Memory Bit of Bloomfilter Usage, 30 means 2^30 = 128MB, defaults to 30BLOOMFILTER_BIT = 30# PersistSCHEDULER_PERSIST = True 其实也是修改了调度器与去重方法，有兴趣的可以了解下。","tags":[{"name":"scrapy","slug":"scrapy","permalink":"https://zhangslob.github.io/tags/scrapy/"},{"name":"scrapy_redis","slug":"scrapy-redis","permalink":"https://zhangslob.github.io/tags/scrapy-redis/"}]},{"title":"Python&Golang:冒泡排序","date":"2019-03-21T07:08:09.000Z","path":"2019/03/21/Python-Golang-冒泡排序/","text":"使用Python&amp;Golang尝试 (๑• . •๑) 1这是崔斯特的第九十篇原创文章 冒泡排序定义冒泡排序（英语：Bubble Sort）是一种简单的排序算法。它重复地遍历要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。遍历数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 冒泡排序算法的运作如下： 比较相邻的元素。如果第一个比第二个大（升序），就交换他们两个。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 伪代码123456789function bubble_sort (array, length) &#123; var i, j; for(i from 1 to length-1)&#123; for(j from 0 to length-1-i)&#123; if (array[j] &gt; array[j+1]) swap(array[j], array[j+1]) &#125; &#125;&#125; 123456789函数 冒泡排序 输入 一个数组名称为array 其长度为length i 从 1 到 (length - 1) j 从 0 到 (length - 1 - i) 如果 array[j] &gt; array[j + 1] 交换 array[j] 和 array[j + 1] 的值 如果结束 j循环结束 i循环结束 函数结束 可以看到每经过以此排序之后，将会有一个数字(最后那个)确定，所以下一次就排序时就会减少一个参与排序的对象 Python12345678def bubble_sorted(iterable): new_list = list(iterable) list_len = len(new_list) for i in range(list_len - 1): for j in range(list_len - 1, i, -1): if new_list[j] &lt; new_list[j - 1]: new_list[j], new_list[j - 1] = new_list[j - 1], new_list[j] return new_list Golang12345678910111213141516package mainimport \"fmt\"func main() &#123; var arr = [10]int&#123;2, 4, 6, 8, 9, 7 ,5 ,3 ,1&#125; for i := 0; i&lt;len(arr)-1 ; i++ &#123; for j:=0; j &lt; len(arr)-i-1; j++ &#123; if arr[j] &gt; arr[j+1] &#123; arr[j], arr[j+1] = arr[j+1], arr[j] &#125; &#125; &#125; fmt.Println(arr)&#125;","tags":[{"name":"冒泡排序","slug":"冒泡排序","permalink":"https://zhangslob.github.io/tags/冒泡排序/"}]},{"title":"Leetcode: 2.Add Two Numbers","date":"2019-03-15T02:02:40.000Z","path":"2019/03/15/Leetcode-2-Add-Two-Numbers-1/","text":"这是崔斯特的第八十九篇原创文章 2. Add Two Numbers难度: Medium 刷题内容原题连接：https://leetcode.com/problems/add-two-numbers/description/&gt; 内容描述 123456789You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list.You may assume the two numbers do not contain any leading zero, except the number 0 itself.Example:Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 0 -&gt; 8Explanation: 342 + 465 = 807. 解题方案 Python思路 1 时间复杂度: O(N) 空间复杂度: O(N) 将两个链表转为列表，反转之后相加，再反转，最后将结果转为链表。 12345678910111213141516171819202122232425262728293031# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def addTwoNumbers(self, l1: ListNode, l2: ListNode) -&gt; ListNode: if not l1: return l2 if not l2: return l1 val1, val2 = [l1.val], [l2.val] while l1.next: val1.append(l1.next.val) l1 = l1.next while l2.next: val2.append(l2.next.val) l2 = l2.next num1 = ''.join([str(i) for i in val1[::-1]]) num2 = ''.join([str(i) for i in val2[::-1]]) tmp = str(int(num1) + int(num2))[::-1] res = ListNode(int(tmp[0])) run_res = res for i in range(1, len(tmp)): run_res.next = ListNode(int(tmp[i])) run_res = run_res.next return res result： 123Runtime: 124 ms, faster than 33.23% of Python3 online submissions for Add Two Numbers.Memory Usage: 13.4 MB, less than 5.21% of Python3 online submissions for Add Two Numbers. 这是最简单的解法，但是面试官往往不会满足，他肯定还会问，你还有简单的解法吗？空间复杂度可以再低一些吗？ 思路2 时间复杂度: O(N) 空间复杂度: O(1) 可以使用递归，每次算一位的相加 1234567891011121314151617181920# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def addTwoNumbers(self, l1: ListNode, l2: ListNode) -&gt; ListNode: if not l1 and not l2: return elif not (l1 and l2): return l1 or l2 else: if l1.val + l2.val &lt; 10: l3 = ListNode(l1.val+l2.val) l3.next = self.addTwoNumbers(l1.next, l2.next) else: l3 = ListNode(l1.val+l2.val-10) l3.next = self.addTwoNumbers(l1.next, self.addTwoNumbers(l2.next, ListNode(1))) return l3 Result: 12Runtime: 116 ms, faster than 42.88% of Python3 online submissions for Add Two Numbers.Memory Usage: 13.3 MB, less than 5.21% of Python3 online submissions for Add Two Numbers. Golang这个问题比较简单，基本上解题思路是比较清晰的。输入是两个链表，链表的元素都是单个数字（0-9），要求将两个列表的相应节点数字相加，并作为结果链表返回。 这个题咋看可以马上开始解答，但是在此之前还是有一些需要注意的地方。第一点是，题目并没有说明链表的长度，所以 A 和 B 两个链表可能不一定相同长度，那么如果一个链表更长，那么相加怎么处理呢？这里就考虑直接返回即可，相当于+0。第二点是，如果相加溢出怎么处理，其实题目的例子里面已经很清晰了，溢出会发生进位，依次向后处理。第三点是，如果最后一位发生进位呢，这点容易被遗忘，需要新增一个节点。 123456789101112131415161718192021222324252627282930313233package add_two_numberstype ListNode struct &#123; Val int Next *ListNode&#125;func addTwoNumbers(l1 *ListNode, l2 *ListNode) *ListNode &#123; resPre := &amp;ListNode&#123;&#125; cur := resPre carry := 0 for l1 != nil || l2 != nil || carry &gt; 0 &#123; sum := carry if l1 != nil &#123; sum += l1.Val l1 = l1.Next &#125; if l2 != nil &#123; sum += l2.Val l2 = l2.Next &#125; carry = sum / 10 cur.Next = &amp;ListNode&#123;Val: sum % 10&#125; cur = cur.Next &#125; return resPre.Next&#125; 测试 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package add_two_numbersimport ( \"github.com/stretchr/testify/assert\" \"testing\")type para struct &#123; one *ListNode two *ListNode&#125;type ans struct &#123; one *ListNode&#125;type question struct &#123; p para a ans&#125;func makeListNode(is []int) *ListNode &#123; if len(is) == 0 &#123; return nil &#125; res := &amp;ListNode&#123; Val: is[0], &#125; temp := res for i := 1; i &lt; len(is); i++ &#123; temp.Next = &amp;ListNode&#123;Val: is[i]&#125; temp = temp.Next &#125; return res&#125;func Test_OK(t *testing.T) &#123; ast := assert.New(t) qs := []question&#123; &#123; p: para&#123; one:makeListNode([]int&#123;2, 4, 3&#125;), two:makeListNode([]int&#123;5, 6, 4&#125;), &#125;, a: ans&#123; one:makeListNode([]int&#123; 7, 0, 8&#125;), &#125;, &#125;, &#123; p: para&#123; one:makeListNode([]int&#123;9, 8, 7, 6, 5&#125;), two:makeListNode([]int&#123;1, 1, 2, 3, 4&#125;), &#125;, a:ans&#123; one:makeListNode([]int&#123;0, 0, 0, 0, 0, 1&#125;), &#125;, &#125;, &#123; p: para&#123; one:makeListNode([]int&#123;0&#125;), two:makeListNode([]int&#123;5, 6, 4&#125;), &#125;, a: ans&#123; one:makeListNode([]int&#123;5, 6, 4&#125;), &#125;, &#125;, &#125; for _, q := range qs &#123; a, p := q.a, q.p ast.Equal(a.one, addTwoNumbers(p.one, p.two), \"输入:%v\", p) &#125;&#125; 消耗 递归解法 123456789101112131415161718192021func addTwoNumbers(l1 *ListNode, l2 *ListNode) *ListNode &#123; sum := l1.Val + l2.Val val := sum % 10 if (l1.Next == nil &amp;&amp; l2.Next == nil &amp;&amp; sum &lt; 10) &#123; return &amp;ListNode&#123;sum, nil&#125; &#125; if (l1.Next == nil) &#123; l1.Next = &amp;ListNode&#123;&#125; &#125; if (l2.Next == nil) &#123; l2.Next = &amp;ListNode&#123;&#125; &#125; if (sum &gt;= 10) &#123; l1.Next.Val += 1 &#125; res := addTwoNumbers(l1.Next, l2.Next) return &amp;ListNode&#123;val, res&#125;&#125; 有时候很无语，同样的解法，提交时间不一样，性能差别竟然有这么大，12ms与20ms。(⊙_⊙)?","tags":[{"name":"leetcode","slug":"leetcode","permalink":"https://zhangslob.github.io/tags/leetcode/"},{"name":"刷题","slug":"刷题","permalink":"https://zhangslob.github.io/tags/刷题/"}]},{"title":"Leetcode：1.Two Sum","date":"2019-03-04T09:45:26.000Z","path":"2019/03/04/Leetcode-1-Two-Sum/","text":"这是崔斯特的第八十九篇原创文章 1. Two Sum难度: Easy 刷题内容原题连接：https://leetcode.com/problems/two-sum 内容描述： 12345678910Given an array of integers, return indices of the two numbers such that they add up to a specific target.You may assume that each input would have exactly one solution, and you may not use the same element twice.Example:Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1]. 解题方案Python思路 1- 时间复杂度: O(N^2)- 空间复杂度: O(1)* 暴力解法，两轮遍历 beats 12.80%1234567891011class Solution(object): def twoSum(self, nums, target): \"\"\" :type nums: List[int] :type target: int :rtype: List[int] \"\"\" for i in range(len(nums)): for j in range(i+1, len(nums)): if nums[i] + nums[j] == target: return [i, j] Runtime: 5248 ms, faster than 12.80% of Python3 online submissions forTwo Sum. Memory Usage: 13.5 MB, less than 51.76% of Python3 online submissions for Two Sum. 太吓人，遍历太慢了。 思路 2A + B = target，同样的：A = target - B，我们可以维护一个字典来记录说访问过的值。 这里我犯了个错，我先贴代码 12345678class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: lookup = &#123;&#125; for i, j in enumerate(nums): if target - j not in lookup.values(): lookup[i] = j else: return [i, nums.index(target - i)] Runtime: 568 ms, faster than 34.25% of Python3 online submissions forTwo Sum. Memory Usage: 14.1 MB, less than 9.73% of Python3 online submissions for Two Sum. 虽然比第一种解法好，但是才超过34.25%，这也太慢了吧。 其实是我写法不够好，字典维序反了，这样查找会慢很多，每次查找都需要查询lookup.values()，这其实是一个列表，应该就是这里导致速度变慢。 改良后的代码： 12345678class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: lookup = &#123;&#125; for i, j in enumerate(nums): if target - j in lookup: return [i, nums.index(target - j)] else: lookup[j] = i Runtime: 40 ms, faster than 73.58% of Python3 online submissions for Two Sum. Memory Usage: 14.1 MB, less than 10.10% of Python3 online submissions for Two Sum. 比73.58%的要快，果然细节决定成败啊。 Golang这些天正好在学习Golang的基础语法，做这题应该是没什么问题，思路什么的和上面的一样。代码如下： two_sum.go 12345678910111213package two_sumfunc twoSum(nums []int, target int) []int &#123; index := make(map[int]int, len(nums)) for i, b := range nums &#123; if j, ok := index[target-b]; ok &#123; return []int&#123;j, i&#125; &#125; index[b] = i &#125; return nil&#125; two_sum_test.go 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package two_sumimport ( \"github.com/stretchr/testify/assert\" \"testing\")type para struct &#123; one []int two int&#125;type ans struct &#123; one []int&#125;type question struct &#123; p para a ans&#125;func Test_OK(t *testing.T) &#123; ast := assert.New(t) qs := []question&#123; &#123; p: para&#123; one: []int&#123;3, 2, 4&#125;, two: 6, &#125;, a: ans&#123; one: []int&#123;1, 2&#125;, &#125;, &#125;, &#123; p: para&#123; one: []int&#123;3, 2, 4&#125;, two: 8, &#125;, a: ans&#123; one: nil, &#125;, &#125;, &#125; for _, q := range qs &#123; a, p := q.a, q.p ast.Equal(a.one, twoSum(p.one, p.two), \"输入: %v\", p) &#125;&#125; 作为还没写过单元测试的人来说，刚开始接触这种真的很不适应，慢慢来吧。 Runtime: 4 ms, faster than 100.00% of Go online submissions for Two Sum. Memory Usage: 3.4 MB, less than 63.51% of Go online submissions forTwo Sum. Go和Python的对比，简直了 这个解法额外的创建了一个map来储存状态，所以在空间上不够好，看到别人的解法，如下： 1234567891011func twoSum(nums []int, target int) []int &#123; length := len(nums) for i := 0; i &lt; length; i++ &#123; for j := i + 1; j &lt; length; j++ &#123; if nums[i]+nums[j] == target &#123; return []int&#123;i, j&#125; &#125; &#125; &#125; return nil&#125; 这就相当于上面开始的暴力解法，牺牲时间来换取空间，最后的结果是： Runtime: 36 ms, faster than 41.39% of Go online submissions for Two Sum. Memory Usage: 2.9 MB, less than 68.92% of Go online submissions for Two Sum. 时间是原来的9倍，变化真的大！","tags":[{"name":"leetcode","slug":"leetcode","permalink":"https://zhangslob.github.io/tags/leetcode/"},{"name":"刷题","slug":"刷题","permalink":"https://zhangslob.github.io/tags/刷题/"}]},{"title":"MongoDB参数限制和阀值","date":"2019-02-28T12:46:10.000Z","path":"2019/02/28/MongoDB参数限制和阀值/","text":"1这是崔斯特的第八十八篇原创文章 今天搜索spark mongo的资料，意外发现了MongoDB的一些知识，这些都是之前没有接触过的，所以专门记录下。 (๑• . •๑) 一、BSON文档 BSON文档尺寸：一个document文档最大尺寸为16M；大于16M的文档需要存储在GridFS中。 文档内嵌深度：BSON文档的结构（tree）深度最大为100。 二、Namespaces collection命名空间：.，最大长度为120字节。这也限定了database和collection的名字不能太长。 命名空间的个数：对于MMAPV1引擎，个数最大为大约为24000个，每个collection以及index都是一个namespace；对于wiredTiger引擎则没有这个限制。 namespace文件的大小：对于MMAPV1引擎而言，默认大小为16M，可以通过在配置文件中修改。wiredTiger不受此限制。 三、indexes index key：每条索引的key不得超过1024个字节，如果index key的长度超过此值，将会导致write操作失败。 每个collection中索引的个数不得超过64个。 索引名称：我们可以为index设定名称，最终全名为..$，最长不得超过128个字节。默认情况下为filed名称与index类型的组合，我们可以在创建索引时显式的指定index名字，参见createIndex()方法。 组合索引最多能包含31个field。 四、Data Capped Collection：如果你在创建“Capped”类型的collection时指定了文档的最大个数，那么此个数不能超过2的32次方，如果没有指定最大个数，则没有限制。 Database Size：MMAPV1引擎而言，每个database不得持有超过16000个数据文件，即单个database的总数据量最大为32TB，可以通过设置“smallFiles”来限定到8TB。 Data Size：对于MMAVPV1引擎而言，单个mongod不能管理超过最大虚拟内存地址空间的数据集，比如linux（64位）下每个mongod实例最多可以维护64T数据。wiredTiger引擎没有此限制。 每个Database中collection个数：对于MMAPV1引擎而然，每个database所能持有的collections个数取决于namespace文件大小（用来保存namespace）以及每个collection中indexes的个数，最终总尺寸不超过namespace文件的大小（16M）。wiredTiger引擎不受到此限制。 五、Replica Sets 每个replica set中最多支持50个members。 replica set中最多可以有7个voting members。（投票者） 如果没有显式的指定oplog的尺寸，其最大不会超过50G。 六、Sharded Clusters group聚合函数，在sharding模式下不可用。请使用mapreduce或者aggregate方法。 Coverd Queries：即查询条件中的Fields必须是index的一部分，且返回结果只包含index中的fields；对于sharding集群，如果query中不包含shard key，索引则无法进行覆盖。虽然_id不是“shard key”，但是如果查询条件中只包含_id，且返回的结果中也只需要_id字段值，则可以使用覆盖查询，不过这个查询似乎并没有什么意义（除非是检测此_id的document是否存在）。 对于已经存有数据的collections开启sharding（原来非sharding），则其最大数据不得超过256G。当collection被sharding之后，那么它可以存储任意多的数据。 对于sharded collection，update、remove对单条数据操作（操作选项为multi:false或者justOne），必须指定shard key或者_id字段；否则将会抛出error。 唯一索引：shards之间不支持唯一索引，除非这个“shard key”是唯一索引的最左前缀。比如collection的shard key为{“zipcode”:1,”name”: 1}，如果你想对collection创建唯一索引，那么唯一索引必须将zipcode和name作为索引的最左前缀，比如：collection.createIndex({“zipcode”:1,”name”:1,”company”:1},{unique:true})。 在chunk迁移时允许的最大文档个数：如果一个chunk中documents的个数超过250000（默认chunk大小为64M）时，或者document个数大于 1.3 *（chunk最大尺寸（有配置参数决定）/ document平均尺寸），此chunk将无法被“move”（无论是balancer还是人工干预），必须等待split之后才能被move。 七、shard key shard key的长度不得超过512个字节。 “shard key索引”可以为基于shard key的正序索引，或者以shard key开头的组合索引。shard key索引不能是multikey索引（基于数组的索引）、text索引或者geo索引。 Shard key是不可变的，无论何时都不能修改document中的shard key值。如果需要变更shard key，则需要手动清洗数据，即全量dump原始数据，然后修改并保存在新的collection中。 单调递增（递减）的shard key会限制insert的吞吐量；如果_id是shard key，需要知道_id是ObjectId()生成，它也是自增值。对于单调递增的shard key，collection上的所有insert操作都会在一个shard节点上进行，那么此shard将会承载cluster的全部insert操作，因为单个shard节点的资源有限，因此整个cluster的insert量会因此受限。如果cluster主要是read、update操作，将不会有这方面的限制。为了避免这个问题，可以考虑使用“hashed shard key”或者选择一个非单调递增key作为shard key。（rang shard key 和hashed shard key各有优缺点，需要根据query的情况而定）。 八、Operations 如果mongodb不能使用索引排序来获取documents，那么参与排序的documents尺寸需要小于32M。 Aggregation Pileline操作。Pipeline stages限制在100M内存，如果stage超过此限制将会发生错误，为了能处理较大的数据集，请开启“allowDiskUse”选项，即允许pipeline stages将额外的数据写入临时文件。 九、命名规则 database的命名区分大小写。 database名称中不要包含：/ .‘’$*&lt;&gt;:|? database名称长度不能超过64个字符。 collection名称可以以“_”或者字母字符开头，但是不能包含”$”符号，不能为空字符或者null，不能以“system.”开头，因为这是系统保留字。 document字段名不能包含“.”或者null，且不能以“$”开头，因为$是一个“引用符号”。 最后记录下json嵌套中含有列表的查询方法，样例数据： 1234567891011121314&#123; \"_id\" : ObjectId(\"5c6cc376a589c200018f7312\"), \"id\" : \"9472\", \"data\" : &#123; \"name\" : \"测试\", \"publish_date\" : \"2009-05-15\", \"authors\" : [ &#123; \"author_id\" : 3053, \"author_name\" : \"测试数据\" &#125; ], &#125;&#125; 我要查询authors中的author_id，query可以这样写： 1db.getCollection().find(&#123;'data.authors.0.author_id': 3053&#125;) 用0来代表第一个索引，点代表嵌套结构。但是spark mongo中是不能这样导入的，需要使用别的方法。","tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://zhangslob.github.io/tags/MongoDB/"}]},{"title":"使用pyppeteer淘宝登录","date":"2019-02-17T07:31:06.000Z","path":"2019/02/17/使用pyppeteer淘宝登录/","text":"1这是崔斯特的第八十七篇原创文章 使用pyppeteer淘宝登录，获取Cookies 现在淘宝的商品搜索页必须要登录才能见，所以必须要cookies才能进行下一步操作。本期介绍如何使用pyppeteer登录淘宝，获取Cookies。 pyppeteer介绍地址：https://github.com/miyakogi/pyppeteer 介绍：Unofficial Python port of puppeteer JavaScript (headless) chrome/chromium browser automation library.非官方的chrome浏览器，前身是JavaScript的puppeteer 安装：python3 -m pip install pyppeteer 打开网站并截图 12345678910111213import asynciofrom pyppeteer import launchasync def main(): browser = await launch() page = await browser.newPage() await page.goto('https://zhangslob.github.io/') await page.screenshot(&#123;'path': 'zhangslob.png'&#125;) await browser.close()asyncio.get_event_loop().run_until_complete(main()) 第一次运行时需要下载Chromium，你可以提前在终端输入pyppeteer-install来下载。 登录淘宝整体逻辑很简单： 设置pyppeteer启动项，打开一个浏览器 打开淘宝登录页面 修改浏览器属性 输入账号密码 滑动滑块 登录 代码太长，可以直接去Github上看，记得要加上自己的账号密码。记得要用小号，如果长时间使用一个账号，会出现各种各样的验证。 登录的主代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354async def taobao_login(username, password, url): \"\"\" 淘宝登录主程序 :param username: 用户名 :param password: 密码 :param url: 登录网址 :return: 登录cookies \"\"\" ### await page.click('#J_QRCodeLogin &gt; div.login-links &gt; a.forget-pwd.J_Quick2Static') page.mouse time.sleep(1) # 输入用户名，密码 await page.type('#TPL_username_1', username, &#123;'delay': input_time_random() - 50&#125;) # delay是限制输入的时间 await page.type('#TPL_password_1', password, &#123;'delay': input_time_random()&#125;) time.sleep(2) # 检测页面是否有滑块。原理是检测页面元素。 slider = await page.Jeval('#nocaptcha', 'node =&gt; node.style') # 是否有滑块 if slider: print('当前页面出现滑块') # await page.screenshot(&#123;'path': './headless-login-slide.png'&#125;) # 截图测试 flag, page = await mouse_slide(page=page) # js拉动滑块过去。 if flag: await page.keyboard.press('Enter') # 确保内容输入完毕，少数页面会自动完成按钮点击 print(\"print enter\", flag) await page.evaluate('''document.getElementById(\"J_SubmitStatic\").click()''') # 如果无法通过回车键完成点击，就调用js模拟点击登录按钮。 time.sleep(2) cookies_list = await page.cookies() print(cookies_list) return await get_cookie(page) # 导出cookie 完成登陆后就可以拿着cookie玩各种各样的事情了。 else: print(\"\") await page.keyboard.press('Enter') print(\"print enter\") await page.evaluate('''document.getElementById(\"J_SubmitStatic\").click()''') await page.waitFor(20) await page.waitForNavigation() try: global error # 检测是否是账号密码错误 print(\"error_1:\", error) error = await page.Jeval('.error', 'node =&gt; node.textContent') print(\"error_2:\", error) except Exception as e: error = None finally: if error: print('确保账户安全重新入输入') # 程序退出。 loop.close() else: print(page.url) return await get_cookie(page) 所有代码在AwesomeCrawler/taobao_login 如何建立Cookie池必须多个账号，那么如何注册多个淘宝账号呢。。 可以通过第三方提供手机号验证码服务商，通过pyppeteer注册账号，保存账号信息 登录账号并保存在redis 开线程检查账号是否已过期，若过期重新登录即可","tags":[{"name":"登录","slug":"登录","permalink":"https://zhangslob.github.io/tags/登录/"},{"name":"淘宝","slug":"淘宝","permalink":"https://zhangslob.github.io/tags/淘宝/"}]},{"title":"说点什么","date":"2019-02-13T13:35:33.000Z","path":"2019/02/13/说点什么/","text":"1这是崔斯特的第八十六篇原创文章 聊聊天 (๑• . •๑) 过年这些天，公众号没怎么发文，有很多原因，主要还是因为自己懒。期间也发生了很多有趣的事，今天就来说说。 带女朋友回家。认识她有一年了，今天过年就带回家了，父母也挺满意。家在湖北，别的都好，就是感觉太冷了，在北京习惯了就感觉家里真的太冷了。每天都坐在“炕”旁边，对，南方也会有炕。给她买了两个热水袋，每天换着换着来。感觉最开心的就是终于不用一个人坐火车了，两个人有个伴，会好很多。 由于家里太冷，MacBook根本没法正常使用，一开机就关机，而且还充电不进去。过年期间，基本上是没写代码的，现在更需要好好努力，多学点东西，坚持刷题。 最尴尬的事情是我TM被狗咬了，就在初二。邻居家的狗跑过来，咬了一口，有破皮，直接去医院打了狂犬疫苗，被判定是二级咬伤，这年过的真惨。最痛苦的是不能吃辣的，导致后来几天基本上只能吃点青菜了，现在超级想吃辣的。 被狗咬的那天心情超级差，因为狂犬病死亡率是百分之百，担心自己会不会就这样挂了，因为就算打了疫苗也不能说一定没事。就想着，如果自己还剩下一年时间，要去做什么，家庭、爱情什么的该如何处理，都没有在这个世界上留下来过的痕迹。所以在过年那几天心情真的超级郁闷。而且今年是猪年，我的本命年，我妈说让我去买点红袜子、红内裤穿，解解霉运。 现在身体感觉还好，都有按时打针，保持心态，别自己吓自己。我还没活够呢。 还有个话题就是买房。打算以后回武汉定居了，北京真的不用想，在武汉买100平的房子需要一百多万，首付需要三四十万，对于我来说还需要奋斗几年才行。有句话这样说，买房才能体验一个家庭的硬实力，穷人家的孩子只能靠自己啊。（其实很想去深山老林，过有wifi的生活） 最后我在想自己现在能做什么，想写点什么。好像自己现在比较精通的就只有爬虫了，可是现在爬虫教程泛滥，又不想写这方面的东西，倒是可以写点分布式爬虫的理解。所以还是需要充实自己，这样才不会感觉到空虚。 好了，先说到这里，继续去学习了。你们想看什么，可以留言。","tags":[{"name":"随笔","slug":"随笔","permalink":"https://zhangslob.github.io/tags/随笔/"}]},{"title":"scrapy自定义重试方法","date":"2019-01-25T10:59:29.000Z","path":"2019/01/25/scrapy自定义重试方法/","text":"1这是崔斯特的第八十五篇原创文章 自定义重试方法 (๑• . •๑) Scrapy是自带有重试的，但一般是下载出错才会重试，当然你可以在Middleware处来完成你的逻辑。这篇文章主要介绍的是如何在spider里面完成重试。使用场景比如，我解析json出错了，html中不包含我想要的数据，我要重试这个请求（request）。 我们先看看官方是如何完成重试的 scrapy/downloadermiddlewares/retry.py 1234567891011121314151617181920212223242526272829def _retry(self, request, reason, spider): retries = request.meta.get('retry_times', 0) + 1 retry_times = self.max_retry_times if 'max_retry_times' in request.meta: retry_times = request.meta['max_retry_times'] stats = spider.crawler.stats if retries &lt;= retry_times: logger.debug(\"Retrying %(request)s (failed %(retries)d times): %(reason)s\", &#123;'request': request, 'retries': retries, 'reason': reason&#125;, extra=&#123;'spider': spider&#125;) retryreq = request.copy() retryreq.meta['retry_times'] = retries retryreq.dont_filter = True retryreq.priority = request.priority + self.priority_adjust if isinstance(reason, Exception): reason = global_object_name(reason.__class__) stats.inc_value('retry/count') stats.inc_value('retry/reason_count/%s' % reason) return retryreq else: stats.inc_value('retry/max_reached') logger.debug(\"Gave up retrying %(request)s (failed %(retries)d times): %(reason)s\", &#123;'request': request, 'retries': retries, 'reason': reason&#125;, extra=&#123;'spider': spider&#125;) 可以看到非常清晰，在meta中传递一个参数retry_times，来记录当前的request采集了多少次，如果重试次数小于设置的最大重试次数，那么重试。 根据这段代码我们自定义的重试可以这么写 12345678def parse(self, response): try: data = json.loads(response.text) except json.decoder.JSONDecodeError: r = response.request.copy() r.dont_filter = True yield r 捕获异常，如果返回不是json，那就重试，注意需要设置不过滤。 这种方法简单粗暴，存在BUG，就是会陷入死循环。我也可以记录重试的次数，用meta传递。 123456789101112131415def parse(self, response): try: data = json.loads(response.text) except json.decoder.JSONDecodeError: retries = response.meta.get('cus_retry_times', 0) + 1 if retries &lt;= self.cus_retry_times: r = response.request.copy() r.meta['cus_retry_times'] = retries r.dont_filter = True yield r else: self.logger.debug(\"Gave up retrying &#123;&#125;, failed &#123;&#125; times\".format( response.url, retries )) 这样就完成了自定义重试，你完全可以在中间件完成，但是我更喜欢这种方法，可以清楚地知道爬虫具体哪里会存在问题。 其实以上这种方法也不好，因为你可能会在很多地方都需要重试，每个函数都需要，那每次都写一遍，太不美观。更好的方法是将此方法封装为scrapy.http.Response的一个函数，需要用的时候直接调。代码就不贴了，有兴趣的可以研究下，用到python的继承。","tags":[{"name":"scrapy","slug":"scrapy","permalink":"https://zhangslob.github.io/tags/scrapy/"}]},{"title":"刷题之合并K个排序链表","date":"2019-01-22T12:51:17.000Z","path":"2019/01/22/刷题之合并K个排序链表/","text":"这是崔斯特的第八十四篇原创文章 拼命刷题 (๑• . •๑) 题目：合并 k 个排序链表，返回合并后的排序链表。 示例: 1234567输入:[ 1-&gt;4-&gt;5, 1-&gt;3-&gt;4, 2-&gt;6]输出: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6 思路一从21. 合并两个有序链表的基础上，我们已经能够解决两个有序链表的问题，现在是k个有序链表，我们可以将第一二个有序链表进行合并，然后将新的有序链表再继续跟第三个有序链表合并，直到将所有的有序链表合并完成。 这样做思路上是可行的，但是算法的时间复杂度将会很大，具体就不计算了。有兴趣的自己计算下。 思路二根据思路一，我们是一个一个地将有序链表组成新的链表，这里一个进行了k-1次两个有序链表的合并操作。而且随着新链表越来越大，时间复杂度也会越来越高。 这里有一种简化的方式，可以先将k个有序链表先以2个链表为一组进行合并，得出结果后，再将所有的新有序链表继续上面的方式，2个链表为一组进行合并。直至将所有的有序链表进行合并。 这个思路会比思路一的算法复杂度少一点。 思路三（推荐）我们换个不一样的思路。我们先遍历一次所有的链表中的元素。然后将元素全部放在一个数组里面。接着对这个数组进行排序，最终将排序后的数组里面的所有元素链接起来。 这种方案的复杂度和代码量会比前集中思路更好，更简单。 空间复杂度：因为需要一个数组，所以需要额外的空间。这个空间的大小就是链表元素的个数 时间复杂度：假设一个是n个元素，对链表进行遍历(n),对数组进行排序(排序算法可以达到nlogn)，最终链接所有元素(n),就是 （n+nlogn+n），也就是O(nlogn)。 12345678910111213141516171819202122232425262728# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def mergeKLists(self, lists): \"\"\" :type lists: List[ListNode] :rtype: ListNode \"\"\" nodeList = [] for i in range(len(lists)): currentNode = lists[i] while currentNode: nodeList.append(currentNode) currentNode = currentNode.next nodeList = sorted(nodeList, key=lambda x: x.val) tempHead = ListNode(0) currentNode = tempHead for i in range(len(nodeList)): currentNode.next = nodeList[i] currentNode = currentNode.next return tempHead.next 思路四偷懒方法，使用内置库heapq，最小堆, 每个list有一个指针, k个指针放入堆中, 每次pop出最小的, 然后指向相应list的下一个node, 再push入堆。 最小堆是一个数组, 所有元素满heap[k] &lt;= heap[2*k+1]和heap[k] &lt;= heap[2*k+2], heap[0]即堆顶最小。 1234567891011121314151617181920212223# class ListNode:# def __init__(self, x):# self.val = x# self.next = None class Solution: # @param a list of ListNode # @return a ListNode def mergeKLists(self, lists): heap = [] for node in lists: if node != None: heap.append((node.val, node)) heapq.heapify(heap) head = ListNode(0) curr = head while heap: pop = heapq.heappop(heap) curr.next = ListNode(pop[0]) curr = curr.next if pop[1].next: heapq.heappush(heap, (pop[1].next.val, pop[1].next)) return head.next","tags":[{"name":"刷题","slug":"刷题","permalink":"https://zhangslob.github.io/tags/刷题/"},{"name":"算法","slug":"算法","permalink":"https://zhangslob.github.io/tags/算法/"}]},{"title":"用Golang写爬虫（一）","date":"2019-01-16T07:38:28.000Z","path":"2019/01/16/Golang写爬虫/","text":"这是崔斯特的第八十三篇原创文章 新的旅程开始了 (๑• . •๑) 前言近期有些项目需要用到Golang，大概花了一周来看语法，然后就开始看爬虫相关的。这里记录下如何使用Golang来写爬虫的几个步骤，最终完成的效果如下图 环境安装比较简单 12sudo apt-get install golang # (Linux)brew install go # (Mac) 安装之后注意GOPATH和GOROOT等环境变量设置，IDE用的是jetbrains家的GoLand。 建议先去看看Golang的官方文档，学习基本语法知识。地址：官方教程中文版 创建文档新建文件crawler.go，并写入如下代码： 1234567package mainimport \"fmt\"func main() &#123; fmt.Println(\"Hello, world\")&#125; 运行方法：go run crawler.go，肉眼可见，编译速度比JAVA要快得多。 下载网页这里先从Golang原生http库开始，直接使用net/http包内的函数请求 123import \"net/http\"...resp, err := http.Get(\"http://wwww.baidu.com\") 所以代码可以这样写 12345678910111213141516171819202122package mainimport ( \"fmt\" \"io/ioutil\" \"net/http\")func main() &#123; fmt.Println(\"Hello, world\") resp, err := http.Get(\"http://www.baidu.com/\") if err != nil &#123; fmt.Println(\"http get error\", err) return &#125; body, err := ioutil.ReadAll(resp.Body) if err != nil &#123; fmt.Println(\"read error\", err) return &#125; fmt.Println(string(body))&#125; Golang的错误处理就是这样的，习惯就好。 这里更好的做法是把下载方法封装为函数。 1234567891011121314151617181920212223242526272829303132333435package mainimport ( \"fmt\" \"io/ioutil\" \"net/http\")func main() &#123; fmt.Println(\"Hello, world\") url := \"http://www.baidu.com/\" download(url)&#125;func download(url string) &#123; client := &amp;http.Client&#123;&#125; req, _ := http.NewRequest(\"GET\", url, nil) // 自定义Header req.Header.Set(\"User-Agent\", \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\") resp, err := client.Do(req) if err != nil &#123; fmt.Println(\"http get error\", err) return &#125; //函数结束后关闭相关链接 defer resp.Body.Close() body, err := ioutil.ReadAll(resp.Body) if err != nil &#123; fmt.Println(\"read error\", err) return &#125; fmt.Println(string(body))&#125; 解析网页go常见的解析器xpath、jquery、正则都有，直接搜索即可，我这里偷懒，直接用别人写好的轮子collectlinks，可以提取网页中所有的链接，下载方法go get -u github.com/jackdanger/collectlinks 123456789101112131415161718192021222324252627282930313233package mainimport ( \"fmt\" \"github.com/jackdanger/collectlinks\" \"net/http\")func main() &#123; fmt.Println(\"Hello, world\") url := \"http://www.baidu.com/\" download(url)&#125;func download(url string) &#123; client := &amp;http.Client&#123;&#125; req, _ := http.NewRequest(\"GET\", url, nil) // 自定义Header req.Header.Set(\"User-Agent\", \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\") resp, err := client.Do(req) if err != nil &#123; fmt.Println(\"http get error\", err) return &#125; //函数结束后关闭相关链接 defer resp.Body.Close() links := collectlinks.All(resp.Body) for _, link := range links &#123; fmt.Println(\"parse url\", link) &#125;&#125; 并发Golang使用关键字go即可开启一个新的go程，也叫goroutine，使用 go 语句开启一个新的 goroutine 之后，go 语句之后的函数调用将在新的 goroutine 中执行，而不会阻塞当前的程序执行。所以使用Golang可以很容易写成异步IO。 12345678910111213141516171819202122232425262728293031323334353637383940414243package mainimport ( \"fmt\" \"github.com/jackdanger/collectlinks\" \"net/http\")func main() &#123; fmt.Println(\"Hello, world\") url := \"http://www.baidu.com/\" queue := make(chan string) go func() &#123; queue &lt;- url &#125;() for uri := range queue &#123; download(uri, queue) &#125;&#125;func download(url string, queue chan string) &#123; client := &amp;http.Client&#123;&#125; req, _ := http.NewRequest(\"GET\", url, nil) // 自定义Header req.Header.Set(\"User-Agent\", \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\") resp, err := client.Do(req) if err != nil &#123; fmt.Println(\"http get error\", err) return &#125; //函数结束后关闭相关链接 defer resp.Body.Close() links := collectlinks.All(resp.Body) for _, link := range links &#123; fmt.Println(\"parse url\", link) go func() &#123; queue &lt;- link &#125;() &#125;&#125; 现在的流程是main有一个for循环读取来自名为queue的通道，download下载网页和链接解析，将发现的链接放入main使用的同一队列中，并再开启一个新的goroutine去抓取形成无限循环。 这里对于新手来说真的不好理解，涉及到Golang的两个比较重要的东西：goroutine和channels，这个我也不大懂，这里也不多讲了，以后有机会细说。 官方：A goroutine is a lightweight thread managed by the Go runtime.翻译过来就是：Goroutine是由Go运行时管理的轻量级线程。channels是连接并发goroutine的管道，可以理解为goroutine通信的管道。 可以将值从一个goroutine发送到通道，并将这些值接收到另一个goroutine中。对这部分有兴趣的可以去看文档。 好了，到这里爬虫基本上已经完成了，但是还有两个问题：去重、链接是否有效。 链接转为绝对路径1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package mainimport ( \"fmt\" \"github.com/jackdanger/collectlinks\" \"net/http\" \"net/url\")func main() &#123; fmt.Println(\"Hello, world\") url := \"http://www.baidu.com/\" queue := make(chan string) go func() &#123; queue &lt;- url &#125;() for uri := range queue &#123; download(uri, queue) &#125;&#125;func download(url string, queue chan string) &#123; client := &amp;http.Client&#123;&#125; req, _ := http.NewRequest(\"GET\", url, nil) // 自定义Header req.Header.Set(\"User-Agent\", \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\") resp, err := client.Do(req) if err != nil &#123; fmt.Println(\"http get error\", err) return &#125; //函数结束后关闭相关链接 defer resp.Body.Close() links := collectlinks.All(resp.Body) for _, link := range links &#123; absolute := urlJoin(link, url) if url != \" \" &#123; fmt.Println(\"parse url\", absolute) go func() &#123; queue &lt;- absolute &#125;() &#125; &#125;&#125;func urlJoin(href, base string) string &#123; uri, err := url.Parse(href) if err != nil &#123; return \" \" &#125; baseUrl, err := url.Parse(base) if err != nil &#123; return \" \" &#125; return baseUrl.ResolveReference(uri).String()&#125; 这里新写了一个urlJoin函数，功能和Python中的urllib.parse.urljoin一样。 去重我们维护一个map用来记录，那些是已经访问过的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package mainimport ( \"fmt\" \"github.com/jackdanger/collectlinks\" \"net/http\" \"net/url\")var visited = make(map[string]bool)func main() &#123; fmt.Println(\"Hello, world\") url := \"http://www.baidu.com/\" queue := make(chan string) go func() &#123; queue &lt;- url &#125;() for uri := range queue &#123; download(uri, queue) &#125;&#125;func download(url string, queue chan string) &#123; visited[url] = true client := &amp;http.Client&#123;&#125; req, _ := http.NewRequest(\"GET\", url, nil) // 自定义Header req.Header.Set(\"User-Agent\", \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\") resp, err := client.Do(req) if err != nil &#123; fmt.Println(\"http get error\", err) return &#125; //函数结束后关闭相关链接 defer resp.Body.Close() links := collectlinks.All(resp.Body) for _, link := range links &#123; absolute := urlJoin(link, url) if url != \" \" &#123; if !visited[absolute] &#123; fmt.Println(\"parse url\", absolute) go func() &#123; queue &lt;- absolute &#125;() &#125; &#125; &#125;&#125;func urlJoin(href, base string) string &#123; uri, err := url.Parse(href) if err != nil &#123; return \" \" &#125; baseUrl, err := url.Parse(base) if err != nil &#123; return \" \" &#125; return baseUrl.ResolveReference(uri).String()&#125; 好了大功告成，运行程序，会像一张网铺开一直不停的抓下去。 写到这里，我突然觉得我忘了什么，哦，忘记加timeout了，必须要为每次请求加上超时，前两天才写了的。完整代码就补贴上来了，在github中。 运行一段时间后的资源消耗 CPU使用率并不高，内存因为会保存一张不断增大的map，所以会一直上涨。如果是用Python，该怎么写呢？资源消耗和Golang比会如何呢？有兴趣的小伙伴可以去试试。 后记都说Golang的并发好，体验了下确实如此。Golang起步晚，但是发展的块。采集还是多学点技能防身吧。我从上周开始学习Golang语法，跟着官方文档学习，基本上都可以看懂在做什么，除了那几块难理解的，需要自己多写多用才行。 有兴趣的小伙伴一起入坑啊。","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"Golang","slug":"Golang","permalink":"https://zhangslob.github.io/tags/Golang/"}]},{"title":"Python重试的多重方法","date":"2019-01-14T10:56:03.000Z","path":"2019/01/14/Python重试的多重方法/","text":"1这是崔斯特的第八十二篇原创文章 没人能保证自己的的程序没BUG，所以重试非常有必要。 下面说下我知道的几种Python重试方法。 装饰器这是最最简单的重试方法，而且有现成的轮子，推荐两个： retrying tenacity 两种用法比较类似，我经常用后者，看下 1234567891011import randomfrom tenacity import retry@retrydef do_something_unreliable(): if random.randint(0, 10) &gt; 1: raise IOError(\"Broken sauce, everything is hosed!!!111one\") else: return \"Awesome sauce!\"print(do_something_unreliable()) 用法非常简单，直接加上装饰器。当然也可以有各种自定义。 123456from tenacity import *@retry(stop=(stop_after_delay(10) | stop_after_attempt(5)), wait=wait_fixed(2))def stop_after_10_s_or_5_retries(): print(\"Stopping after 10 seconds or 5 retries\") raise Exception 以上是重试5次，每次间隔10秒，重试前等待2秒。 捕获异常这种方法更常见 12345678910def func(): passfor _ in range(0,100): while True: try: func() except SomeSpecificException: continue break 这里一定不要写成except或者except Exception，一定要指定异常，让别的错误打印出来，然后看日志再修改爬虫，或者会出现意想不到的情况。 举一个例： 12345678910111213141516171819202122def verify_url(url): import requests try: requests.get(url, timeout=10) return True except requests.exceptions.ConnectTimeout: return Falsedef main(): for _ in range(5): try: if verify_url(''): return else: continue except KeyError: continueif __name__ == '__main__': main()","tags":[{"name":"python","slug":"python","permalink":"https://zhangslob.github.io/tags/python/"}]},{"title":"pyspark操作MongoDB","date":"2019-01-03T10:31:23.000Z","path":"2019/01/03/pyspark操作MongoDB/","text":"pyspark对mongo数据库的基本操作 (๑• . •๑) 1这是崔斯特的第八十一篇原创文章 有几点需要注意的： 不要安装最新的pyspark版本，请安装pip3 install pyspark==2.3.2 spark-connector与平常的MongoDB写法不同，格式是：mongodb://127.0.0.1:database.collection 如果计算数据量比较大，你的电脑可能会比较卡，^_^ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#!/usr/bin/env python # -*- coding: utf-8 -*-\"\"\"@author: zhangslob@file: spark_count.py @time: 2019/01/03@desc: 不要安装最新的pyspark版本 `pip3 install pyspark==2.3.2` 更多pyspark操作MongoDB请看https://docs.mongodb.com/spark-connector/master/python-api/\"\"\"import osfrom pyspark.sql import SparkSession# set PYSPARK_PYTHON to python36os.environ['PYSPARK_PYTHON'] = '/usr/bin/python36'# load mongodb data# 格式是：\"mongodb://127.0.0.1:database.collection\"input_uri = \"mongodb://127.0.0.1:27017/spark.spark_test\"output_uri = \"mongodb://127.0.0.1:27017/spark.spark_test\"# 创建spark，默认使用本地环境，或者\"spark://master:7077\"spark = SparkSession \\ .builder \\ .master(\"local\") \\ .appName(\"MyApp\") \\ .config(\"spark.mongodb.input.uri\", input_uri) \\ .config(\"spark.mongodb.output.uri\", output_uri) \\ .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.2.0') \\ .getOrCreate()def except_id(collection_1, collection_2, output_collection, pipeline): \"\"\" 计算表1与表2中不同的数据 :param collection_1: 导入表1 :param collection_2: 导入表2 :param output_collection: 保存的表 :param pipeline: MongoDB查询语句 str :return: \"\"\" # 可以在这里指定想要导入的数据库，将会覆盖上面配置中的input_uri。下面保存数据也一样 # .option(\"collection\", \"mongodb://127.0.0.1:27017/spark.spark_test\") # .option(\"database\", \"people\").option(\"collection\", \"contacts\") df_1 = spark.read.format('com.mongodb.spark.sql.DefaultSource').option(\"collection\", collection_1) \\ .option(\"pipeline\", pipeline).load() df_2 = spark.read.format('com.mongodb.spark.sql.DefaultSource').option(\"collection\", collection_2) \\ .option(\"pipeline\", pipeline).load() # df_1有但是不在 df_2，同理可以计算df_2有，df_1没有 df = df_1.subtract(df_2) df.show() # mode 参数可选范围 # * `append`: Append contents of this :class:`DataFrame` to existing data. # * `overwrite`: Overwrite existing data. # * `error` or `errorifexists`: Throw an exception if data already exists. # * `ignore`: Silently ignore this operation if data already exists. df.write.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"collection\", output_collection).mode(\"append\").save() spark.stop()if __name__ == '__main__': # mongodb query, MongoDB查询语句，可以减少导入数据量 pipeline = \"[&#123;'$project': &#123;'uid': 1, '_id': 0&#125;&#125;]\" collection_1 = \"spark_1\" collection_2 = \"spark_2\" output_collection = 'diff_uid' except_id(collection_1, collection_2, output_collection, pipeline) print('success') 完整代码地址：spark_count_diff_uid.py","tags":[{"name":"-pyspark","slug":"pyspark","permalink":"https://zhangslob.github.io/tags/pyspark/"}]},{"title":"Chrome断点JS寻找淘宝签名sign","date":"2018-12-21T10:45:00.000Z","path":"2018/12/21/Chrome断点JS寻找淘宝签名sign/","text":"这是崔斯特的第八十篇原创文章 Chrome断点JS寻找淘宝签名sign (๑• . •๑) 写了这篇文章淘宝sign加密算法之后，很多人问我Chrome断点调试怎么做，今天会尽量详细聊聊。如果你用使用过Pycharm的断点，会更好理解。 我们还是以淘宝为例，使用Chrome的移动请求头打开这个网站，https://s.m.taobao.com/h5?q=%E9%9E%8B%E6%9E%B6，然后打开开发者工具。 可以看到数据都在这里: 1curl 'https://acs.m.taobao.com/h5/mtop.taobao.wsearch.h5search/1.0/?jsv=2.3.16&amp;appKey=12574478&amp;t=1545389573843&amp;sign=b7d69692d1cc16adec65502f0bac2018&amp;api=mtop.taobao.wsearch.h5search&amp;v=1.0&amp;H5Request=true&amp;ecode=1&amp;type=jsonp&amp;dataType=jsonp&amp;callback=mtopjsonp1&amp;data=%7B%22q%22%3A%22%E9%9E%8B%E6%9E%B6%22%2C%22sst%22%3A%221%22%2C%22n%22%3A20%2C%22buying%22%3A%22buyitnow%22%2C%22m%22%3A%22api4h5%22%2C%22token4h5%22%3A%22%22%2C%22abtest%22%3A%2214%22%2C%22wlsort%22%3A%2214%22%2C%22page%22%3A1%7D' -H 'accept-encoding: gzip, deflate, br' -H 'accept-language: zh-CN,zh;q=0.9' -H 'user-agent: Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1' -H 'accept: */*' -H 'referer: https://s.m.taobao.com/h5?q=%E9%9E%8B%E6%9E%B6' -H 'authority: acs.m.taobao.com' -H 'cookie: _m_h5_tk=2d819f0b9e7ed54e22c296a4d0a2ad24_1545397060981; _m_h5_tk_enc=1337c23b9d98086e735af8c00bdeeea1; t=6b9cfdffa68f4d6ea189913d5288bde7; cna=vbOjFJRUeAYCAd3aK4xZQfic; l=aBVCFdFpyHMPonDmBMaOIX41pVt4SOZzuUwM1MazniYktO-39Z2vPjYb-VwWK_qC5B7y_jt5F; isg=BC8v8gXjr7rLXauG9FFT5HkjvkUz3onQo3vs1UG8yx6lkE-SSaQTRi1BFoau6Ftu' --compressed 重要参数是这些，简单看看，很多都是固定值，可能会变的有时间戳、data中的搜索关键词、页码等信息。重要的就是sign是怎么来的。 下面开始一步步介绍。 首先搜索sign数值，看看在其他地方时候是否出现过 接着搜索方法名。选择一个冷门的变量名可能会有帮助，如下图，一个个地点进去看看，搜索sign，看是否有匹配。 最终确定关键代码在第二个，也就是https://g.alicdn.com/mtb/lib-mtop/2.3.16/mtop.js 格式化代码，并搜索sign，确定代码位置。 直接贴出来 1234567891011121314151617 if (d.H5Request === !0) &#123; var f = \"//\" + (d.prefix ? d.prefix + \".\" : \"\") + (d.subDomain ? d.subDomain + \".\" : \"\") + d.mainDomain + \"/h5/\" + c.api.toLowerCase() + \"/\" + c.v.toLowerCase() + \"/\" , g = c.appKey || (\"waptest\" === d.subDomain ? \"4272\" : \"12574478\") , i = (new Date).getTime() , j = h(d.token + \"&amp;\" + i + \"&amp;\" + g + \"&amp;\" + c.data) , k = &#123; jsv: x, appKey: g, t: i, sign: j &#125; , l = &#123; data: c.data, ua: c.ua &#125;;/// &#125; 可以看到sign等于j，j等于h(d.token + &quot;&amp;&quot; + i + &quot;&amp;&quot; + g + &quot;&amp;&quot; + c.data)，感觉是一些字符串通过“&amp;”连接起来，下一步就可以对js代码打断点去看看具体数据是什么。如下图位置。 此时我们再次刷新网页，稍等片刻。会出现各个变量的信息 我们把鼠标放到变量的位置就可以看到相关信息。我们关注的是如下参数： d.token i g c.data 获取d.token我们一一来看这些是什么。我这里的token是c2b90ff4dd1405aa02fda3b5a2ee72c0，先去搜索下，发现在请求网页时返回的。可是这里是Request Cookies。 我们清空缓存，再来。发现第一次token是空的 继续往下走，直到在Network中看到请求发出去了 直接复制Curl在命令行，结果是： 1mtopjsonp1(&#123;\"api\":\"mtop.taobao.wsearch.h5search\",\"data\":&#123;&#125;,\"ret\":[\"FAIL_SYS_TOKEN_EMPTY::令牌为空\"],\"v\":\"1.0\"&#125;) 我们接着往下走，发现终于有返回的Cookies了，而且对比发现下个请求使用的就是这里的Cookies，token就分号处理了。 首先无cookie状态下去访问该请求：https://acs.m.taobao.com/h5/mtop.taobao.wsearch.h5search/1.0，对方会返回关键Cookies信息，_m_h5_tk和_m_h5_tk_enc，接下来会用到。 下一步接下来的就比较简单了，i很明显就是时间戳，g是定值：12574478，那么data是什么呢，我们可以直接打印到console 发现就是Query String 里的data 好了我们现在已经可以找到所有的参数了，如下 12&gt; d.token + \"&amp;\" + i + \"&amp;\" + g + \"&amp;\" + c.data&lt; \"59e8d998ee8b36a04b6e9d8037ac22b7&amp;1545409677267&amp;12574478&amp;&#123;\"q\":\"鞋架\",\"sst\":\"1\",\"n\":20,\"buying\":\"buyitnow\",\"m\":\"api4h5\",\"token4h5\":\"\",\"abtest\":\"15\",\"wlsort\":\"15\",\"page\":1&#125;\" 最后sign是11a08dbbaa04aa6f9e4b35f6d9213242，看起来像。复制字符串到在线网站里测试，发现结果是正确的。 总结sign生成公式是： 1md5Hex(token&amp;t&amp;appKey&amp;data) 公式可能会发生变化，只要掌握了断点调试的方法，分析起来会简单很多。想学更多搜索Chrome调试技巧。 本文还有些地方没有说清楚，比如如何清空缓存，最佳方法是Application中的Storage，将下面的所有Value清空。","tags":[{"name":"Chrome","slug":"Chrome","permalink":"https://zhangslob.github.io/tags/Chrome/"},{"name":"签名","slug":"签名","permalink":"https://zhangslob.github.io/tags/签名/"}]},{"title":"MongoDB保存数据的优化方法","date":"2018-12-18T10:53:31.000Z","path":"2018/12/18/MongoDB保存数据的优化方法/","text":"这是崔斯特的第七十九篇原创文章 MongoDB保存数据的优化方法 (๑• . •๑) 这两天频繁遇到MongoDB插入数据的问题，这里记录下。 问题描述：我有多个线程在抓数据，每天数据里有含有多个文档（Document），使用Pymongo的插入方法，逐条插入。形如下 123def save_to_mongo(data): for i in data: db.insert_one(i) 在接收到数据后直接调用该方法即可。但是运维那边反馈，数据库压力比较大，让我修改。仔细想了想，可以使用insert_many方法。 插入可迭代的文档 1234567&gt;&gt;&gt; db.test.count_documents(&#123;&#125;)0&gt;&gt;&gt; result = db.test.insert_many([&#123;'x': i&#125; for i in range(2)])&gt;&gt;&gt; result.inserted_ids[ObjectId('54f113fffba522406c9cc20e'), ObjectId('54f113fffba522406c9cc20f')]&gt;&gt;&gt; db.test.count_documents(&#123;&#125;)2 有几个参数需要了解 documents: 可迭代文档 ordered :（可选）如果“True”（默认）文档将按顺序插入服务器，按提供的顺序。 如果发生错误，则中止所有剩余插入。 如果为“False”，文档将以任意顺序插入服务器，可能并行，并且将尝试所有文档插入。 bypass_document_validation: （可选）如果为“True”，则允许写入选择退出文档级别验证。 默认为“False”。 session (optional): a ClientSession. 好了最简单的方法就是把所有需要保存的数据暂时存放在列表中，最后再插入。建议加上ordered=False参数，可以防止数据保存异常。 12345678910111213141516171819202122def save_mongo(): while True: while len(tmp) &gt; 100: try: c = db[collection_name] c.insert_many(tmp, ordered=False) tmp.clear() except pymongo.errors.BulkWriteError: tmp.clear() except Exception as e: logging.error('mongodb_save insert_many: &#123;&#125;, &#123;&#125;'.format(e, tmp)) time.sleep(3)tmp = []for i in data: tmp.append(i)t_save = threading.Thread(target=save_mongo)t_save.setDaemon(True)t_save.start() 新开一个线程去不停的检查，如果列表数据大于100，则批量插入，或者等待3秒。 这里捕获pymongo.errors.BulkWriteError异常，如果在insert_many时发生错误，会产生该异常。在我这里通常是插入重复数据引起的。 还有一种情况，是在多线程情况下。多个线程共享一个列表对象，肯定是需要加锁的，如果使用Lock来管理数据插入问题，需要去给列表加锁。之前还没用过锁，去看看教程。 12345678910111213141516171819202122232425import threadingclass SharedCounter: ''' A counter object that can be shared by multiple threads. ''' def __init__(self, initial_value = 0): self._value = initial_value self._value_lock = threading.Lock() def incr(self,delta=1): ''' Increment the counter with locking ''' self._value_lock.acquire() self._value += delta self._value_lock.release() def decr(self,delta=1): ''' Decrement the counter with locking ''' self._value_lock.acquire() self._value -= delta self._value_lock.release() 觉得太麻烦，可以将保存数据等方法封装成一个类对象，实例化一个列表，在每个线程中实例化一个类对象即可，这样多个线程中是不会共享列表数据的。 当然也可以使用另外一种数据结构：Queue队列。Queue是线程安全的，自带锁，使用的时候，不用对队列加锁操作。可以将数据暂时存入queue，然后用列表取出来，数量大于100则插入，并清空列表。","tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://zhangslob.github.io/tags/MongoDB/"}]},{"title":"Python堆排序之heapq","date":"2018-12-05T14:37:54.000Z","path":"2018/12/05/Python堆排序之heapq/","text":"堆排序 (๑• . •๑) 1这是崔斯特的第七十八篇原创文章 Python中的堆排序heapq模块实现了Python中的堆排序，并提供了有关方法。让用Python实现排序算法有了简单快捷的方式。 heapq的官方文档和源码：Heap queue algorithm 下面通过举例的方式说明heapq的应用方法 实现堆排序123456789101112from heapq import *def heap_sort(iterable): h = [] for value in iterable: heappush(h, value) return [heappop(h) for _ in range(len(h))]if __name__ == '__main__': print(heap_sort([1, 3, 5, 9, 2, 123, 4, 88])) Output: [1, 2, 3, 4, 5, 9, 88, 123] 下面说说几个主要方法 heappush()heapq.heappush(heap, item):将item压入到堆数组heap中。如果不进行此步操作，后面的heappop()失效 heappop()heapq.heappop(heap):从堆数组heap中取出最小的值，并返回。 123456789101112131415161718192021222324&gt;&gt;&gt; h = [] #定义一个list&gt;&gt;&gt; from heapq import * #引入heapq模块&gt;&gt;&gt; h[]&gt;&gt;&gt; heappush(h, 5) #向堆中依次增加数值&gt;&gt;&gt; heappush(h, 2)&gt;&gt;&gt; heappush(h, 3)&gt;&gt;&gt; heappush(h, 9)&gt;&gt;&gt; h #h的值[2, 5, 3, 9]&gt;&gt;&gt; heappop(h) #从h中删除最小的，并返回该值2&gt;&gt;&gt; h[3, 5, 9]&gt;&gt;&gt; h.append(1) #注意，如果不是压入堆中，而是通过append追加一个数值&gt;&gt;&gt; h #堆的函数并不能操作这个增加的数值，或者说它堆对来讲是不存在的[3, 5, 9, 1]&gt;&gt;&gt; heappop(h) #从h中能够找到的最小值是3,而不是13&gt;&gt;&gt; heappush(h, 2) #这时，不仅将2压入到堆内，而且1也进入了堆。&gt;&gt;&gt; h[1, 2, 9, 5]&gt;&gt;&gt; heappop(h) #操作对象已经包含了11 heapq.heappushpop(heap, item)是上述heappush和heappop的合体，同时完成两者的功能.注意：相当于先操作了heappush(heap,item),然后操作heappop(heap) 12345678&gt;&gt;&gt; h[1, 2, 9, 5]&gt;&gt;&gt; heappop(h)1&gt;&gt;&gt; heappushpop(h, 4) #增加4同时删除最小值2并返回该最小值，与下列操作等同：2 #heappush(h,4),heappop(h)&gt;&gt;&gt; h[4, 5, 9] heapq.heapify(x)x必须是list，此函数将list变成堆，实时操作。从而能够在任何情况下使用堆的函数。 12345678910&gt;&gt;&gt; a = [3, 6, 1]&gt;&gt;&gt; heapify(a) #将a变成堆之后，可以对其操作&gt;&gt;&gt; heappop(a)1&gt;&gt;&gt; b = [4, 2, 5] #b不是堆，如果对其进行操作，显示结果如下&gt;&gt;&gt; heappop(b) #按照顺序，删除第一个数值并返回,不会从中挑选出最小的4&gt;&gt;&gt; heapify(b) #变成堆之后，再操作&gt;&gt;&gt; heappop(b)2 heapq.heapreplace(heap, item)是heappop(heap)和heappush(heap,item)的联合操作。注意，与heappushpop(heap,item)的区别在于，顺序不同，这里是先进行删除，后压入堆 12345678910111213141516171819202122232425&gt;&gt;&gt; a = []&gt;&gt;&gt; heapreplace(a, 3) #如果list空，则报错Traceback (most recent call last):File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;IndexError: index out of range&gt;&gt;&gt; heappush(a, 3)&gt;&gt;&gt; a[3]&gt;&gt;&gt; heapreplace(a, 2) #先执行删除（heappop(a)-&gt;3),再执行加入（heappush(a, 2))3&gt;&gt;&gt; a[2]&gt;&gt;&gt; heappush(a, 5) &gt;&gt;&gt; heappush(a, 9)&gt;&gt;&gt; heappush(a, 4)&gt;&gt;&gt; a[2, 4, 9, 5]&gt;&gt;&gt; heapreplace(a, 6) #先从堆a中找出最小值并返回，然后加入62&gt;&gt;&gt; a[4, 5, 9, 6]&gt;&gt;&gt; heapreplace(a, 1) #1是后来加入的，在1加入之前，a中的最小值是44&gt;&gt;&gt; a[1, 5, 9, 6] heapq.merge(*iterables)举例： &gt;&gt;&gt; a = [2, 4, 6] &gt;&gt;&gt; b = [1, 3, 5] &gt;&gt;&gt; c = merge(a, b) &gt;&gt;&gt; list(c) [1, 2, 3, 4, 5, 6] 在归并排序中详细演示了本函数的使用方法。 heapq.nlargest(n, iterable[, key]),heapq.nsmallest(n, iterable[, key])获取列表中最大、最小的几个值。 &gt;&gt;&gt; a [2, 4, 6] &gt;&gt;&gt; nlargest(2,a) [6, 4] 数组中的第K个最大元素其实以上说了那么多，只是为了说这道题。 在未排序的数组中找到第 k 个最大的元素。请注意，你需要找的是数组排序后的第 k 个最大的元素，而不是第 k 个不同的元素。 示例 1: 12输入: [3,2,1,5,6,4] 和 k = 2输出: 5 示例 2: 12输入: [3,2,3,1,2,4,5,5,6] 和 k = 4输出: 4 说明: 你可以假设 k 总是有效的，且 1 ≤ k ≤ 数组的长度。 这里不说别的解法。当然面试中你肯定不能这么写，但这是一个很好的思路 12345678910class Solution: def findKthLargest(self, nums, k): \"\"\" :type nums: List[int] :type k: int :rtype: int \"\"\" import heapq heapq.heapify(nums) return heapq.nlargest(k, nums)[-1] 看到有人用return sorted(nums)[-k]，真的要被气死了。 参考 https://github.com/qiwsir/algorithm/blob/master/heapq.md","tags":[{"name":"heapq","slug":"heapq","permalink":"https://zhangslob.github.io/tags/heapq/"}]},{"title":"聊一聊足球","date":"2018-12-04T10:12:13.000Z","path":"2018/12/04/聊一聊足球/","text":"1这是崔斯特的第七十七篇原创文章 喜爱足球 (๑• . •๑) 以后除了技术还会聊点别的，这次就来聊聊我的爱好之一——足球。这里主要是说看球，看球以前主要看西甲，现在主要看意甲，C罗铁粉。 我是从高中开始喜欢足球，那时候选的文科嘛，然后男生又少，全班11人，大家都去踢球，就慢慢爱上了这个运动。大学时参加了院队，可惜自己技术太水，始终在打酱油，大二就退队了，以后就只看看球。 这里介绍下梅西和C罗，当今最顶尖的两位球员。如果你要问谁更厉害，我觉得这个问题就和PHP和Java谁是世界上最好的语言一样。我喜欢C罗，同时也敬佩梅西，两人各有伟大之处，创造无数记录，各夺得了5次金球奖。 说到金球奖，就不得不说今天的金球奖得主——莫德里奇了。其实我觉得既然都已经是事实了，再去说别人怎么怎么水，没意思。对于梅罗来说，这可能是最好的结局，双方各获奖5次，打了个平手，挺好的。我最受不了的是某些球迷，天天骂来骂去，只是为了争论自己心中谁才是历史最佳，这种行为真的很无聊，有这个时间还不如多写写代码。 看篮球与看足球有很多不同，因为篮球进球太容易，而足球打90分钟0：0是很正常的，半夜看球有时候会很困，不过足球仍然是世界第一运动。世界杯上有一场比赛超级经典，是\u0005葡萄牙\u00063-3\u0005西班牙那场，来看看https://v.qq.com/x/page/p0703fedijp.html 喜欢C罗不仅是因为进球多，也因为他的职业态度。他的职业态度是行业内任何人都不会怀疑的，求胜的欲望，好胜的心理，使得C罗一直都兢兢业业的训练着，即使是休赛期间，C罗也会仍然坚持每天锻炼。 各个时期的C罗肌肉 最后，说说如何看球。现在基本上都可以在pp体育上看，但是有些是会员场。有时候CCTV5也会有转播，上周末就转播意甲尤文3-0弗洛伦萨那场。还有龙珠直播的体育频道，经常会有小主播转播比赛，然后不停地叫你加微信。。 最后想说，趁着年轻能熬夜多看看，以后估计就没精神了。","tags":[{"name":"足球","slug":"足球","permalink":"https://zhangslob.github.io/tags/足球/"}]},{"title":"说一道排序题","date":"2018-11-27T13:47:25.000Z","path":"2018/11/27/说一道排序题/","text":"1这是崔斯特的第七十六篇原创文章 很经典的排序问题 (๑• . •๑) 先看题目，“前K个高频元素” 给定一个非空的整数数组，返回其中出现频率前 k 高的元素。 示例 1: 12输入: nums = [1,1,1,2,2,3], k = 2输出: [1,2] 示例 2: 12输入: nums = [1], k = 1输出: [1] 简单解法这题很简单，两步： 用字典保存数字及其出现的对应频率 排序 那么第一步就不用说了，很简单 123456m = dict()for num in nums: if num in m: m[num] += 1 else: m[num] = 1 有趣的就在第二步，排序。很多人都是使用内置库sorted 1234567891011121314151617181920class Solution(object): def topKFrequent(self, nums, k): \"\"\" :type nums: List[int] :type k: int :rtype: List[int] \"\"\" m = dict() for num in nums: if num in m: m[num] += 1 else: m[num] = 1 output = sorted(m.items(), key=lambda e: e[1], reverse=True) final = [] for i in range(k): final.append(output[i][0]) return final 最简洁的代码是直接使用Python内置的collections 12345678910class Solution(object): def topKFrequent(self, nums, k): \"\"\" :type nums: List[int] :type k: int :rtype: List[int] \"\"\" import collections counter = collections.Counter(nums) return [item[0] for item in counter.most_common(k)] 这真是是我们想要的吗？并不是。我们的目标是不使用任何内置库。 sorted原理关于Python的sorted排序算法，这篇文章讲的比较详细：python sort函数内部实现原理，说到Python使用的是著名的Timesort算法。 Timsort是结合了合并排序（merge sort）和插入排序（insertion sort）而得出的排序算法，它在现实中有很好的效率。 Tim Peters在2002年设计了该算法并在Python中使用（TimSort 是 Python 中 list.sort 的默认实现）。该算法找到数据中已经排好序的块-分区，每一个分区叫一个run，然后按规则合并这些run。Pyhton自从2.3版以来一直采用Timsort算法排序，现在Java SE7和Android也采用Timsort算法对数组排序。 如果想自己用Python来写一个排序算法，完成本题要求该如何写？也就是对这个字典进行排序，{5: 1, 1: 3, 4: 1, 2: 2, 3: 1}，有什么好办法。 思路可以是两个指针遍历字典，如果左边大于右边，则替换位置。 更好的方法12345678910111213141516171819202122232425class Solution(object): def topKFrequent(self, nums, k): \"\"\" :type nums: List[int] :type k: int :rtype: List[int] \"\"\" m = dict() for num in nums: if num in m: m[num] += 1 else: m[num] = 1 bucket = [[] for _ in range(len(nums) + 1)] for key, value in m.items(): bucket[value].append(key) result = [] for i in range(len(nums), 0, -1): for n in bucket[i]: result.append(n) if len(result) == k: return result return result 很巧妙的使用列表的索引来保存value。 此时bucket值是[[], [5, 4, 3], [2], [1], [], [], [], [], []]，索引即出现次数。 此解法用时72ms，战胜 50.84 % 的 python3 提交记录。但是看了排在前面的算法，都是使用的Python内置的collections。 如果是你，会用什么方法呢？","tags":[]},{"title":"用node来DNS抓包","date":"2018-11-22T11:18:11.000Z","path":"2018/11/22/用node来DNS抓包/","text":"1这是崔斯特的第七十四篇原创文章 用node来DNS抓包 (๑• . •๑) 前提准备安装node，并安装依赖，去server.js同级目录下安装 12npm install native-dnsnpm install async 保存以下文件为server.js： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566let dns = require('native-dns');let server = dns.createServer();let authority = &#123; address: '114.114.114.114', port: 53, type: 'udp' &#125;;server.on('listening', () =&gt; console.log('server listening on', server.address()));server.on('close', () =&gt; console.log('server closed', server.address()));server.on('error', (err, buff, req, res) =&gt; console.error(err.stack));server.on('socketError', (err, socket) =&gt; console.error(err));function proxy(question, response, cb) &#123; console.log('proxying', question.name); var request = dns.Request(&#123; question: question, // forwarding the question server: authority, // this is the DNS server we are asking timeout: 1000 &#125;); // when we get answers, append them to the response request.on('message', (err, msg) =&gt; &#123; msg.answer.forEach(a =&gt; response.answer.push(a)); &#125;); request.on('end', cb); request.send();&#125;let async = require('async');let entries = [ &#123; domain: \"^weixin.keruyun.com*\", records: [ &#123; type: \"A\", address: \"10.10.10.90\", ttl: 1800 &#125; ] &#125;];function handleRequest(request, response) &#123; console.log('request from', request.address.address, 'for', request.question[0].name); let f = []; request.question.forEach(question =&gt; &#123; let entry = entries.filter(r =&gt; new RegExp(r.domain, 'i').exec(question.name)); if (entry.length) &#123; entry[0].records.forEach(record =&gt; &#123; record.name = question.name; record.ttl = record.ttl || 1800; response.answer.push(dns[record.type](record)); &#125;); &#125; else &#123; f.push(cb =&gt; proxy(question, response, cb)); &#125; &#125;); async.parallel(f, function() &#123; response.send(); &#125;);&#125;server.on('request', handleRequest);server.serve(53); 手机wifi设置如下： 在DNS1和DNS2，都设置为电脑端地址 找到域名使用命名node server.js运行js文件，手机打开飞行模式，再关闭飞行模式，关闭手机所有应用后，打开目标应用，查看输出 12345678910111213141516request from 10.10.10.30 for api.huoshan.comproxying api.huoshan.comrequest from 10.10.10.30 for api.huoshan.comproxying api.huoshan.comrequest from 10.10.10.30 for sf3-ttcdn-tos.pstatp.comproxying sf3-ttcdn-tos.pstatp.comrequest from 10.10.10.30 for nbsdk-baichuan.alicdn.comproxying nbsdk-baichuan.alicdn.comrequest from 10.10.10.30 for sf1-hscdn-tos.pstatp.comproxying sf1-hscdn-tos.pstatp.comrequest from 10.10.10.30 for wgo.mmstat.comproxying wgo.mmstat.comrequest from 10.10.10.30 for v7.pstatp.comproxying v7.pstatp.comrequest from 10.10.10.30 for sf1-ttcdn-tos.pstatp.comproxying sf1-ttcdn-tos.pstatp.com 找到自己想要抓取的域名，假设现在我们想抓取的域名是superapp.kiwa-tech.com，修改server.js文件，如下 12345678let entries = [ &#123; domain: \"^superapp.kiwa-tech.com*\", records: [ &#123; type: \"A\", address: \"10.10.10.90\", ttl: 1800 &#125; ] &#125;]; 配置Charles打开Charles，注意，要使用sudo打开，sudo /Applications/Charles.app/Contents/MacOS/Charles 安装证书，手机端也要安装 打开ssl proxying Reverse Proxies，设置如下 最后最后就大功告成了，此时在手机端打开该App，即可查看相关DNS抓包数据。 这种方法叫做DNS抓包","tags":[]},{"title":"App爬虫思路","date":"2018-11-22T11:14:55.000Z","path":"2018/11/22/App爬虫思路/","text":"1这是崔斯特的第七十五篇原创文章 App爬虫 (๑• . •๑) 看图一点点介绍 评估分析在抓包前，建议一定要先分析。 有网页端抓网页端、有移动端抓移动端、有客户端抓客户端，最后的选择才是App。 举个例子。针对腾讯视频考虑顺序： 网页端：https://v.qq.com/ 移动端：https://m.v.qq.com/index.html 客户端：通过charles设置代理抓取 App 腾讯自己就有做应用加固的，旗下产品自然很难被编译 如果你能编译腾讯旗下App，请告诉我方法 如何抓包在知乎上也有这个问题 如何在 Android 手机上实现抓包？ 但是我觉得不够全，首先对于一般的HTTP和HTTPS协议，通过最基本的Fiddler和Charles就可以抓包，具体方法请Google。需要保证两点：App走代理，证书被信任。 有些应用不走操作系统的 HTTP 代理，直接走 TCP 协议，无法使用 HTTP 代理抓包。虽然 Charles 支持 SOCKS 代理，但无法分析 TCP 包。这就需要祭出大杀器 tcpdump 和 Wireshark 。 说一下DNS抓包，。这个网上基本找不到资料，这也是大佬教我的，详细步骤在博客里，感兴趣自行翻阅。 还有一个比较出名的是mitmproxy，没怎么用过。 最后，苹果公司有做一个比较详细的抓包说明 https://developer.apple.com/library/archive/qa/qa1176/_index.html 如何逆向逆向一直是一个很大的话题，而且还专门有逆向工程师这个职位，可见其中水之深。 对于爬虫，简单了解即可。这里需要三个逆向工具： Apktool：获取资源文件和smail代码 dex2jar：将dex文件转换成jar文件 JD-GUI：查看到反编译后的dex的代码 最后看到的是JAVA代码，所以能看懂JAVA是必要能力。 JD-GUI我用的比较多，简单演示下。 主要是使用搜索功能，搜索前需要先花很多时间反编译。","tags":[]},{"title":"神经网络基础及Keras入门","date":"2018-11-20T14:09:44.000Z","path":"2018/11/20/神经网络基础及Keras入门/","text":"1这是崔斯特的第七十三篇原创文章 深度学习 (๑• . •๑) 神经网络定义人工神经网络，简称神经网络，在机器学习和认知科学领域，是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。 为了描述神经网络，我们先从最简单的神经网络讲起，这个神经网络仅由一个“神经元”构成，以下即是这个“神经元”的图示： 这个“神经元”是一个以 及截距 为输入值的运算单元，其输出为 ，其中函数 被称为“激活函数”。在本教程中，我们选用sigmoid函数作为激活函数 可以看出，这个单一“神经元”的输入－输出映射关系其实就是一个逻辑回归（logistic regression）。 神经网络模型所谓神经网络就是将许多个单一“神经元”联结在一起，这样，一个“神经元”的输出就可以是另一个“神经元”的输入。例如，下图就是一个简单的神经网络： 我们用 来表示网络的层数，本例中 ，我们将第 层记为 ，于是 是输入层，输出层是 。本例神经网络有参数 ，其中 （下面的式子中用到）是第 层第 单元与第 层第 单元之间的联接参数（其实就是连接线上的权重，注意标号顺序）， 是第 层第 单元的偏置项。因此在本例中， ， 。注意，没有其他单元连向偏置单元(即偏置单元没有输入)，因为它们总是输出 。同时，我们用 表示第 层的节点数（偏置单元不计在内）。 Keras实战使用keras实现如下网络结构, 并训练模型: 使用场景: 输入值(x1,x2,x3)代表人的身高体重和年龄, 输出值(y1,y2) 123456789101112131415161718import numpy as np# 总人数是1000, 一半是男生n = 1000# 所有的身体指标数据都是标准化数据, 平均值0, 标准差1tizhong = np.random.normal(size = n) shengao = np.random.normal(size=n)nianling = np.random.normal(size=n)# 性别数据, 前500名学生是男生, 用数字1表示gender = np.zeros(n)gender[:500] = 1# 男生的体重比较重,所以让男生的体重+1tizhong[:500] += 1# 男生的身高比较高, 所以让男生的升高 + 1shengao[:500] += 1# 男生的年龄偏小, 所以让男生年龄降低 1nianling[:500] -= 1 创建模型1234567891011121314151617from keras import Sequentialfrom keras.layers import Dense, Activationmodel = Sequential()# 只有一个神经元, 三个输入数值model.add(Dense(4, input_dim=3, kernel_initializer='random_normal', name=\"Dense1\"))# 激活函数使用softmaxmodel.add(Activation('relu', name=\"hidden\"))# 添加输出层model.add(Dense(2, input_dim=4, kernel_initializer='random_normal', name=\"Dense2\"))# 激活函数使用softmaxmodel.add(Activation('softmax', name=\"output\")) 编译模型需要指定优化器和损失函数: 123model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) 训练模型1234567# 转换成one-hot格式from keras import utilsgender_one_hot = utils.to_categorical(gender, num_classes=2)# 身体指标都放入一个矩阵data data = np.array([tizhong, shengao, nianling]).T# 训练模型model.fit(data, gender_one_hot, epochs=10, batch_size=8) 12345678910111213141516171819202122输出(stream):Epoch 1/101000/1000 [==============================] - 0s 235us/step - loss: 0.6743 - acc: 0.7180Epoch 2/101000/1000 [==============================] - 0s 86us/step - loss: 0.6162 - acc: 0.7310Epoch 3/101000/1000 [==============================] - 0s 88us/step - loss: 0.5592 - acc: 0.7570Epoch 4/101000/1000 [==============================] - 0s 87us/step - loss: 0.5162 - acc: 0.7680Epoch 5/101000/1000 [==============================] - 0s 89us/step - loss: 0.4867 - acc: 0.7770Epoch 6/101000/1000 [==============================] - 0s 88us/step - loss: 0.4663 - acc: 0.7830Epoch 7/101000/1000 [==============================] - 0s 87us/step - loss: 0.4539 - acc: 0.7890Epoch 8/101000/1000 [==============================] - 0s 86us/step - loss: 0.4469 - acc: 0.7920Epoch 9/101000/1000 [==============================] - 0s 88us/step - loss: 0.4431 - acc: 0.7940Epoch 10/101000/1000 [==============================] - 0s 88us/step - loss: 0.4407 - acc: 0.7900输出(plain): 进行预测123456test_data = np.array([[0, 0, 0]])probability = model.predict(test_data)if probability[0, 0]&gt;0.5: print('女生')else: print('男生') 12输出(stream):女生 关键词解释 input_dim: 输入的维度数 kernel_initializer: 数值初始化方法, 通常是正太分布 batch_size: 一次训练中, 样本数据被分割成多个小份, 每一小份包含的样本数叫做batch_size epochs: 如果说将所有数据训练一次叫做一轮的话。epochs决定了总共进行几轮训练。 optimizer: 优化器, 可以理解为求梯度的方法 loss: 损失函数, 可以理解为用于衡量估计值和观察值之间的差距, 差距越小, loss越小 metrics: 类似loss, 只是metrics不参与梯度计算, 只是一个衡量算法准确性的指标, 分类模型就用accuracy","tags":[]},{"title":"WebSocket爬虫之爬取龙珠弹幕","date":"2018-11-17T07:04:54.000Z","path":"2018/11/17/WebSocket爬虫之爬取龙珠弹幕/","text":"1这是崔斯特的第七十二篇原创文章 弹幕爬虫 (๑• . •๑) 我是一个直播爱好者，喜欢看很多人直播，以前可以看一天直播不出门。现在主要看这么些主播，虎牙的韦神、Dopa，斗鱼的狗贼嘘嘘。 对于其中的弹幕文化，非常感兴趣，就研究下，发现弹幕是用WebSocket实现的，那首先来说说什么是WebSocket。 WebSocket是什么详细内容可以看看这个问题WebSocket 是什么原理？为什么可以实现持久连接？ 简单解释下： HTTP 协议是一种无状态的、无连接的、单向的应用层协议。它采用了请求/响应模型。通信请求只能由客户端发起，服务端对请求做出应答处理。 这种通信模型有一个弊端：HTTP 协议无法实现服务器主动向客户端发起消息。大多数 Web 应用程序将通过频繁的异步JavaScript和XML（AJAX）请求实现长轮询。轮询的效率低，非常浪费资源（因为必须不停连接，或者 HTTP 连接始终打开）。 WebSocket的最大特点就是，服务器可以主动向客户端推送信息，客户端也可以主动向服务器发送信息。 WebSocket 如何工作 事件 事件处理程序 描述 open WebSocket.onopen 连接建立时触发 message WebSocket.onmessage 客户端接收服务端数据时触发 error WebSocket.onerror 通信发生错误时触发 close WebSocket.onclose 连接关闭时触发 一个非常典型的WebSocket创建方式如下（来自某巨头）： 1234567891011function r() &#123; if (!d) &#123; var t = i(); I.info(\"%cconnecting \" + t, p(\"#0000E3\")), u = new WebSocket(t), u.onopen = n, u.onclose = o, u.onerror = a, u.onmessage = h &#125;&#125; WebSocket获取龙珠直播弹幕本次使用的Python第三方库是 websocket-client 看看官方例子： 1234567891011121314151617181920212223242526272829303132333435import websockettry: import threadexcept ImportError: import _thread as threadimport timedef on_message(ws, message): print(message)def on_error(ws, error): print(error)def on_close(ws): print(\"### closed ###\")def on_open(ws): def run(*args): for i in range(3): time.sleep(1) ws.send(\"Hello %d\" % i) time.sleep(1) ws.close() print(\"thread terminating...\") thread.start_new_thread(run, ())if __name__ == \"__main__\": websocket.enableTrace(True) ws = websocket.WebSocketApp(\"ws://echo.websocket.org/\", on_message = on_message, on_error = on_error, on_close = on_close) ws.on_open = on_open ws.run_forever() 是不是非常熟悉，和上面讲到的一模一样，4种主要思想方法都是一致的，可以直接调用。 那么到了实践环节，本次选取的是龙珠直播，为啥不是虎牙、斗鱼呢？这个待会再说，我们打开龙珠某个直播间 在网络里面选择ws这一项，即可看到相关连接，而且这些消息是加密过的，别急，我们打开m站试试 这个时候传输的弹幕消息已经没有加密过，直接对比，看到了一条“哈哈哈”的消息，所以我们现在可以确定就是这个websocket连接在传输相关消息。 依葫芦画瓢，我们尝试用Python来连接 Curl: 1curl 'wss://mbgows.plu.cn:8806/?room_id=2185&amp;group=0' -H 'Pragma: no-cache' -H 'Origin: http://m.longzhu.com' -H 'Accept-Encoding: gzip, deflate, br' -H 'Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7' -H 'Sec-WebSocket-Key: n72+EfLt2iSrQ0EswTZ+2A==' -H 'User-Agent: Mozilla/5.0 (Linux; Android 8.0.0; Pixel 2 XL Build/OPD1.170816.004) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Mobile Safari/537.36' -H 'Upgrade: websocket' -H 'Sec-WebSocket-Extensions: permessage-deflate; client_max_window_bits' -H 'Cache-Control: no-cache' -H 'Cookie: pluguest=81D781D68480BE065D952CA699B38E6627B61756AEF57338B39053154850A9502BC7FD850F86922BDF3DBD7F774BFDE5CBC80838A34B8F26' -H 'Connection: Upgrade' -H 'Sec-WebSocket-Version: 13' --compressed Python代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#!/usr/bin/env python # -*- coding: utf-8 -*-\"\"\"@author: zhangslob@file: longzhu_websocket.py @time: 2018/11/17@desc: simple websocket client to connect longzhu\"\"\"import websockettry: import threadexcept ImportError: import _thread as threadimport timedef on_message(ws, message): import json try: print(json.loads(message)) except: print(message)def on_error(ws, error): print(error)def on_close(ws): print(\"### closed ###\")def on_open(ws): pass # def run(*args): # for i in range(3): # time.sleep(1) # ws.send(\"Hello %d\" % i) # time.sleep(1) # ws.close() # print(\"thread terminating...\") # thread.start_new_thread(run, ())headers = &#123; 'Pragma': 'no-cache', 'Origin': 'http://m.longzhu.com', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7', # 'Sec-WebSocket-Key': 'n72+EfLt2iSrQ0EswTZ+2A==', 'User-Agent': 'Mozilla/5.0 (Linux; Android 8.0.0; Pixel 2 XL Build/OPD1.170816.004) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Mobile Safari/537.36', 'Upgrade': 'websocket', 'Sec-WebSocket-Extensions': 'permessage-deflate; client_max_window_bits', 'Cache-Control': 'no-cache', 'Connection': 'Upgrade', 'Sec-WebSocket-Version': '13',&#125;if __name__ == \"__main__\": websocket.enableTrace(True) ws = websocket.WebSocketApp(\"wss://mbgows.plu.cn:8806/?room_id=2185&amp;group=0\", on_message=on_message, on_error=on_error, on_close=on_close, header=headers) ws.on_open = on_open ws.run_forever() 你可以直接运行上面的代码，看看会有什么结果 龙珠直播这个有点奇怪，你根本不用去向服务器发送什么消息，服务器无脑给你推送弹幕，常见的是客户端需要先告诉服务器“我是谁”，然后服务器再推送消息给你，并且还有有心跳检测，“我”告诉服务器我还在看呢，你继续给我弹幕，看看虎牙和斗鱼。 图中绿色的是发送的消息，红色是接受的消息。像这种情况就需要自己去看js代码是如何处理消息的。斗鱼的话有公开自己的弹幕服务器第三方接入协议。","tags":[]},{"title":"Pycharm插件推荐：Docker integration","date":"2018-11-15T10:58:11.000Z","path":"2018/11/15/Pycharm插件推荐：Docker-integration/","text":"1这是崔斯特的第七十一篇原创文章 简易Docker操作 (๑• . •๑) 插件Docker在设置-Plugins-BROWSE REPOSITORIES… 中搜索“Docker” 安装即可，重启使用 当然本地必须先安装Docker，并且打开Docker，而且需要加入用户组 1$ sudo groupadd docker &amp;&amp; sudo usermod -aG docker $USER 之后就可以不加sudo运行Docker命令了 1$ docker run hello-world 打开之后就是这样的 分别就是容器和镜像，下面举一个简单的栗子，先去拉python的镜像，然后运行helloworld.py文件 12345FROM pythonCOPY helloworld.py /app/helloworld.pyWORKDIR /app 在Dockerfile左边有一个绿色按钮，Run Dockerfile helloworld.py 1print 'hello world' 这里会直接报错，因为python3嘛，然后我们现在需要修复，可以先修改python文件，再重新build，这里介绍一种更简单的方法。 右键点击构建好的容器，点击“Exec”，输入/bin/bash进入shell 12345678910root@a8b31f52a720:/app# lshelloworld.pyroot@a8b31f52a720:/app# python helloworld.py File \"helloworld.py\", line 1 print 'hello world' ^SyntaxError: Missing parentheses in call to 'print'. Did you mean print('hello world')?root@a8b31f52a720:/app# python --versionPython 3.7.1root@a8b31f52a720:/app# 我们拉取的镜像是Python3.7版本的，我想要直接在Pycharm中编辑PY文件，并让Docker直接同步文件变化，可以使用下面这个功能：Volume Bindings 分别填上Docker中的目录与本地所在目录，一定要点击右下角的SAVE，稍等片刻，点击Volume Bindings 就可以看到关联了。 其实这个功能就是Docker中的Volume，Docker 中可以通过 Volume 实现持久化存储和数据共享。只不过这里通过可视化非常方便调试。 还有其他功能，像设置环境变量、端口暴露等等，使用这个插件就会比较容易处理。 其实我用的比较多的情况是爬虫本地和线上有差距时就会来测试，有时候是发现某个依赖没有装，有时候则是网络问题。 Docker测试可以保证的一点是环境完全一致。","tags":[{"name":"Pycharm","slug":"Pycharm","permalink":"https://zhangslob.github.io/tags/Pycharm/"},{"name":"Docker","slug":"Docker","permalink":"https://zhangslob.github.io/tags/Docker/"}]},{"title":"Python实现常见的回文字符串算法","date":"2018-11-13T14:09:41.000Z","path":"2018/11/13/Python实现常见的回文字符串算法/","text":"1这是崔斯特的第七十篇原创文章 回文利用python 自带的翻转 函数reversed() 12def is_plalindrome(string): return string == ''.join(list(reversed(string))) 自己实现 1234567891011def is_plalindrome(string): string = list(string) length = len(string) left = 0 right = length - 1 while left &lt; right: if string[left] != string[right]: return False left += 1 right -= 1 return True 最长的回文子串暴力破解暴力破解，枚举所有的子串，对每个子串判断是否为回文， 时间复杂度为 O(n^3) 动态规划12345678910111213141516171819202122def solution(s): s = list(s) l = len(s) dp = [[0] * l for i in range(l)] for i in range(l): dp[i][i] = True # 当 k = 2时要用到 dp[i][i - 1] = True resLeft = 0 resRight = 0 # 枚举子串的长度 for k in range(2, l+1): # 子串的起始位置 for i in range(0, l-k+1): j = i + k - 1 if s[i] == s[j] and dp[i + 1][j - 1]: dp[i][j] = True # 保存最长的回文起点和终点 if resRight - resLeft + 1 &lt; k: resLeft = i resRight = j return ''.join(s[resLeft:resRight+1]) 时间复杂度为 O(n^2), 空间复杂度为 O(n^2) Manacher 算法Manacher 算法首先对字符串做一个预处理,使得所有的串都是奇数长度, 插入的是同样的符号且符号不存在与原串中，串的回文性不受影响 12aba =&gt; #a#b#a#abab =&gt; #a#b#a#b# 我们把回文串中最右位置与其对称轴的距离称为回文半径，Manacher 算法定义了一个回文半径数组 RL，RL[i]表示以第 i 个字符为对称轴的回文半径，对于上面得到的插入分隔符的串来说，我们可以得到 RL数组 12345678char: # a # b # a #RL: 1 2 1 4 1 2 1RL-1: 0 1 0 3 0 1 0i: 0 1 2 3 4 5 6char: # a # b # a # b #RL: 1 2 1 4 1 4 1 2 1RL-1: 0 1 0 3 0 3 0 1 0i: 0 1 2 3 4 5 6 7 8 我们还求了 RL[i] - 1: 我们发现RL[i] -1 正好是初始字符串中以位置i 为对称轴的最长回文长度 所以下面就是重点如何求得 RL 数组了， 可以参考这篇文章 (讲得比较清晰) 下面是算法实现 12345678910111213141516171819def manacher(preS): s = '#' + '#'.join(preS) + '#' l = len(s) RL = [0] * l maxRight = pos = maxLen = 0 for i in range(l): if i &lt; maxRight: RL[i] = min(RL[2*pos - i], maxRight-i) else: RL[i] = 1 while i - RL[i] &gt;= 0 and i + RL[i] &lt; l and s[i - RL[i]] == s[i + RL[i]]: RL[i] += 1 if i + RL[i] - 1 &gt; maxRight: maxRight = i + RL[i] - 1 pos = i maxLen = max(RL) idx = RL.index(maxLen) sub = s[idx - maxLen + 1: idx + maxLen] return sub.replace('#', '') 空间复杂度：借助了一个辅助数组，空间复杂度为 O(n)时间复杂度：尽管内层存在循环，但是内层循环只对尚未匹配的部分进行，对于每一个字符来说，只会进行一次，所以时间复杂度是 O(n) 最长回文前缀所谓前缀，就是以第一个字符开始 下面的最长回文前缀 123abbabbc =&gt; abbcabababb =&gt; ababasogou =&gt; s 将原串逆转，那么问题就转变为求原串的前缀和逆串后缀相等且长度最大的值, 这个问题其实就是 KMP 算法中的 next 数组的求解了 具体求解： 将原串逆转并拼接到原串中， 以’#’ 分隔原串和逆转避免内部字符串干扰。 12345678910111213141516def longest_palindrome_prefix(s): if not s: return 0 s = s + '#' + s[::-1] + '$' i = 0 j = -1 nt = [0] * len(s) nt[0] = -1 while i &lt; len(s) - 1: if j == -1 or s[i] == s[j]: i += 1 j += 1 nt[i] = j else: j = nt[j] return nt[len(s) - 1] 添加字符生成最短回文字符串这道题其实跟上面基本是一样的，实例： 12aacecaaa -&gt; aaacecaaa # 添加 aabcd -&gt; dcbabcd # 添加 dcb 我们先求字符串的最长回文前缀, 然后剩余的字符串逆转并拼接到字符串的头部即是问题所求 123def solution(s): length = longest_palindrome_prefix(s) return s[length:][::-1] + s 最长回文子序列动态规划法 dp[i][j] 表示子序列 s[i..j] 中存在的最长回文子序列长度 初始化dp[i][i] = 1 当 s[i] == s[j] 为 true 时，dp[i][j] = dp[i+1][j - 1] + 2 当 s[i] == s[j] 为 false 时，dp[i][j] = max(dp[i+1][j], dp[i][j - 1]) 12345678910111213141516# 求得最长回文子序列的长度def solution(s): l = len(s) dp = [[0] * l for i in range(l)] for i in range(l): dp[i][i] = 1 # 枚举子串的长度 for k in range(2, l+1): # 枚举子串的起始位置 for i in range(0, l-k+1): j = i + k - 1 if s[i] == s[j]: dp[i][j] = dp[i + 1][j - 1] + 2 else: dp[i][j] = max(dp[i][j - 1], dp[i + 1][j]) return dp[0][l-1] 时间复杂度为 O(n^2), 空间复杂度为 O(n^2) 转自：http://youbookee.com/2016/09/06/plalindrome-substring/","tags":[{"name":"算法","slug":"算法","permalink":"https://zhangslob.github.io/tags/算法/"}]},{"title":"爬虫之全站爬取方法","date":"2018-11-07T11:23:43.000Z","path":"2018/11/07/爬虫之全站爬取方法/","text":"1这是崔斯特的第六十九篇原创文章 方法做过好几个关于网站全站的项目，这里总结一下。 先把上面那张图写下来，全站爬取的两种方法： 关系网络： 优点：简单；可以抓取“热门”数据 缺点：无法抓取全量数据；速度慢；需要解决去重问题 可行性：比较高 遍历ID 优点：可以抓取所有数据；不用数据去重 缺点：资源消耗大；速度慢；可能被发现 可行性：仅可用于ID自增 关于关系网络其实这个很好理解。比如说知乎，一个大V有100W粉丝，从这个大V出发，抓取粉丝的粉丝，一直循环下去。（可能是个死循环） 这个方法就比较简单，Scrapy中就是继承CrawlSpider，再编写匹配规则就好。 Example 12345678910111213141516171819202122232425import scrapyfrom scrapy.spiders import CrawlSpider, Rulefrom scrapy.linkextractors import LinkExtractorclass MySpider(CrawlSpider): name = 'example.com' allowed_domains = ['example.com'] start_urls = ['http://www.example.com'] rules = ( # Extract links matching 'category.php' (but not matching 'subsection.php') # and follow links from them (since no callback means follow=True by default). Rule(LinkExtractor(allow=('category\\.php', ), deny=('subsection\\.php', ))), # Extract links matching 'item.php' and parse them with the spider's method parse_item Rule(LinkExtractor(allow=('item\\.php', )), callback='parse_item'), ) def parse_item(self, response): self.logger.info('Hi, this is an item page! %s', response.url) item = scrapy.Item() item['id'] = response.xpath('//td[@id=\"item_id\"]/text()').re(r'ID: (\\d+)') item['name'] = response.xpath('//td[@id=\"item_name\"]/text()').extract() item['description'] = response.xpath('//td[@id=\"item_description\"]/text()').extract() return item 这种方法一般是搜索引擎会做的。而且抓取的内容基本是最多人看到的，所以月排在前面，和SEO有关。 但是这种方法的缺点也是很明显的，最明显的就是没法抓全数据，像那种冷门的数据就没法抓取到，速度也是比较慢的，必须保存去重队列，以防止重复抓取页面。（了解下布隆过滤器） 如果对数据完整性要求没那么高可以考虑这种方法。 遍历ID找各种方法就比较无脑了，啥也不用想，从0开始遍历跑吧。 毫无疑问，这种方法可以抓取网站所有的数据，因为在开始抓取前就已经完成的去重，所以这方面就不用管了。 但是缺点也很明显，因为是遍历ID，所以需要很多服务器资源和代理资源，有可能某个ID已经下架或失效。所以整个工程请求量会非常大。而且可能被别人发现，一般人都去看那些热门帖子，结果你把那么重来没人看的翻了一遍，别人也会发现数据异常的（也会存在假数据的情况😭）。 而且这种方法之适用于ID自增的，大多数是数字ID递增，比如说天眼查的： 123https://www.tianyancha.com/company/24762997https://www.tianyancha.com/company/150041670https://www.tianyancha.com/company/1073358312 知乎也是： 123https://zhuanlan.zhihu.com/p/47969297https://zhuanlan.zhihu.com/p/48652497https://zhuanlan.zhihu.com/p/47805332 应该是和数字有关系，可以先采样进行抓取，研究数据分布情况。 当提供不正确ID时，也会返回数据不存在的情况 在这里提供一个生成ID的方法 1234567891011121314def gen_uid(num): \"\"\" 使用生成器生成ID :param num: 起始ID :return: 生成器 \"\"\" js = 0 result = list() while js &lt; 20000: num += 1 js += 1 result.append(num) yield result 最后再看看这张图。两种方法都有优缺点，根据实际需求选取，如果你还知道别的抓取方法，欢迎指出。","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"}]},{"title":"Scala学习之相同的函数名","date":"2018-11-05T14:29:02.000Z","path":"2018/11/05/Scala学习之相同的函数名/","text":"1这是崔斯特的第六十八篇原创文章 最近在慢慢的开始用Scala写Spark，刚开始也是在菜鸟教程上简单过了一遍Scala，现在还记得一些基本语法，还有各种奇怪的语法糖。想要更快的学习这门语言，还是需要自己多谢谢。 今天在看别人代码时，发现他在类中定义了两个名字一模一样的函数，但是我还以为他写错了。当我以Python的思维去看待这们语言时，当然是在作死的边缘了。 举一个简单的栗子，从MongoDB读取数据时使用的配置所定义的两个函数 12345678910def readConfig(context: SparkContext, database: String, collection: String): ReadConfig = &#123; ReadConfig(Map(\"database\" -&gt; database, \"collection\" -&gt; collection), Some(ReadConfig(context)))&#125;def readConfig(session: SparkSession, database: String, collection: String): ReadConfig = &#123; ReadConfig(Map(\"database\" -&gt; database, \"collection\" -&gt; collection), Some(ReadConfig(session)))&#125; 可以看到两个函数名一样的，刚开始我也很懵逼，但是仔细一看，他的参数是不一样的，一个是context: SparkContext，一个是session: SparkSession。这两个函数的功能一样，但是接受的参数不一样，所以这才需要去定义两个函数。（试想一下，在Python中如何去实现） 可能你不懂Scala，让我简单跟你讲一下。首先我们使用def关键字定义了一个名为readConfig的函数，和Python一模一样；然后就是函数接受的参数了，可以看到在每个参数后面有冒号，这个冒号就表示前面的数据类型，比如定义一个相加函数 1234567object add&#123; def addInt(a:Int, b:Int) : Int = &#123; var sum:Int = 0 sum = a + b return sum &#125;&#125; Scala会在变量后定义类型，这样就可以区别不同的函数了。 回到上面的栗子，一个类型是SparkContext，一个是SparkSession，这是Spark的数据类型；然后我们可以看到在函数末尾是冒号加ReadConfig，这里说的返回的数据类型是ReadConfig，也就是从MongoDB读取数据时使用的配置类型。（如果是Unit则不用返回任何数据，和Java类似） 是不是很好理解，在调用函数时，只要我们传递的第一个参数是不同类型，就会使用相应的函数。 都说Scala是一种函数式语言，函数是 Scala 语言的核心，看看Scala函数的特性，不说了，继续撸Scala了。","tags":[{"name":"scala","slug":"scala","permalink":"https://zhangslob.github.io/tags/scala/"}]},{"title":"Spark实战（二）学习UDF","date":"2018-10-29T03:15:55.000Z","path":"2018/10/29/Spark实战（二）学习UDF/","text":"1这是崔斯特的第六十七篇原创文章 在开始正式数据处理之前，我觉得有必要去学习理解下UDF。 UDFUDF全称User-Defined Functions，用户自定义函数，是Spark SQL的一项功能，用于定义新的基于列的函数，这些函数扩展了Spark SQL的DSL用于转换数据集的词汇表。 我在databricks上找到一个比较简单理解的入门栗子： Register the function as a UDF1234val squared = (s: Int) =&gt; &#123; s * s&#125;spark.udf.register(\"square\", squared) Call the UDF in Spark SQL1spark.range(1, 20).registerTempTable(\"test\") 1%sql select id, square(id) as id_squared from test 我理解就是先定义一个函数squared，返回输入数字的平方，然后register，并绑定square方法名为square，然后就在Spark SQL中直接使用square方法。 实例一：温度转化1234567891011121314151617import org.apache.spark.sql.SparkSessionimport org.apache.spark.SparkConfobject ScalaUDFExample &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName(\"Scala UDF Example\") val spark = SparkSession.builder().enableHiveSupport().config(conf).getOrCreate() val ds = spark.read.json(\"temperatures.json\") ds.createOrReplaceTempView(\"citytemps\") // Register the UDF with our SparkSession spark.udf.register(\"CTOF\", (degreesCelcius: Double) =&gt; ((degreesCelcius * 9.0 / 5.0) + 32.0)) spark.sql(\"SELECT city, CTOF(avgLow) AS avgLowF, CTOF(avgHigh) AS avgHighF FROM citytemps\").show() &#125;&#125; 我们将定义一个 UDF 来将以下 JSON 数据中的温度从摄氏度（degrees Celsius）转换为华氏度（degrees Fahrenheit）： 1234567&#123;\"city\":\"St. John's\",\"avgHigh\":8.7,\"avgLow\":0.6&#125;&#123;\"city\":\"Charlottetown\",\"avgHigh\":9.7,\"avgLow\":0.9&#125;&#123;\"city\":\"Halifax\",\"avgHigh\":11.0,\"avgLow\":1.6&#125;&#123;\"city\":\"Fredericton\",\"avgHigh\":11.2,\"avgLow\":-0.5&#125;&#123;\"city\":\"Quebec\",\"avgHigh\":9.0,\"avgLow\":-1.0&#125;&#123;\"city\":\"Montreal\",\"avgHigh\":11.1,\"avgLow\":1.4&#125;... 实例二：时间转化1234567891011case class Purchase(customer_id: Int, purchase_id: Int, date: String, time: String, tz: String, amount:Double)val x = sc.parallelize(Array( Purchase(123, 234, \"2007-12-12\", \"20:50\", \"UTC\", 500.99), Purchase(123, 247, \"2007-12-12\", \"15:30\", \"PST\", 300.22), Purchase(189, 254, \"2007-12-13\", \"00:50\", \"EST\", 122.19), Purchase(187, 299, \"2007-12-12\", \"07:30\", \"UTC\", 524.37)))val df = sqlContext.createDataFrame(x)df.registerTempTable(\"df\") 自定义函数 1234567def makeDT(date: String, time: String, tz: String) = s\"$date $time $tz\"sqlContext.udf.register(\"makeDt\", makeDT(_:String,_:String,_:String))// Now we can use our function directly in SparkSQL.sqlContext.sql(\"SELECT amount, makeDt(date, time, tz) from df\").take(2)// but not outsidedf.select($\"customer_id\", makeDt($\"date\", $\"time\", $\"tz\"), $\"amount\").take(2) // fails 如果想要在SQL外面使用，必须通过spark.sql.function.udf来创建UDF 1234import org.apache.spark.sql.functions.udfval makeDt = udf(makeDT(_:String,_:String,_:String))// now this worksdf.select($\"customer_id\", makeDt($\"date\", $\"time\", $\"tz\"), $\"amount\").take(2) 实践操作写一个UDF来将一些Int数字分类 123456789101112131415val formatDistribution = (view: Int) =&gt; &#123; if (view &lt; 10) &#123; \"&lt;10\" &#125; else if (view &lt;= 100) &#123; \"10~100\" &#125; else if (view &lt;= 1000) &#123; \"100~1K\" &#125; else if (view &lt;= 10000) &#123; \"1K~10K\" &#125; else if (view &lt;= 100000) &#123; \"10K~100K\" &#125; else &#123; \"&gt;100K\" &#125;&#125; 注册： 1session.udf.register(\"formatDistribution\", UDF.formatDistribution) SQL： 1session.sql(\"select user_id, formatDistribution(variance_digg_count) as variance from video\") 写到这里，再回顾UDF，我感觉这就像是去为了方便做一个分类转化等操作，和Python里面的函数一样，只不过这里的UDF一般特指Spark SQL里面使用的函数。然后发现这里和SQL中的自定义函数挺像的: 12345678910111213CREATE FUNCTION [函数所有者.]&lt;函数名称&gt; ( -- 添加函数所需的参数，可以没有参数 [&lt;@param1&gt; &lt;参数类型&gt;] [,&lt;@param1&gt; &lt;参数类型&gt;]…)RETURNS TABLE ASRETURN ( -- 查询返回的SQL语句 SELECT查询语句) 123456789101112131415161718192021222324252627282930/** 创建内联表值函数，查询交易总额大于1W的开户人个人信息*/create function getCustInfo()returns @CustInfo table --返回table类型( --账户ID CustID int, --帐户名称 CustName varchar(20) not null, --身份证号 IDCard varchar(18), --电话 TelePhone varchar(13) not null, --地址 Address varchar(50) default('地址不详'))asbegin --为table表赋值 insert into @CustInfo select CustID,CustName,IDCard,TelePhone,Address from AccountInfo where CustID in (select CustID from CardInfo where CardID in (select CardID from TransInfo group by CardID,transID,TransType,TransMoney,TransDate having sum(TransMoney)&gt;10000)) returnendgo-- 调用内联表值函数select * from getCustInfo()go 好像有异曲同工之妙~","tags":[{"name":"spark","slug":"spark","permalink":"https://zhangslob.github.io/tags/spark/"}]},{"title":"spark实战（一）：对数据处理的理解","date":"2018-10-24T03:05:16.000Z","path":"2018/10/24/spark实战一：对数据处理的理解/","text":"1这是崔斯特的第六十六篇原创文章 大数据的学习经历大数据是什么，我觉得应该有这么几步： 数据采集（Python&amp;&amp;Scrapy） 数据清洗（Scala&amp;&amp;Spark） 指标计算（Scala&amp;&amp;Spark） 数据展示（Postgre） 做了一段时间的爬虫，或者说叫数据采集，但是从来没有接触到大数据的下一步。最近和慢慢聊了下，对数据处理也很感兴趣，便想去接触、尝试。 在慢慢、登登等人的帮助下，开始学习了一段时间的Spark。刚开始学习SQL语句，现在就记得简单的那些了。然后去看Spark文档，自己动手开始写。 因为团队主要用Scala开发，所以中途又花了一些时间来学习Scala语言，感觉和Python有许多类似的地方，并不是太难，但是Scala有一些特性，不是太明白，主要是自己写太少了。爬虫什么的还是用Python。 然后再去看具体的Spark 项目，模仿（抄袭）别人的代码，大概也看懂了一些，由于始终没有自己实践来操作，所以缺乏实际操作能力。最好的办法就是自己去写，所以计划自己去单独做一个项目试试看，手头上还有好多已开发但是还没开始计算的项目，我自己可以先试试看。 大数据处理步骤在上面所说的基础上再说细一些。 数据采集就别说了，就是采集我们所需要的（产品经理要求的）数据 数据清洗这里，是和数据采集有很大关系的。分为这么两步： 提取指定数据。因为我们在做数据采集的时候，为了保证数据的完整性，是尽可能多的保存原始数据，也就是别人给多少，我就取多少，但是这些数据并非都是有用的数据，所以第一步是需要去提取原始数据表中我们所需要的指定数据。这里方法有很多，用SQL，或者在道路数据的时候直接选取那些字段即可。最后我们把这些提取的数据重新保存在新的表中，这样后面计算会方便很多。（其实这一步在数据采集就可以完成，但是谁也说不清到底以后会不会需要更多的数据） 清洗数据。比如时间维度、为空字段等等，这些现在遇到的还不多，以后再补上。 指标计算。这一步就需要根据具体的需求来计算各种维度，并把计算好的数据保存新表。 数据展示。这部分好像是后端做的，待了解。 需要了解的知识 开发工具和环境 MongoDB Postgre SQL，JOIN Docker IDEA、SBT Scala（2.10+） Spark 官方文档 UDF 目前感觉以上部分都有一知半解，必须要实际去做才能知道到底那部分掌握，那部分还不懂。 我的计划 找PM要DM文档，理解具体业务需求 根据需求来选择数据库中的数据维度 在本地尝试数据清洗并计算指标 有问题请教慢慢＆登登","tags":[{"name":"spark","slug":"spark","permalink":"https://zhangslob.github.io/tags/spark/"}]},{"title":"Spark教程（二）Spark连接MongoDB","date":"2018-09-03T14:00:59.000Z","path":"2018/09/03/Spark教程（二）Spark连接MongoDB/","text":"1这是崔斯特的第六十五篇原创文章 学习Spark (๑• . •๑) 如何导入数据数据可能有各种格式，虽然常见的是HDFS，但是因为在Python爬虫中数据库用的比较多的是MongoDB，所以这里会重点说说如何用spark导入MongoDB中的数据。 当然，首先你需要在自己电脑上安装spark环境，简单说下，在这里下载spark，同时需要配置好JAVA，Scala环境。 这里建议使用Jupyter notebook，会比较方便，在环境变量中这样设置 12PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=notebook ./bin/pyspark 如果你的环境中有多个Python版本，同样可以制定你想要使用的解释器，我这里是python36，根据需求修改。 1PYSPARK_PYTHON=/usr/bin/python36 启动命令进入spark根目录，./bin/pyspark这是最简单的启动命令，默认会打开Python的交互式解释器，但是由于我们上面有设置过，会打开Jupyter notebook，接下来变成会方便很多。 先来看看最简单的例子： 12345678910111213&gt;&gt;&gt; textFile = spark.read.text(\"README.md\")&gt;&gt;&gt; textFile.count() # Number of rows in this DataFrame126&gt;&gt;&gt; textFile.first() # First row in this DataFrameRow(value=u'# Apache Spark')&gt;&gt;&gt; linesWithSpark = textFile.filter(textFile.value.contains(\"Spark\"))&gt;&gt;&gt; textFile.filter(textFile.value.contains(\"Spark\")).count() # How many lines contain \"Spark\"?15 这里有我之前写过的例子，可以照着写一遍 basic_exercise 我们的启动方式是./bin/pyspark，我们可以家后面加很多参数，比如说如若我们要连接MongoDB，就需要这样 完整的可以参考Spark Connector Python Guide 123./bin/pyspark --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\ --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\ --packages org.mongodb.spark:mongo-spark-connector_2.11:2.3.0 这里有两个uri，分别是input和output，对应读取的数据库和写入的数据库，最后面的packages相当于引入的包的名字，我一般喜欢在代码中定义。 读取/保存数据这里我们可以增加参数option，在这里设置想要读取的数据库地址，注意格式。 读取数据 12df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/people.contacts\").load() 保存数据 123people.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").option(\"uri\",\"mongodb://127.0.0.1/people.contacts\").option(\"database\",\"people\").option(\"collection\", \"contacts\").save() 简单对比下，option还可以定义database和collection，这样就不需要在启动Spark时定义。 以上是官网推荐的连接方式，这里需要说的是另一种，如果我没有从命令行中启动，而是直接新建一个py文件，该如何操作？ 搜索相关资料后，发现是这样 123456789101112131415161718192021222324#!/usr/bin/env python# -*- coding: utf-8 -*-__author__ = 'zhangslob'import osfrom pyspark.sql import SparkSession# set PYSPARK_PYTHON to python36os.environ['PYSPARK_PYTHON'] = '/usr/bin/python36'# load mongo datainput_uri = \"mongodb://127.0.0.1:spark.spark_test\"output_uri = \"mongodb://127.0.0.1:spark.spark_test\"my_spark = SparkSession\\ .builder\\ .appName(\"MyApp\")\\ .config(\"spark.mongodb.input.uri\", input_uri)\\ .config(\"spark.mongodb.output.uri\", output_uri)\\ .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.11:2.2.0')\\ .getOrCreate()df = my_spark.read.format('com.mongodb.spark.sql.DefaultSource').load() 必须要增加默认设置(&#39;spark.jars.packages&#39;,&#39;org.mongodb.spark:mongo-spark-connector_2.11:2.2.0&#39;)，否则会报错。","tags":[{"name":"spark","slug":"spark","permalink":"https://zhangslob.github.io/tags/spark/"}]},{"title":"Spark教程（一）为什么要学spark","date":"2018-09-01T14:00:50.000Z","path":"2018/09/01/Spark教程（一）为什么要学spark/","text":"1这是崔斯特的第六十四篇原创文章 学习Spark (๑• . •๑) 介绍Apache Spark是一个开源集群运算框架，最初是由加州大学柏克莱分校AMPLab所开发。相对于Hadoop的MapReduce会在运行完工作后将中介数据存放到磁盘中，Spark使用了存储器内运算技术，能在数据尚未写入硬盘时即在存储器内分析运算。 Spark在存储器内运行程序的运算速度能做到比Hadoop MapReduce的运算速度快上100倍，即便是运行程序于硬盘时，Spark也能快上10倍速度。Spark允许用户将数据加载至集群存储器，并多次对其进行查询，非常适合用于机器学习算法 Spark也支持伪分布式（pseudo-distributed）本地模式，不过通常只用于开发或测试时以本机文件系统取代分布式存储系统。在这样的情况下，Spark仅在一台机器上使用每个CPU核心运行程序。 Apache Spark™ is a unified analytics engine for large-scale data processing. 优势 首先，Spark为我们提供了一个全面、统一的框架用于管理各种有着不同性质（文本数据、图表数据等）的数据集和数据源（批量数据或实时的流数据）的大数据处理的需求。 Spark可以将Hadoop集群中的应用在内存中的运行速度提升100倍，甚至能够将应用在磁盘上的运行速度提升10倍。 Spark让开发者可以快速的用Java、Scala或Python编写程序。它本身自带了一个超过80个高阶操作符集合。而且还可以用它在shell中以交互式地查询数据。 除了Map和Reduce操作之外，它还支持SQL查询，流数据，机器学习和图表数据处理。开发者可以在一个数据管道用例中单独使用某一能力或者将这些能力结合在一起使用。 亲身体会经过这两周的折腾，总算是在本地环境下完成了第一个spark项目，完成十万级文本分词和去重，速度还是挺快的，从读取数据、处理数据、再到保存数据，大概花了十分钟左右。这里操作的数据库都是MongoDB，因为爬虫爬取的数据都是直接保存到Mongo。 之后再增加数据量，达到四千多万，读取数据花了8分钟，下图是正在处理和保存数据的Spark UI。 以前处理数据会使用pandas，数据会保存在内存中，数据量过大就会崩了，这也是为什么要使用分布式计算的原因。没有做过横向对比，暂时还不知道有多大差别。 最后花了1.3h，本地处理完了四千多万数据，CPU和内存都要炸了，看来以后计算部分还是要搭集群。 学习计划我在Github上开了一个仓库，记录所学，地址在这：learning-spark 刚开始使用的语言还是Python，目标是学Scala，看了些基础语法，和Python挺类似的，以后多写写，维持这个项目，记录各种坑。 ok，BB了这么多，下一篇就要开始真正的代码实战了。","tags":[{"name":"spark","slug":"spark","permalink":"https://zhangslob.github.io/tags/spark/"}]},{"title":"使用scrapy发送post请求的坑","date":"2018-08-24T13:54:28.000Z","path":"2018/08/24/使用scrapy发送post请求的坑/","text":"1这是崔斯特的第六十三篇原创文章 使用scrapy发送post请求的坑 使用requests发送post请求先来看看使用requests来发送post请求是多少好用，发送请求 Requests 简便的 API 意味着所有 HTTP 请求类型都是显而易见的。例如，你可以这样发送一个 HTTP POST 请求： 1&gt;&gt;&gt; r = requests.post('http://httpbin.org/post', data = &#123;'key':'value'&#125;) 使用data可以传递字典作为参数，同时也可以传递元祖 12345678910111213&gt;&gt;&gt; payload = (('key1', 'value1'), ('key1', 'value2'))&gt;&gt;&gt; r = requests.post('http://httpbin.org/post', data=payload)&gt;&gt;&gt; print(r.text)&#123; ... \"form\": &#123; \"key1\": [ \"value1\", \"value2\" ] &#125;, ...&#125; 传递json是这样 123456&gt;&gt;&gt; import json&gt;&gt;&gt; url = 'https://api.github.com/some/endpoint'&gt;&gt;&gt; payload = &#123;'some': 'data'&#125;&gt;&gt;&gt; r = requests.post(url, data=json.dumps(payload)) 2.4.2 版的新加功能： 1234&gt;&gt;&gt; url = 'https://api.github.com/some/endpoint'&gt;&gt;&gt; payload = &#123;'some': 'data'&#125;&gt;&gt;&gt; r = requests.post(url, json=payload) 也就是说，你不需要对参数做什么变化，只需要关注使用data=还是json=，其余的requests都已经帮你做好了。 使用scrapy发送post请求官方推荐的 Using FormRequest to send data via HTTP POST 123return [FormRequest(url=\"http://www.example.com/post/action\", formdata=&#123;'name': 'John Doe', 'age': '27'&#125;, callback=self.after_post)] 这里使用的是FormRequest，并使用formdata传递参数，看到这里也是一个字典。 但是，超级坑的一点来了，今天折腾了一下午，使用这种方法发送请求，怎么发都会出问题，返回的数据一直都不是我想要的 1return scrapy.FormRequest(url, formdata=(payload)) 在网上找了很久，最终找到一种方法，使用scrapy.Request发送请求，就可以正常的获取数据。 1return scrapy.Request(url, body=json.dumps(payload), method='POST', headers=&#123;'Content-Type': 'application/json'&#125;,) 参考：Send Post Request in Scrapy 1234my_data = &#123;'field1': 'value1', 'field2': 'value2'&#125;request = scrapy.Request( url, method='POST', body=json.dumps(my_data), headers=&#123;'Content-Type':'application/json'&#125; ) FormRequest 与 Request 区别在文档中，几乎看不到差别， The FormRequest class adds a new argument to the constructor. The remaining arguments are the same as for the Request class and are not documented here. Parameters: formdata (dict or iterable of tuples) – is a dictionary (or iterable of (key, value) tuples) containing HTML Form data which will be url-encoded and assigned to the body of the request. 说FormRequest新增加了一个参数formdata，接受包含表单数据的字典或者可迭代的元组，并将其转化为请求的body。并且FormRequest是继承Request的 12345678910111213141516171819202122232425class FormRequest(Request): def __init__(self, *args, **kwargs): formdata = kwargs.pop('formdata', None) if formdata and kwargs.get('method') is None: kwargs['method'] = 'POST' super(FormRequest, self).__init__(*args, **kwargs) if formdata: items = formdata.items() if isinstance(formdata, dict) else formdata querystr = _urlencode(items, self.encoding) if self.method == 'POST': self.headers.setdefault(b'Content-Type', b'application/x-www-form-urlencoded') self._set_body(querystr) else: self._set_url(self.url + ('&amp;' if '?' in self.url else '?') + querystr) ###def _urlencode(seq, enc): values = [(to_bytes(k, enc), to_bytes(v, enc)) for k, vs in seq for v in (vs if is_listlike(vs) else [vs])] return urlencode(values, doseq=1) 最终我们传递的{‘key’: ‘value’, ‘k’: ‘v’}会被转化为’key=value&amp;k=v’ 并且默认的method是POST，再来看看Request 123456789class Request(object_ref): def __init__(self, url, callback=None, method='GET', headers=None, body=None, cookies=None, meta=None, encoding='utf-8', priority=0, dont_filter=False, errback=None, flags=None): self._encoding = encoding # this one has to be set first self.method = str(method).upper() 默认的方法是GET，其实并不影响。仍然可以发送post请求。这让我想起来requests中的request用法，这是定义请求的基础方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344def request(method, url, **kwargs): \"\"\"Constructs and sends a :class:`Request &lt;Request&gt;`. :param method: method for the new :class:`Request` object. :param url: URL for the new :class:`Request` object. :param params: (optional) Dictionary or bytes to be sent in the query string for the :class:`Request`. :param data: (optional) Dictionary or list of tuples ``[(key, value)]`` (will be form-encoded), bytes, or file-like object to send in the body of the :class:`Request`. :param json: (optional) json data to send in the body of the :class:`Request`. :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`. :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`. :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``&#123;'name': file-tuple&#125;``) for multipart encoding upload. ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')`` or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers to add for the file. :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth. :param timeout: (optional) How many seconds to wait for the server to send data before giving up, as a float, or a :ref:`(connect timeout, read timeout) &lt;timeouts&gt;` tuple. :type timeout: float or tuple :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``. :type allow_redirects: bool :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy. :param verify: (optional) Either a boolean, in which case it controls whether we verify the server's TLS certificate, or a string, in which case it must be a path to a CA bundle to use. Defaults to ``True``. :param stream: (optional) if ``False``, the response content will be immediately downloaded. :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair. :return: :class:`Response &lt;Response&gt;` object :rtype: requests.Response Usage:: &gt;&gt;&gt; import requests &gt;&gt;&gt; req = requests.request('GET', 'http://httpbin.org/get') &lt;Response [200]&gt; \"\"\" # By using the 'with' statement we are sure the session is closed, thus we # avoid leaving sockets open which can trigger a ResourceWarning in some # cases, and look like a memory leak in others. with sessions.Session() as session: return session.request(method=method, url=url, **kwargs)","tags":[{"name":"scrapy","slug":"scrapy","permalink":"https://zhangslob.github.io/tags/scrapy/"},{"name":"post","slug":"post","permalink":"https://zhangslob.github.io/tags/post/"}]},{"title":"淘宝sign加密算法","date":"2018-08-14T13:50:33.000Z","path":"2018/08/14/淘宝sign加密算法/","text":"1这是崔斯特的第六十二篇原创文章 淘宝sign加密算法 淘宝sign加密算法淘宝对于h5的访问采用了和客户端不同的方式，由于在h5的js代码中保存appsercret具有较高的风险，mtop采用了随机分配令牌的方式，为每个访问端分配一个token，保存在用户的cookie中，通过cookie带回服务端分配的token, 客户端利用分配的token对请求的URL参数生成摘要值sign, MTOP利用这个摘用值和cookie中的token来防止URL篡改。 流程 当本地cookie中的token为空时（通常是第一次访问），mtop会收到”FAIL_SYS_TOKEN_EXOIRED:: 令牌过期“这个错误应答，同时mtop会生成token写入cookie中（response.cookies）。 第二次请求时，js通过读取cookie中的token值，按照约定的算法生成sign, sign在mtop的请求中带上，mtop通过cookie中和token用同样的方式计算出sign,与请求的sign进行比较，检查通过将返回api的应答，失败提示“FAIL_SYS_ILLEGAL_ACCESS:: 非法请求” cookie中的token是有时效性的，遇到token失效时，将收到应答”FAIL_SYS_TOKEN_EXOIRED:: 令牌过期”, 同时会写入新的token,js利用新的token重新计算sign并重发请求。 关于cookie中的token的自我检查，由于token在cookie中是明文的，可能会被仿冒，在输出的cookie中包含一个用非对称密钥的公钥加密后的token, MTOP在每次请求时会先检查cookie中的token是否是由服务端分配出去的（利用加密后的token和私钥还原token，与回传的明文token比较） sign 生成关于sign的生成公式： md5Hex(token&amp;t&amp;appKey&amp;data) 如：md5Hex(“645d1f414d4914297dfaab40f3f76016 &amp;1234&amp;4272&amp;{“itemNumId”:”1500011132496”}”) sign=d2b2f818a03496b296b899a230c03abd token关于cookie的有效时长，cookie的有效时长为7天，但是token的有效时长目前为60分钟_m_h5_tk: 格式为 明文token_expireTime, 从response.cookies处获取，如： 2fcd2baa62fc60f73c0487a9f8a0a9d1_1362559577301 token就是2fcd2baa62fc60f73c0487a9f8a0a9d1 t很简单，即时间戳 int(time.time()*1000) appKey一般是固定数值 data一般是提交的参数 example1234567891011121314151617181920212223242526import requestsheaders = &#123; 'accept-encoding': 'gzip, deflate, br', 'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7', 'user-agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 10_3 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) CriOS/56.0.2924.75 Mobile/14E5239e Safari/602.1', 'accept': '*/*', 'referer': 'https://h5.m.taobao.com/?sprefer=sypc00', 'authority': 'h5api.m.taobao.com', 'cookie': 't=cff5759b3198bafb639030a7296d6bff; cna=OOz3EwDBHU8CAS9eVNkZGaaY; thw=cn; _m_h5_tk=4dab06478749cf71bcb31296c169e46f_1534260967070; _m_h5_tk_enc=eb5abdfc8a3e52d0f7982d2ab34eb471; isg=BH9_A4W7GMQHLxzbVJKP32QcDlqleywDp44sWxFMGy51IJ-iGTRjVv02ZvbeeKt-',&#125;params = ( ('jsv', '2.4.11'), ('appKey', '12574478'), ('t', '1534253767277'), ('sign', '12c6fac6a03cf98c8f912413feeaeaaf'), ('api', 'mtop.taobao.wireless.home.load'), ('v', '1.0'), ('type', 'jsonp'), ('dataType', 'jsonp'), ('callback', 'mtopjsonp2'), ('data', '&#123;\"containerId\":\"main\",\"ext\":\"&#123;\\\\\"h5_platform\\\\\":\\\\\"h5\\\\\",\\\\\"h5_ttid\\\\\":\\\\\"60000@taobao_h5_1.0.0\\\\\"&#125;\"&#125;'),)response = requests.get('https://h5api.m.taobao.com/h5/mtop.taobao.wireless.home.load/1.0/', headers=headers, params=params) 如何寻找看下面这段js 1234567891011121314151617# https://g.alicdn.com/hollywood/hollywood-lib/2.0.2/mtop.jsif (d.H5Request === !0) &#123; var f = \"//\" + (d.prefix ? d.prefix + \".\" : \"\") + (d.subDomain ? d.subDomain + \".\" : \"\") + d.mainDomain + \"/h5/\" + c.api.toLowerCase() + \"/\" + c.v.toLowerCase() + \"/\" , g = c.appKey || (\"waptest\" === d.subDomain ? \"4272\" : \"12574478\") , i = (new Date).getTime() , j = h(d.token + \"&amp;\" + i + \"&amp;\" + g + \"&amp;\" + c.data) , k = &#123; jsv: w, appKey: g, t: i, sign: j &#125; , l = &#123; data: c.data, ua: c.ua &#125;; 你可能会问，如何寻找，答案是js断点，一步步调试。 有兴趣的可以持续关注，以后会深入。 好久都没法东西了，主要是最近太忙，加班比较多；自己也变懒了，也在学一些新东西，就不知道写什么好，以后还是勤快点吧。","tags":[{"name":"淘宝","slug":"淘宝","permalink":"https://zhangslob.github.io/tags/淘宝/"},{"name":"加密","slug":"加密","permalink":"https://zhangslob.github.io/tags/加密/"}]},{"title":"Python多线程（一）","date":"2018-07-28T07:01:34.000Z","path":"2018/07/28/Python多线程（一）/","text":"1这是崔斯特的第六十一篇原创文章 记录自己对多线程的理解 (๑• . •๑) 线程与影分身多线程就是鸣人的影分身之术，可能不大恰当，但是非常容易理解。 平时一个鸣人会一个人，但是遇到强敌时，他会使用影分身之术，召唤自己的兄弟来帮助他一起来战斗。这样，他就从一个人战斗变成多个人战斗。 多线程也是这样，平时一个线程去做某件事，当使用多线程后，就会有很多个线程按照命令去做事，就像影分身一样。 Python使用鸣人和使用需要先结印 Python 中需要先调用，让我们看一个简单的例子： 123456789import threadingdef shadow(num): print('产生了第&#123;&#125;个分身...'.format(num))for i in range(1, 6): t = threading.Thread(target=shadow, args=(i,)) t.start() t.join() Output 12345产生了第1个分身...产生了第2个分身...产生了第3个分身...产生了第4个分身...产生了第5个分身... 可以看出用法非常简单，只需要先导入threading模块，然后实例化一个线程对象，注意args是元祖 t = threading.Thread(target=shadow, args=(i,)) 线程被创建之后并不会马上运行，需要手动调用 start() ， join() 让调用它的线程一直等待直到执行结束： 12t.start()t.join() 查看线程名字上述例子中，我们只知道产生了多个线程，但还是并不知道具体是哪个线程，如果某个线程出错了，我们没法去追踪，所以我们可以给线程加上名字。 123456789import threadingdef shadow(num): print('&#123;&#125;产生了第&#123;&#125;个分身...'.format(threading.currentThread().getName(), num))for i in range(1, 6): t = threading.Thread(target=shadow, args=(i,), name='第&#123;&#125;个鸣人'.format(i)) t.start() t.join() Out: 12345第1个鸣人产生了第1个分身...第2个鸣人产生了第2个分身...第3个鸣人产生了第3个分身...第4个鸣人产生了第4个分身...第5个鸣人产生了第5个分身... 当然，你可以这样 1234567891011import threadingdef shadow(num): print('&#123;&#125;产生了第&#123;&#125;个分身...'.format(threading.currentThread().getName(), num))for i in range(1, 6): t = threading.Thread(target=shadow, args=(i,)) t.start() t.join()print(threading.currentThread().getName()) Out 123456Thread-1产生了第1个分身...Thread-2产生了第2个分身...Thread-3产生了第3个分身...Thread-4产生了第4个分身...Thread-5产生了第5个分身...MainThread 你可以看到在所有创建的线程结束后，我们打印当前线程名字，就是主线程MainThread 什么时候使用join刚开始学多线程时，有个问题困扰了我很久，就是什么时候使用join() 缕一缕步骤： 主线程中创建子线程t 开始线程t.star() 调用t.join() 阻塞：等待子线程t结束后，再执行接下来的任务 也就是说使用了join()后会阻塞当前的线程，只有这个线程执行完毕后才会接着走。要是一直不结束呢？你可以加上超时时间，如t.join(timeout=10) 后续鸣人使用影分身后，产生了A、B、C三个小鸣人，鸣人可以使用join()方法，让B在A耗尽查克拉之后再进行攻击，如果没有使用join()，那么就会是这样的场景，大家一起上 这个时候如果鸣人命令A、B同时去写家庭作业，那么可能会产生什么情况呢？A先写还是B先写，大家可以想想。 下一期会说说 锁","tags":[{"name":"多线程","slug":"多线程","permalink":"https://zhangslob.github.io/tags/多线程/"}]},{"title":"为什么Selenium点不到元素","date":"2018-07-24T14:35:10.000Z","path":"2018/07/24/为什么Selenium点不到元素/","text":"这是崔斯特的第六十篇原创文章 明明可以显示，为什么就是点不到呢 (๑• . •๑) 最近做了许多登陆项目，我会优先选择使用requests来模拟请求，但是有些参数实在是很难获取，这个时候我会使用Selenium，也还是遇到了各种坑，也算是见识到了很多的验证措施。 今天说说如何解决selenium点选不到数据的问题。 等待这还是最常见的一种情况，推荐最多的是使用显示等待： 12345678910111213from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECdriver = webdriver.Firefox()driver.get(\"http://somedomain/url_that_delay_loading\")try: element = WebDriverWait(driver,10).until( EC.presence_of_element_located((By.ID,\"myDynamicElement\")) )finally: driver.quit() 这段代码会等待10秒，如果10秒内找到元素则立即返回，否则会抛出TimeoutException异常。 但是我比较懒，因为time.sleep()可以达到同样效果。 鼠标事件官方把它叫做“行为链”。ActionChains可以完成简单的交互行为，例如鼠标移动，鼠标点击事件，键盘输入，以及内容菜单交互。 123456789101112131415161718192021222324252627282930click(on_element=None) ——单击鼠标左键click_and_hold(on_element=None) ——点击鼠标左键，不松开context_click(on_element=None) ——点击鼠标右键double_click(on_element=None) ——双击鼠标左键drag_and_drop(source, target) ——拖拽到某个元素然后松开drag_and_drop_by_offset(source, xoffset, yoffset) ——拖拽到某个坐标然后松开key_down(value, element=None) ——按下某个键盘上的键key_up(value, element=None) ——松开某个键move_by_offset(xoffset, yoffset) ——鼠标从当前位置移动到某个坐标move_to_element(to_element) ——鼠标移动到某个元素move_to_element_with_offset(to_element, xoffset, yoffset) ——移动到距某个元素（左上角坐标）多少距离的位置perform() ——执行链中的所有动作release(on_element=None) ——在某个元素位置松开鼠标左键send_keys(*keys_to_send) ——发送某个键到当前焦点的元素send_keys_to_element(element, *keys_to_send) ——发送某个键到指定元素 深入了解可以参考 https://blog.csdn.net/huilan_same/article/details/52305176 move_to_element_with_offset和click_and_hold会经常用到破解验证码中。 触摸操作 (TouchAction)该事件仅仅针对移动端、触屏版 12345678910111213flick_element(on_element, xoffset, yoffset, speed) # 以元素为起点以一定速度向下滑动scroll_from_element(on_element xoffset yoffset) #以元素为起点向下滑动double_tap(on_element) #双击 flick_element(on_element, xoffset, yoffset, speed) #从元素开始以指定的速度移动long_press(on_element) #长按不释放move(xcoord, ycoord) #移动到指定的位置perform() #执行链中的所有动作release(xcoord, ycoord) #在某个位置松开操作scroll(xoffset, yoffset) #滚动到某个位置scroll_from_element(on_element, xoffset, yoffset) #从某元素开始滚动到某个位置tap(on_element) #单击tap_and_hold(xcoord, ycoord) #某点按住 为什么要说到移动端，在做登陆时，移动端往往会更加简单，但是触屏版的点击和PC端时完全不同的，点击与按住时不同的。 在某个项目我换成TouchAction后，神奇的发现，注册不再需要处理验证码了，真是太棒了。 使用js当你使用浏览器已经找到该元素，使用click()方法但是不起作用时，这个时候建议尝试js，例如在我的主页 https://www.zhihu.com/people/cuishite/activities，点击 “查看详细资料” 12js = 'document.getElementsByClassName(\"Button ProfileHeader-expandButton Button--plain\")[0].click();'driver.execute_script(js) 你可以先在控制台调试 js通常可以解决绝大多是问题，如果还是解决不了，那你可能和我遇到了同样的问题，比如说，我在处理某移动端网站登陆，处理如下验证码时，我会使用到move_to_element_with_offset，该方法是“移动到距某个元素（左上角坐标）多少距离的位置”。 计算出坐标后，会调用该方法，如action.move_to_element_with_offset(element, width, height).click().perform()，然而实际上问题并没有这么简单，多次点击失效。具体的有时间再说。 实用方法提取selenium的cookies 介绍把selenium的cookies船体给requests使用的方法： 12345cookies = driver.get_cookies()s = requests.Session()for cookie in cookies: s.cookies.set(cookie['name'], cookie['value']) How do I load session and cookies from Selenium browser to requests library in Python? 元素截图方法 1234567891011121314151617181920212223from selenium import webdriverfrom PIL import Imagefox = webdriver.Firefox()fox.get('https://stackoverflow.com/')# now that we have the preliminary stuff out of the way time to get that image :Delement = fox.find_element_by_id('hlogo') # find part of the page you want image oflocation = element.locationsize = element.sizefox.save_screenshot('screenshot.png') # saves screenshot of entire pagefox.quit()im = Image.open('screenshot.png') # uses PIL library to open image in memoryleft = location['x']top = location['y']right = location['x'] + size['width']bottom = location['y'] + size['height']im = im.crop((left, top, right, bottom)) # defines crop pointsim.save('screenshot.png') # saves new cropped image selenium cannot screenshot a web element 最后推荐一个神器 appium/python-client 至于验证码部分，现在主要还是靠第三方工具，并没有自己尝试机器学习等方法处理。","tags":[{"name":"Selenium","slug":"Selenium","permalink":"https://zhangslob.github.io/tags/Selenium/"}]},{"title":"使用Selenium与Requests模拟登陆","date":"2018-07-17T13:18:35.000Z","path":"2018/07/17/使用Selenium与Requests模拟登陆/","text":"这是崔斯特的第五十九篇原创文章 模拟登陆的两种方式，你喜欢哪种 (๑• . •๑) 本期讲一讲模拟登录相关的东西，目标网站是Github 简单的Selnium想说说简单的方法，使用浏览器登录，基本上就是傻瓜操作了。 如上图所示，登录设计的很简单，没有验证码什么的，代码如下： 12345678910111213141516171819202122#!/usr/bin/env python# -*- coding: utf-8 -*-import timefrom selenium import webdriverdriver = webdriver.Chrome()driver.maximize_window()def login(account, password): driver.get('https://github.com/login') time.sleep(2) driver.find_element_by_id('login_field').send_keys(account) driver.find_element_by_id('password').send_keys(password) driver.find_element_by_xpath('//input[@class=\"btn btn-primary btn-block\"]').click() # do whatever you wantif __name__ == '__main__': account, password = 'account', 'password' login(account, password) 分析请求之Requests打开F12，使用错误的账号密码登录，复制curl 1curl 'https://github.com/session' -H 'Cookie: has_recent_activity=1; _octo=GH1.1.1477592343.1531820067; logged_in=no; _gh_sess=UEZzYnVCMVlhNkVOdE5rU1hWRFpDbmFlY0UyQ1Y2b3Z4TGw2NFlTMmJLUWk5VENVQ3Q4TWxiSWN5ckEyZXN0MUFkT29XVjQvbWJVbm9RV0JNQmc1TmU0UnBtK0taUXJpcElqUk5PNGZ5TjZOQ2ZPRVR4NU5WQXcrb2xWRnRBMnRPMkRWYzYvWmVGY0FrYU12Q3BVVTY3dXVSblliNG4rWjc2QXVwR2pjQ1pzZXM1MFk1MjU5OUw2WkFLTU1BMzJDWGlTeXliNzNaejlUaW43cWhFNzQ0MFFVVmJ1aEppbzdtQTZkRERmUm5mWExkRDlmWW5lNk9mdlFYb05MQUtubDZBbXFJWjV6eFhic3JiWlRtZ2QxZ2FqZUxnOGFheUgzaXJmc290b0Jma09pRTJZdHZySEVmdVdGZHVBU3ZTVTJRM0pESnE1N1VPRDM0ck9FZzNJZTN5VWljUktyZ3FZQU16THVBeFBXV3BNPS0tSDh4WVV6U2RSNjlBL3FNQ3VaRGxEUT09--71cf0886128d55b42c82cf6f7b76e007ebfdc77b; _ga=GA1.2.57857743.1531820085; _gat=1; tz=Asia%2FShanghai' -H 'Origin: https://github.com' -H 'Accept-Encoding: gzip, deflate, br' -H 'Accept-Language: zh-CN,zh;q=0.9,en;q=0.8' -H 'Upgrade-Insecure-Requests: 1' -H 'User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36' -H 'Content-Type: application/x-www-form-urlencoded' -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8' -H 'Cache-Control: max-age=0' -H 'Referer: https://github.com/login' -H 'Connection: keep-alive' --data 'commit=Sign+in&amp;utf8=%E2%9C%93&amp;authenticity_token=%2BtgUHwMIxnoHOHNMqQFkLak9mJzrxt%2B4yfFiZaf66WiMB5ZyRaVXq%2BFpZsM%2BtxgaRRX6Fzfezu1IdRqy%2BTGHwg%3D%3D&amp;login=123456788%40gmail.com&amp;password=123456788' --compressed 转为Python代码， 12345678910111213141516171819202122232425262728293031323334import requestscookies = &#123; 'has_recent_activity': '1', '_octo': 'GH1.1.1477592343.1531820067', 'logged_in': 'no', '_gh_sess': 'UEZzYnVCMVlhNkVOdE5rU1hWRFpDbmFlY0UyQ1Y2b3Z4TGw2NFlTMmJLUWk5VENVQ3Q4TWxiSWN5ckEyZXN0MUFkT29XVjQvbWJVbm9RV0JNQmc1TmU0UnBtK0taUXJpcElqUk5PNGZ5TjZOQ2ZPRVR4NU5WQXcrb2xWRnRBMnRPMkRWYzYvWmVGY0FrYU12Q3BVVTY3dXVSblliNG4rWjc2QXVwR2pjQ1pzZXM1MFk1MjU5OUw2WkFLTU1BMzJDWGlTeXliNzNaejlUaW43cWhFNzQ0MFFVVmJ1aEppbzdtQTZkRERmUm5mWExkRDlmWW5lNk9mdlFYb05MQUtubDZBbXFJWjV6eFhic3JiWlRtZ2QxZ2FqZUxnOGFheUgzaXJmc290b0Jma09pRTJZdHZySEVmdVdGZHVBU3ZTVTJRM0pESnE1N1VPRDM0ck9FZzNJZTN5VWljUktyZ3FZQU16THVBeFBXV3BNPS0tSDh4WVV6U2RSNjlBL3FNQ3VaRGxEUT09--71cf0886128d55b42c82cf6f7b76e007ebfdc77b', '_ga': 'GA1.2.57857743.1531820085', '_gat': '1', 'tz': 'Asia%2FShanghai',&#125;headers = &#123; 'Origin': 'https://github.com', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8', 'Upgrade-Insecure-Requests': '1', 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36', 'Content-Type': 'application/x-www-form-urlencoded', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Cache-Control': 'max-age=0', 'Referer': 'https://github.com/login', 'Connection': 'keep-alive',&#125;data = [ ('commit', 'Sign in'), ('utf8', '\\u2713'), ('authenticity_token', '+tgUHwMIxnoHOHNMqQFkLak9mJzrxt+4yfFiZaf66WiMB5ZyRaVXq+FpZsM+txgaRRX6Fzfezu1IdRqy+TGHwg=='), ('login', '123456788@gmail.com'), ('password', '123456788'),]response = requests.post('https://github.com/session', headers=headers, cookies=cookies, data=data) 注意两个地方，cookies和参数，先来看看参数，稍微特别的就是authenticity_token，感觉是验证。Ctrl+Shift+F打开搜索，最终在返回的html中找到 1&lt;!-- '\"` --&gt;&lt;!-- &lt;/textarea&gt;&lt;/xmp&gt; --&gt;&lt;/option&gt;&lt;/form&gt;&lt;form action=\"/session\" accept-charset=\"UTF-8\" method=\"post\"&gt;&lt;input name=\"utf8\" type=\"hidden\" value=\"&amp;#x2713;\" /&gt;&lt;input type=\"hidden\" name=\"authenticity_token\" value=\"CTujn/pHGMQBpEhYcJj9Mn6ChsNSkd5ul8rgNSP/6/KxdZlhS0ABKblsq1pLn6EaQvIGLMzl/IQawaDL8KFjDw==\" /&gt; &lt;div class=\"auth-form-header p-0\"&gt; authenticity_token解决了，下一步想办法获取cookies 继续搜索_gh_sess与_octo关键字，看到有这样一段js 1var e, t = void 0, r = void 0, n = this._getCookie(\"_octo\"), a = []; 猜测cookies不是本地生成，查看打开Github首页的请求，果然在Response Cookies中找到了相关数据，那么使用Session就可以维持会话了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/usr/bin/env python# -*- coding: utf-8 -*-import reimport requestsheaders = &#123; 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8', 'Connection': 'keep-alive', 'Host': 'github.com', 'Upgrade-Insecure-Requests': '1', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'&#125;s = requests.session()s.headers.update(headers)def get_token(): url = 'https://github.com/login' response = s.get(url) pat = 'name=\\\"authenticity_token\\\" value=\\\"(.*?)\\\"' authenticity_token = re.findall(pat, response.text)[0] return authenticity_tokendef login(authenticity_token, account, password): payload = &#123; 'commit': 'Sign in', 'utf8': '\\u2713', 'authenticity_token': authenticity_token, 'login': account, 'password': password, &#125; url = 'https://github.com/session' response = s.post(url, data=payload) print(response) # do whatever you wantif __name__ == '__main__': account, password = 'account', 'password' authenticity_token = get_token() login(authenticity_token, account, password) 对比Selenium： 优点：简单、无脑，不用分析复杂的网页请求，不用保持会话状态 缺点：速度慢，速度慢，速度慢（某些情况下会出现js加载不全） Requests： 优点：速度快，可以增加自己对cookies登陆的理解 缺点：需要花时间寻找相关参数 如果对Github感兴趣，可以直接使用 Github API 最近在使用Selenium处理验证码，发现很强大，如果模拟请求，难度会非常大。","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"Selenium","slug":"Selenium","permalink":"https://zhangslob.github.io/tags/Selenium/"},{"name":"Requests","slug":"Requests","permalink":"https://zhangslob.github.io/tags/Requests/"}]},{"title":"说说最近遇到的反爬","date":"2018-07-10T15:07:44.000Z","path":"2018/07/10/说说最近遇到的反爬/","text":"这是崔斯特的第五十八篇原创文章 爬虫与反爬 (๑• . •๑) 参数加密参数加密指的是在请求中需要加上类似token、uuid 字段，例如在某个请求中query string parameters中有_token和uuid、customerKey等字段， 123_token:eJyN0l9L40AQAPDvsg8+hWZ2uuid:59851b5e-92b4-f1f5-19e2-d8148bf7ecustomerKey:0356982437 解决方法 刚开始遇到这个是一脸懵逼的，验证发现有些参数不是必须的，比如uuid，uuid在维基百科上是：通用唯一识别码，估计没啥作用，python也有内置的uuid生成库 uuid — UUID objects according to RFC 4122 问了前端大佬，得知需要在js中打断点，一点点调试，最后终于解决，_token是二次加密的。 给大家看一下该网站的部分代码。看看描述：获得反爬虫的_toke 123456789101112131415161718192021222324252627/** * 获得反爬虫的_token * @param &#123;*&#125; url * @param &#123;*&#125; queryParams */function getRohrToken(host, queryParams) &#123; if (window.Rohr_Opt &amp;&amp; window.Rohr_Opt.reload) &#123; var rohr = window.Rohr_Opt; var _token = ''; var params = queryParams if (typeof params == 'string' &amp;&amp; params.indexOf('_fb_=') !== -1) &#123; params = '' &#125; var queryString = toQueryString(params); var _url = location.protocol + host + '?' + queryString; try &#123; _token = rohr.reload(_url) || ''; &#125; catch (e) &#123; console.log('获取token失败:' + e) &#125; return _token; &#125;;&#125; 关于如何调试，这里有教程，调试 JavaScript 脚本 这里还有另一种app抓包加密情况，暂时还没接触过，听过是需要反编译apk包，然后阅读代码。 登录问题很多网站数据是登录可见，那么就必须要开发该网站的登录系统了。 登录可能会遇到的一些问题： 登录过程中遇到的验证码（下面会说） cookies持久化问题 账号被封禁问题 解决方法 登录账号获得cookies后，经过一段时间，cookies就可能会失效，具体网站情况不同，这时候就必须有个脚本，来保证cookies有效 账号做出一些跟正常用户不同的操作就会产生异常，别人很容易就发现。所以就让你的账号像正常人一样。 最近看到有人再问豆瓣登录采集影评导致被封号的事情，豆瓣我以前也被封过，到现在也没有解封 12依据用户管理细则，此帐号已被永久停用。停用时间:2017-09-29如有疑问，请发送邮件到help@douban.com 我的建议是： 有能力的多注册账号，账号被封了就再去注册呗 手机app抓包，app不需要登录，而且可以持续抓最新评论 图形验证码验证码一直是反爬虫利器，从简单的数字识别，到复杂的滑动拼图、图片点选等等。有兴趣的来试试破解 网易云易盾，感觉很头疼。 解决方法 例如上图，这是我现在遇到的一种验证码情况，依次点击几个文字。 如果是自己来做的话，会考虑这样： 将验证码图片部分截图或下载回来 对图片进行OCR，提取文字 文字识别，获取坐标 使用selenium根据坐标点击 这只是初步思路，但想法很容易，做起来却没那么简单。 在Github上找到大佬写的方法 captcha_crack，知乎上也有 使用深度学习破解点击验证码 那么如果直接接入第三方打码平台来，那就会简单很多，在实际开发中为了提高准确性，更多会使用打码平台。","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"反爬","slug":"反爬","permalink":"https://zhangslob.github.io/tags/反爬/"}]},{"title":"快速写一个爬虫","date":"2018-07-03T15:20:52.000Z","path":"2018/07/03/快速写一个爬虫/","text":"这是崔斯特的第五十七篇原创文章 快、更快 (๑• . •๑) 缘来今天下班前，老板让我帮忙爬一个数据，简单看了下，需要登录，看起来应该不难。回到家，注册一个账号，复制url，然后用postman转代码，简单暴力，直接撸。 这里说下postman的一个BUG，发送请求不会获得任何数据，如果你遇到，建议升级postman为最新版本 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#!/usr/bin/env python# -*- coding: utf-8 -*-import loggingimport pymongoimport requestsimport tracebackfrom concurrent import futuresfrom urllib.request import urljoinfrom scrapy.selector import Selectorlogging.basicConfig(level=logging.INFO)client = pymongo.MongoClient()coll = client['table']['collection']# coll.create_index('url', unique=True)url = \"your urls\"headers = &#123; 'user-agent': \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 \" \"(KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36\", 'accept': \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\", 'accept-encoding': \"gzip, deflate, br\", 'accept-language': \"zh-CN,zh;q=0.9,en;q=0.8\", &#125;def fetch(page): try: proxies = &#123; 'http': proxy, 'https': proxy &#125; response = requests.get(url.format(page), headers=headers, timeout=20, proxies=proxies) if response.status_code == 200: s = Selector(text=response.text) for i in s.xpath('//*[@id=\"search-results\"]/tbody/tr'): url_ = i.xpath('td[4]/a/@href').extract_first() detail_url = urljoin(url, url_) data = get_detail(detail_url) logging.info('success save data &#123;&#125; '.format(data['url'])) save_mongo(data) except Exception as e: logging.error(traceback.format_exc())def get_detail(detail_url): try: proxies = &#123; 'http': proxy, 'https': proxy &#125; response = requests.get(detail_url, headers=headers, timeout=20, proxies=proxies) if response.status_code == 200: s = Selector(text=response.text) data = dict() data['url'] = detail_url data['SMILES '] = s.xpath('//*[@id=\"smiles\"]/text()').extract_first() img = s.xpath('//*[@id=\"molecule-image\"]/img/@src').extract_first() data['img'] = urljoin(detail_url, img) data['formula'] = s.xpath('//*[@id=\"name-structure\"]/tbody/tr[2]/td[2]/text()').extract_first() data['Mass'] = s.xpath('//*[@id=\"name-structure\"]/tbody/tr[3]/td[2]/text()').extract_first() return data except Exception as e: logging.error(traceback.format_exc())def save_mongo(data): try: coll.insert(data) except pymongo.errors.DuplicateKeyError: passif __name__ == '__main__': # for i in range(1, 11): # fetch(str(i)) # if use Thread with futures.ThreadPoolExecutor(max_workers=50) as executor: to_do = [] for i in range(1, 51): future = executor.submit(fetch, str(i)) to_do.append(future) 代码相当简单，fetch函数用来抓取列表页，get_detail函数抓取详情页，save_mongo保存数据库，需要说明下的就是最后使用的多线程了，这里号使用的是futures，并不想说很多大道理，来看看文档 提高速度concurrent.futures 是python3新增加的一个库，用于并发处理，提供了多线程和多进程的并发功能 线程池 1234567891011121314151617181920212223242526import concurrent.futuresimport urllib.requestURLS = ['http://www.foxnews.com/', 'http://www.cnn.com/', 'http://europe.wsj.com/', 'http://www.bbc.co.uk/', 'http://some-made-up-domain.com/']# Retrieve a single page and report the URL and contentsdef load_url(url, timeout): with urllib.request.urlopen(url, timeout=timeout) as conn: return conn.read()# We can use a with statement to ensure threads are cleaned up promptlywith concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor: # Start the load operations and mark each future with its URL future_to_url = &#123;executor.submit(load_url, url, 60): url for url in URLS&#125; for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: data = future.result() except Exception as exc: print('%r generated an exception: %s' % (url, exc)) else: print('%r page is %d bytes' % (url, len(data))) 进程池 12345678910111213141516171819202122232425262728import concurrent.futuresimport mathPRIMES = [ 112272535095293, 112582705942171, 112272535095293, 115280095190773, 115797848077099, 1099726899285419]def is_prime(n): if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return Truedef main(): with concurrent.futures.ProcessPoolExecutor() as executor: for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)): print('%d is prime: %s' % (number, prime))if __name__ == '__main__': main() 关于入库方面，建议是增加唯一索引， coll.create_index(&#39;url&#39;, unique=True)，一个是去重，一个是提高查询速度。","tags":[]},{"title":"Katalon + 傻瓜 == selenium","date":"2018-06-25T13:58:25.000Z","path":"2018/06/25/Katalon-selenium-傻瓜/","text":"这是崔斯特的第五十六篇原创文章 简直是神器啊 (๑• . •๑) 今天在翻莫烦大大的博客时，看到他提到一个工具，便去看了下，第一感受是，太好用了、爱不释手。 下面来说说这个工具：Katalon Recorder Katalon Recorder安装地址：Katalon Recorder 官方介绍是： Best Selenium IDE record, play, debug app. Exports Selenium WebDriver code. Provides reports, logs, screenshots. Fast &amp; extensible. 简单来说，他可以记录你在浏览器上的每一个动作，包括、点击、输入、输入字符等等，最后一键转化为编程代码，可以转化的语言有： C# JAVA Katalon Studio Python2 Roboot Framework Ruby XML 例如下图就是直接转化为 Python2的代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# -*- coding: utf-8 -*-from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.support.ui import Selectfrom selenium.common.exceptions import NoSuchElementExceptionfrom selenium.common.exceptions import NoAlertPresentExceptionimport unittest, time, reclass UntitledTestCase(unittest.TestCase): def setUp(self): self.driver = webdriver.Firefox() self.driver.implicitly_wait(30) self.base_url = \"https://www.katalon.com/\" self.verificationErrors = [] self.accept_next_alert = True def test_untitled_test_case(self): driver = self.driver driver.get(\"https://zhangslob.github.io/\") driver.find_element_by_link_text(u\"Cookies池的后续解决方案\").click() driver.find_element_by_xpath(\"//main[@id='main']/div/div\").click() def is_element_present(self, how, what): try: self.driver.find_element(by=how, value=what) except NoSuchElementException as e: return False return True def is_alert_present(self): try: self.driver.switch_to_alert() except NoAlertPresentException as e: return False return True def close_alert_and_get_its_text(self): try: alert = self.driver.switch_to_alert() alert_text = alert.text if self.accept_next_alert: alert.accept() else: alert.dismiss() return alert_text finally: self.accept_next_alert = True def tearDown(self): self.driver.quit() self.assertEqual([], self.verificationErrors)if __name__ == \"__main__\": unittest.main() 虽然说用的是Python2，但是并不影响，简单改一下就就可以使用了，主要逻辑在test_untitled_test_case函数中，可以直接拿来使用。 注意这里使用了unittest，不熟悉的可以来看看文档 1234567891011121314151617181920import unittestclass TestStringMethods(unittest.TestCase): def test_upper(self): self.assertEqual('foo'.upper(), 'FOO') def test_isupper(self): self.assertTrue('FOO'.isupper()) self.assertFalse('Foo'.isupper()) def test_split(self): s = 'hello world' self.assertEqual(s.split(), ['hello', 'world']) # check that s.split fails when the separator is not a string with self.assertRaises(TypeError): s.split(2)if __name__ == '__main__': unittest.main() 安装方法这里建议直接在Chrome应用市场安装，地址 Katalon Recorder 考虑到有些同学可能那啥，所以我已经下载好了。公众号：Python爬虫与算法进阶，回复：傻瓜 感受这个相当于按键精灵，把我们对浏览器的每一步操作都完成了，可以用来生成代码，和postman一样，所以我才会说 Katalon + 傻瓜 == selenium，完全是傻瓜操作。 比如来做一些自动化登录、注册等板块会非常爽，但是验证码部分还是需要自行解决。 但是该软件也是有一些问题的，它不能进行多页面切换，也就是不能自动切换到新打开的窗口，会有错误提示# ERROR: Caught exception [ERROR: Unsupported command [selectWindow | win_ser_1 | ]]，这一步必须自己手动来操作， 1234driver.current_window_handle 获取当前窗口handledriver.window_handles 获取所有窗口的handle，返回list列表driver.switch_to.window(handle) 切换到对应的窗口driver.close() 关闭当前窗口 测试打开多窗口的代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# -*- coding: utf-8 -*-from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.support.ui import Selectfrom selenium.common.exceptions import NoSuchElementExceptionfrom selenium.common.exceptions import NoAlertPresentExceptionimport unittest, time, reclass Zhihu(unittest.TestCase): def setUp(self): self.driver = webdriver.Firefox() self.driver.implicitly_wait(30) self.base_url = \"https://www.katalon.com/\" self.verificationErrors = [] self.accept_next_alert = True def test_zhihu(self): driver = self.driver driver.get(\"https://www.zhihu.com/people/cuishite/activities\") driver.find_element_by_xpath(\"//div[@id='ProfileMain']/div/ul/li[2]/a\").click() driver.find_element_by_xpath(\"//div[@id='ProfileMain']/div/ul/li[4]/a/span\").click() driver.find_element_by_link_text(u\"Cookies池的后续解决方案\").click() # ERROR: Caught exception [ERROR: Unsupported command [selectWindow | win_ser_1 | ]] driver.find_element_by_xpath(\"//img[contains(@src,'https://pic4.zhimg.com/v2-7ff26e52e6c82c080f62d8e9291e532b_b.jpg')]\").click() driver.find_element_by_link_text(u\"Cookies池的后续解决方案\").click() # ERROR: Caught exception [ERROR: Unsupported command [selectWindow | win_ser_2 | ]] driver.find_element_by_xpath(\"//div[@id='js_article']/div[2]\").click() def is_element_present(self, how, what): try: self.driver.find_element(by=how, value=what) except NoSuchElementException as e: return False return True def is_alert_present(self): try: self.driver.switch_to_alert() except NoAlertPresentException as e: return False return True def close_alert_and_get_its_text(self): try: alert = self.driver.switch_to_alert() alert_text = alert.text if self.accept_next_alert: alert.accept() else: alert.dismiss() return alert_text finally: self.accept_next_alert = True def tearDown(self): self.driver.quit() self.assertEqual([], self.verificationErrors)if __name__ == \"__main__\": unittest.main() 所以只需要这一点就可以完善代码，同时可以看看他们的官网 KATALON - Best automated testing tool for web, mobile, API，他们主要是提供测试工具，感兴趣的可以了解下。","tags":[]},{"title":"Cookies池的后续解决方案","date":"2018-06-16T07:01:54.000Z","path":"2018/06/16/Cookies池的后续解决方案/","text":"这是崔斯特的第五十五篇原创文章 终于有方法了 (๑• . •๑) 作为一名罗吹,先吹一波C罗牛皮,简直太帅了了了 瞧瞧这个眼神建议大家可以看看这场比赛，西班牙VS葡萄牙，我相信你会爱上足球的 正文开始在上一篇文章: 简单说明了自己对于该网站的想法,在经过两天的测试与研究之后,我有了更多的想法. 想法一: 为每个Cookies绑定唯一IP你作为一个普通用户,如果对方网站检测到你不停地变换IP从不同地方来发送请求,那他肯定会有识别.那么我们可以把每一个Cookies分配唯一的IP代理,也就是你这个Cookies发送请求的代理始终是唯一的. 但是这种方法的实现方式还没有想出来,不知道该使用哪些技术栈来实现这个想法 想法二: 解决验证码好,既然你弹出验证码,那我就解决它! 解决方法是: 该网站的验证码图片是base64,用python转一下,接上第三方就OK 同时该网站参数中还带有token参数,经过前端大佬的断点调试,发现是对多个字段的两次加密 保持会话session.你需要让对方网站知道是你这个用户,所以你的cookies,ip,headers相关信息必须保持一致,我当时的想法是使用requests的session来完成这一步操作,但是很难完全模拟,因为整个爬虫使用scrapy来写,处理验证码使用requests来做,session这部分不大好模拟. 附上一些为session添加信息的代码 12345def __init__(self, my_cookie, proxy, headers): self.session = requests.session() self.session.cookies.update(my_cookie) self.session.headers.update(&#123;'User-Agent': headers&#125;) self.session.update(proxy) 想法三: 不解决验证问题,无脑重试这是最蠢的办法,也是我目前在使用的方法. /(ㄒoㄒ)/~~ 思路是: 将cookies保存到mongoDB做持久化,再写一个脚本,持续的向redis中添加cookies和start_urls,然后对这些start_urls不停地重试,对,就是不停地重试,直到把所有的链接跑完.本次需要采集的链接并不多,质量要求不高,只需要我采集到数据就好. 需要注意的是: 需要为你的表增加唯一键,不然会有重复数据 每条请求添加dont_filter=True,不然去重会影响爬取 添加cookies和start_urls代码 12345678910111213# add cookies and start_urls to local_rediswhile True: if local_redis.scard('spider:cookies') &lt; 10: for i in cookies_list: local_redis.sadd('spider:cookies', i) print(\"cookies Done\") if local_redis.scard('spider:start_urls') &lt; 10: for i in url_list: local_redis.sadd('spider:start_urls', i) print(\"start_urls Done\") time.sleep(10) 效果我是用的是第三种方法,目前看来效果还行,数据已经爬取了大半,相信三天假过去了应该就没问题了. 如果以后要长期做这个项目的话,最好的办法应该还是第二种,从根本去解决问题. 本次做这个项目也收获颇多,对于cookies使用更加有经验;感觉最好玩的是token的加密与解密实现,对这方面感兴趣的可以了解下这个: zlib — Compression compatible with gzip","tags":[{"name":"cookie池","slug":"cookie池","permalink":"https://zhangslob.github.io/tags/cookie池/"},{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"}]},{"title":"从cookie池搭建说起","date":"2018-06-14T13:25:46.000Z","path":"2018/06/14/从cookie池搭建说起/","text":"这是崔斯特的第五十四篇原创文章 我快绝望了 (๑• . •๑) 这几天接手了一个很急的项目，要在几天爬取某网站的数据。该站是我知道国内反爬比较严重的网站之一，我也做好了心理准备。 分析该网站数据需要登录才能查看，APP抓包了但是没有发现相关数据，所以选择从PC站入手。 既然需要登录，那就需要验证一个新鲜的cookies可以访问多少链接；验证方法是： 直接拷贝已经登录该网站请求的Curl，转换为Python代码，加一个循环，测试，单个账号可以跑多少页 经过10多次测试，发现单个Cookies可以下载，至少50个网页。 那么就很好做了，可以开始写思路了。 思路首先我们需要多个可以登录的Cookies，然后利用这些Cookies去下载网页；一旦返回状态码不是200，就拉黑该Cookies。 具体的方法看下图： 有几点说明下： 如何模拟注册。有两种方法，模拟请求和浏览器模拟。模拟请求就是去分析注册过程中的每一步操作，这个请求是发送验证码的，那个是注册的，我优先推荐该方法，但是这种方法遇到一些携带有大量签名参数的变态网站时，难度较大，需要自己一步步断点JavaScript。浏览器模拟大家应该很熟悉，Python中就是selenium傻瓜操作，需要哪里点哪里。注意的坑是何时切换iframe Cookies的搭建。其实非常简单，这里利用了redis的集合，取Cookies使用spop即可。这里并没有做验活，因为基本上Cookies产生之后就会被使用。 我要崩溃了不做不知道，一做吓一跳。 但我以为该网站很简单，搭建一个简单的Cookies池就可以解决，但是我明显太年轻了。 我先把采集链接推到redis中，使用了scrapy_redis。 该网站的反爬： 代理问题。其实这里有一个悖论，到底该不该使用代理。首先，我不用代理，刚开始还好，但是很快我的本地代理就被拉黑了；那好上代理，但是接下来问题来了，开始出现一个个的验证码需要填了，作为一个单独的用户，我是不可能频繁的改变自己的IP去访问网站的，解决方法只有一个，Cookies和代理绑定，但是这种方法真的不好实现，尤其是使用Scrapy开发的爬虫。 验证码问题。既然出现了验证码，那就去解决它。验证码一般是和Cookies绑定的，那么我需要把访问该账号的IP、User-Agent、Cookies全部拿出来，再去发送新的请求，而且需要注意，此过程中不能再进行IP的变化。 最后今天了大概两天的斗智斗勇，现在基本上可以爬到数据了，希望明天可以交差，不然端午就要加班了。 希望这只小猫咪可以给我带来好远。","tags":[{"name":"cookie池","slug":"cookie池","permalink":"https://zhangslob.github.io/tags/cookie池/"},{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"}]},{"title":"为了知道胡歌粉丝的男女比率，爬了三百万微博数据","date":"2018-06-07T13:59:53.000Z","path":"2018/06/07/胡歌男粉多还是女粉多-爬爬微博/","text":"这是崔斯特的第五十三篇原创文章 老胡好帅 (๑• . •๑) 最近偶然间看到一条新闻，标题是：“胡歌作为一个男性明星，男粉丝比女粉丝还多，这不科学！” 文中这样说道“胡歌在微博上的粉丝就已经达到了5748.9398万人，并且通过查看粉丝可以发现许多都是男性粉丝，这不得不说这是独一无二了” 当时我就震惊了，“通过查看粉丝”？？？这是什么操作，现在的UC小编越来越多了吗？ 我作为一名老胡的粉丝，简直是不能忍，这完全是在瞎写啊。 所以我有个想法，把胡歌微博上六千万粉丝数据爬取下来，看看到底男粉丝多还是女粉丝多。 大家可以在自己心中猜测一个答案，到底男粉多还是女粉多呢～～。我的答案是男性比较多。 分析问题 这里可以看到胡歌微博粉丝总数约6千万，本次我的目标就是尽力去找到胡歌活跃粉丝的男女比例。 但是我们知道微博是有限制的，微博不会把所有数据都展示出来，如图 那么问题来了，我要怎样才能尽可能多的抓到粉丝数据？ 这里我就想要尽可能多的抓取到老胡的活跃粉丝， 所谓活跃粉丝，指的是除去“不转发、不评论、不点赞”这些“三不”用户，是活跃的、有参与的用户。这些用户才是真正有价值的，正好去除了僵尸粉。 两种思路采集微博粉丝，目前我有两种方法来解决这个问题，： 全量采集。采集微博所有用户数据，包括关注、粉丝等。通过粉丝的粉丝、关注的关注、用户分类、推荐等等各种方法拿到微博全量用户数据。 采样。采集胡歌的所有微博下有评论、点赞、转发的用户，凡是有参与过的亲密值加一，当这个值超过一定限度时（比如说5或者10），我们就认为该用户是胡歌的粉丝。 想了想，第一种方法短时间内是不现实的，方法2倒是可以尝试一波。 爬虫逻辑爬虫分为三步： 采集胡歌所有微博 采集每条微博的三类数据（转发、评论、点赞） 数据清洗 好了，现在已经非常清晰了，下面就开始去寻找爬取方法。 微博接口根据以往的经验，weibo.cn 和 m.weibo.cn 是最简单爬取的，weibo.com 是最难的。这次我们从 m.weibo.cn 入手，分析可以得到胡歌微博的接口，而且是无需登录的！！！很重要。其他入口都需要解决登录难题！ https://m.weibo.cn/api/container/getIndex?containerid=1076031223178222&amp;page={} 返回数据： 1234567cardlistInfo: &#123;containerid: \"1076031223178222\",v_p: 42,show_style: 1,total: 3643,page: 2&#125;, 这里告诉我们总共有3643条数据，每页10条，那么翻页就很清晰了。 其他接口12345转发：https://m.weibo.cn/api/statuses/repostTimeline?id=4238119278366780&amp;page=&#123;&#125;评论：https://m.weibo.cn/api/comments/show?id=4238119278366780&amp;page=&#123;&#125;点赞：https://m.weibo.cn/api/attitudes/show?id=4238119278366780&amp;page=&#123;&#125; （想要爬其他人，替换这里的id即可） 暂时不清楚总共有多少页，虽然返回的数据中有 total_number ，但是此数字并不准确，还需要更多测试。 123total_number: 19526897,hot_total_number: 0,max: 1952690 （简单测后发现总页数为total_number//55） 采集用户信息接口https://m.weibo.cn/api/container/getIndex?type=uid&amp;value=6114792181 其实不需要这一次请求，因为在转发接口中已经有我们想要的数据了，如下： 1234567891011121314151617181920212223242526user: &#123;id: 6431898981,screen_name: \"豪气superiority\",profile_image_url: \"https://tvax2.sinaimg.cn/crop.9.0.220.220.180/0071hAK9ly8fmdxpea2vxj306n064glj.jpg\",profile_url: \"https://m.weibo.cn/u/6431898981?uid=6431898981&amp;featurecode=20000320\",statuses_count: 7255,verified: false,verified_type: -1,close_blue_v: false,description: \"\",gender: \"m\",mbtype: 0,urank: 4,mbrank: 0,follow_me: false,following: false,followers_count: 2,follow_count: 62,cover_image_phone: \"https://tva1.sinaimg.cn/crop.0.0.640.640.640/549d0121tw1egm1kjly3jj20hs0hsq4f.jpg\",avatar_hd: \"https://wx2.sinaimg.cn/orj480/0071hAK9ly8fmdxpea2vxj306n064glj.jpg\",like: false,like_me: false,badge: &#123;user_name_certificate: 1,wenchuan_10th: 1&#125; 很蛋疼的是，点赞和评论接口中并没有相关数据，所以点赞和评论部分要重新爬取，如下： 1234567891011121314151617&#123;id: 4247512245791226,created_at: \"5分钟前\",source: \"微博 weibo.com\",user: &#123;id: 6114792181,screen_name: \"Ming_54456\",profile_image_url: \"https://tvax1.sinaimg.cn/crop.367.164.918.918.180/006FP2Mlly8fjs2mf3x6pj319x0yoqbo.jpg\",verified: false,verified_type: -1,mbtype: 12,profile_url: \"https://m.weibo.cn/u/6114792181?uid=6114792181&amp;featurecode=20000320\",remark: \"\",following: false,follow_me: false&#125;&#125;, 微博官方API同样提供相应数据 ，建议使用前仔细阅读 接口访问频次权限 爬虫代码爬虫完整代码可以去我的公众号（Python爬虫与算法进阶），回复“微博”获得。 爬虫语言是Python3，使用Scrapy框架，数据保存在mongo，没有使用分布式，单机3天跑完。 因为微博的反爬，需要大量代理支撑。 1234567891011121314# code is far away from bugs with the god animal protecting I love animals. They taste delicious. ┏┓ ┏┓ ┏┛┻━━━┛┻┓ ┃ ☃ ┃ ┃ ┳┛ ┗┳ ┃ ┃ ┻ ┃ ┗━┓ ┏━┛ ┃ ┗━━━┓ ┃ 神兽保佑 ┣┓ ┃ 永无BUG！ ┏┛ ┗┓┓┏━┳┓┏┛ ┃┫┫ ┃┫┫ ┗┻┛ ┗┻┛ 爬取的数据实例： 1234567891011&#123; \"_id\" : ObjectId(\"5b162d10e0eafb1d6e63b460\"), \"id\" : NumberLong(5372682651), \"statuses_count\" : 10599, \"screen_name\" : \"用户5372682651\", \"profile_url\" : \"https://m.weibo.cn/u/5372682651?uid=5372682651\", \"description\" : \"暂无数据\", \"gender\" : \"f\", \"followers_count\" : 80, \"follow_count\" : 1060&#125; 简单数据清洗最终跑完一次爬到的数据有3889285，因为有大量页面会跳转到登录页面，对这些请求做一个重试效果会好些。 数据清洗对我来说真的是个头疼的问题，找了很多相关资料，最后使用了mongo的aggregate方法，该方法也是我第一次使用，下面是代码： 1234567891011db.getCollection('Weibo').aggregate( [ &#123;\"$group\" : &#123;_id:&#123;id:\"$id\",gender:\"$gender\"&#125;, count:&#123;$sum:1&#125;&#125;&#125;, &#123;$sort:&#123;\"count\":-1&#125;&#125;, &#123; $out:\"result\"&#125;, ], &#123; allowDiskUse:true, cursor:&#123;&#125; &#125; ) 结果产生了一张新的表，对每个ID进行统计，并排序，如下： 1234567891011121314&#123; \"_id\" : &#123; \"id\" : NumberLong(5737668415), \"gender\" : \"f\" &#125;, \"count\" : 106701.0&#125;&#123; \"_id\" : &#123; \"id\" : NumberLong(5909154992), \"gender\" : \"m\" &#125;, \"count\" : 72298.0&#125; 参与次数达到十万次，天呐，超级真爱粉，缘来是她，疯狂刷屏有没有 好了，现在开始看看真正的数据吧。 本次共采集用户数据3889285条，，原始数据中男性占比%33.68，女性占比%66.32，好吧，看来女性粉丝更多；去重之后数据共有1129035，男性占比%29.58，女性占比%70.42，怎么看着女性粉丝还是更多呢。。 我们再来计算一个数据，亲密度大于10的粉丝共有16486位，其中男性占比%24.05，女性占比%75.95，于是有下面这张表格。 亲密度 男性占比 女性占比 粉丝总数 大于0 29.58% 70.42% 1129035 大于10 24.05% 75.95% 16486 大于50 32.77% 67.23% 4285 大于100 36.77% 63.23% 2578 大于1000 40.18% 59.82% 331 大于10000 37.5% 62.5% 24 Top10 30.00% 70.00% 10 这个数据挺有意思的，画张表瞧瞧 粉丝昵称词云 （感谢BDP） 结论看了这些数据，相信大家自己心中已经有了答案。 胡歌作为一个玉树临风、 英俊潇洒、 风流倜傥、 一表人才、 高大威猛、 气宇不凡、 温文尔雅、 品貌非凡、 仪表不凡的男人，女粉丝比较多是很正常的。但是为啥大家都会有一种男粉丝比女粉丝多的错觉呢，我觉得是对比产生的感觉。我拿胡歌与其他小鲜肉作对比，肯定会跟欣赏胡歌。你说呢？ 本文并不是为了证明什么，只是作为一名普通粉丝想去看看更多东西。其实本次数据爬取有很多地方需要优化，大家不用太过当真。如果你有更好的分析数据的想法，可以联系我。 老大镇楼","tags":[{"name":"胡歌","slug":"胡歌","permalink":"https://zhangslob.github.io/tags/胡歌/"},{"name":"微博","slug":"微博","permalink":"https://zhangslob.github.io/tags/微博/"}]},{"title":"zsh(+fish)=完美终端","date":"2018-06-05T14:27:23.000Z","path":"2018/06/05/zsh-fish-完美终端/","text":"这是崔斯特的第五十二篇原创文章 好看、好用 (๑• . •๑) 自从用了深度，有一个非常明显的变化就是终端的改变，实在是比windows的好用一百倍，尤其是使用一些工具。下面说说我现在的配置。 如下图，是我目前正在使用的终端，集成了zsh和fish的功能，目前用着最顺手的。 https://github.com/robbyrussell/oh-my-zsh 安装zsh一般来说，直接运行sudo apt-get install zsh即可，当然也可以下载源Download zsh source，使用curl安装curl -L &lt;https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh&gt; | sh 把zsh设置为默认终端chsh -s $(which zsh) 更多细节参考Installing ZSH 先欣赏下zsh的效果。来自官网 修改zsh主题vi ~/.zshrc，然后找到ZSH_THEME，默认的是ZSH_THEME=robbyrussell，就像我这样，因为我这里用的是深度终端，而且也修改了终端主题 当然，你可以来这里看看，选一个自己喜欢的主题 Themes agnoster也很好看。 据说大神都用random，是真的吗？ 安装 fish有句话这样说 二逼青年用 bash，普通青年用 zsh，文艺青年用 fish 我最喜欢 fish的一点就是 根据历史输入自动补全，来看图，只要是历史有输入的，都会有记录有提示，对于一些很长的命令，简直超级爽，再也不用手动复制粘贴了。 但是fish和zsh好像不能同时使用，但是有一个插件可以在zsh上达到和fish同样的效果。 地址在这里 zsh-autosuggestions 首先下载下来 git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions 然后vi ~/.zshrc，添加zsh-autosuggestions到plugins中，git是默认就有的。然后新打开一个终端，就可以达到fish有的你是提示功能了。 还可以安装语法高亮插件 zsh-syntax-highlighting，安装方法和上面的一样，在plugins中添加zsh-syntax-highlighting即可。 这些是我目前发现的比较好用的插件和工具，大家有什么推荐的吗？","tags":[{"name":"linux","slug":"linux","permalink":"https://zhangslob.github.io/tags/linux/"},{"name":"zsh","slug":"zsh","permalink":"https://zhangslob.github.io/tags/zsh/"}]},{"title":"爬虫学到什么程度可以去找工作","date":"2018-05-29T14:52:52.000Z","path":"2018/05/29/爬虫学到什么程度可以去找工作/","text":"这是崔斯特的第五十一篇原创文章 分享下我的经验与教训 (๑• . •๑) 最近很多朋友问我，我在自学爬虫，学到什么程度可以去找工作呢？ 这篇文章会说说我自己的心得体验，关于爬虫、关于工作，仅供参考。 学到哪种程度暂且把目标定位初级爬虫工程师，简单列一下吧： （必要部分） 语言选择：一般是了解Python、Java、Golang之一 熟悉多线程编程、网络编程、HTTP协议相关 开发过完整爬虫项目（最好有全站爬虫经验，这个下面会说到） 反爬相关，cookie、ip池、验证码等等 熟练使用分布式 （非必要，建议） 了解消息队列，如RabbitMQ、Kafka、Redis等 具有数据挖掘、自然语言处理、信息检索、机器学习经验 熟悉APP数据采集、中间人代理 大数据处理（Hive/MR/Spark/Storm） 数据库Mysql，redis，mongdb 熟悉Git操作、linux环境开发 读懂js代码，这个真的很重要 如何提升随便看看知乎上的教程就可以入门了，就Python而言，会requests当然是不够的，还需要了解scrapy和pyspider这两个框架，scrapy_redis也是需要理解原理的。 分布式如何搭建、如何解决其中遇到内存、速度问题。 参考 scrapy-redis 和 scrapy 有什么区别？ 什么叫全站爬取最简单的拿拉钩来举例，搜索关键词，有30页，不要以为把这30页爬完就是全站爬取了，你应该想方法把所有数据全部爬下来。 什么办法，通过筛选缩小范围，慢慢来就OK了。 同时，每个职位还会有推荐职位，再写一个采集推荐的爬虫。 这个过程需要注意的是如何去重，Mongo可以、redis也可以 参考 Scrapy中如何提高数据的插入速度 实际项目经验这个面试中肯定会被人问道，如： 你爬过哪些网站 日均最大采集量是多少 你遇到哪些棘手问题，如何解决 等等 那么怎么找项目呢？比如我要爬微博数据，去Github中搜索下，项目还算少吗？ 语言选择我自己建议是Python、Java、Golang最好都了解，Java爬虫的也很多，但是网上教程几乎都是Python的，悲哀。 最后说下Golang，Golang真的很牛逼，说个数字，Golang可以每分钟下载网页数量 2W ，Python可以吗~~ 宣传下自己的刷题项目 Leetcode Solutions By All Language 关于反爬常见的 UA、Refer等需要了解是什么东西，有些验证的ID如何产生的，是否必要；关于IP池这块我不了解，不多说，需要注意的是如何设计拉黑机制；模拟登陆也是必要的，fuck-login 可以研究下代码，或者提PR。 模拟登陆其实就是一步步的请求，保存cookie会话 如何判断能力足够很简单，给个任务，爬取知乎上所有问题。 你会如何思考并设计这个项目？ 欢迎留言指出 以上仅为个人看法，若有不足之处请指出。希望可以帮助你","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"}]},{"title":"你写过哪些让你睡不好觉的BUG","date":"2018-05-28T15:38:35.000Z","path":"2018/05/28/你写过哪些让你睡不好觉的BUG/","text":"这是崔斯特的第五十篇原创文章 你写过什么有趣的BUG？ (๑• . •๑) 今天，聊聊 BUG。 今天下午发现程序中一个BUG，紧急修复之后重新上线，debug上测试没毛病，OK上正式环境。build完之后，程序一直有问题，没法读数据，头疼啊，自己看的眼睛都要瞎了，找不到原因。 增加更多日志，给出错的地方每一条都打印出日志来，看看到底是哪里出了问题。改完，上debug，好的没毛病，上正式环境，根据日志我大概判断出是那里的问题了，改，线下测试，debug测试，都没问题，再上正式版，还是不行。 这个时候已经精疲力竭了，想砸电脑！！！ 为什么debug环境可以，正式环境不行啊？？ 然后程序就跑起来了，是的，就跑起来了。下班，回家。（已经快10点了） 回到家，洗完澡，写完这篇文章，再去检查日志，so far so good！！开心~ 现在我再仔细想了想，可能是这样：线上服务器和数据库压力比较大，程序子线程没有跑起来、或者数据库没建索引、查询时间过久（明天去验证下，因为之前一直没遇到类似问题） 不管了，我要睡觉了。 来讨论下，你写过什么让你睡不好觉的BUG？","tags":[{"name":"BUG","slug":"BUG","permalink":"https://zhangslob.github.io/tags/BUG/"}]},{"title":"git大法好，push需谨慎","date":"2018-05-26T07:38:28.000Z","path":"2018/05/26/git大法好，push需谨慎/","text":"这是崔斯特的第四十九篇原创文章 注意自己的账号安全 (๑• . •๑) 在每次git push前，请检查你的提交文件 故事是这样来的。（我又开始讲故事了） 前几天在 Github上找一些资料，碰巧看到一个合适的，就把他 clone 下来，准备在本地跑着试试看效果，但是在运行的时候却发现提示错误，根绝错误提示原因我发现是缺少了一个名为config.py的文件。 经验告诉我，这应该是一个写有相关配置的文件。现在缺少了这个文件，整个程序就没法运行，自己写的话又不知道格式什么的。 那我该怎么办？ 最后我还是找到了，在Github上。我去该项目上看到作者提交了很多次的commit，从历史的提交中我找到了相关信息。这是一个包含有作者相关数据库的文件，我已经通知作者，让他去删除此项目。 看下图，每个开源项目都会显示所有的commits，每次提交都会把git工作目录下所有文件提交（当然你可以指定具体的文件，我习惯git add .）。即使你下今天把密码删除了，但是你昨天提交的密码还是会保存到commits中，别人还是可以找到。 例如你现在看到的项目 Leetcode-Solutions ，你可以从commit中进入，查看到历史contributors的每一次提交的完整文件， 如：很久之前的提交 所以看到这里，你就有必要想想自己有没有把任何个人隐私数据提交到 Github 上，如果有，建议还是删除项目吧。 当然，这有一个前提，就是你的项目是公开的（Public），如果是私有的（Private），就不用考虑了。 创建私有项目是收费的，一般适合公司和组织。 最后首尾呼应： git大法好，push需谨慎","tags":[{"name":"Git","slug":"Git","permalink":"https://zhangslob.github.io/tags/Git/"}]},{"title":"Python最假的库：Faker","date":"2018-05-22T13:14:35.000Z","path":"2018/05/22/Python最假的库：Faker/","text":"这是崔斯特的第四十八篇原创文章 好假啊 (๑• . •๑) 先申明下，这里说的Faker和LOL的大魔王没有任何关系，只是恰好重名而已。 故事由来最近做一个项目时需要随机生成人的名字，百度之后，我是这样写的 123456789101112131415def random_first_name(): \"\"\"百家姓中选择一个\"\"\" name = ['赵', '钱', '孙', '李', '周', '吴', '郑', '王', '冯', '陈', '褚', '卫', '蒋', '沈', '韩', '杨', '朱', '秦', '尤', '许', '何', '吕', '施', '张', '孔', '曹', '严', '华', '金', '魏', '陶', '姜', '戚', '谢', '邹', '喻', '柏', '水', '窦', '章', '云', '苏', '潘', '葛', '奚', '范', '彭', '郎', '鲁', '韦', '昌', '马', '苗', '凤', '花', '方', '俞', '任', '袁', '柳'] return random.choice(name)def random_last_name(): \"\"\"生成随机汉语\"\"\" head = random.randint(0xb0, 0xf7) body = random.randint(0xa1, 0xf9) # 在head区号为55的那一块最后5个汉字是乱码,为了方便缩减下范围 val = f'&#123;head:x&#125;&#123;body:x&#125;' str_ = bytes.fromhex(val).decode('gb2312') return str_name = random_first_name() + random_last_name() 前辈在review的时候说怎么这么复杂，Python中有一个专门生成各类假数据的库：Faker，你去了解下。 Faker项目地址：faker 安装：pip install Faker 中文生成假数据：Language zh_CN 那么Faker能生成那些假数据了？ 1234from faker import Fakerfake = Faker(locale='zh_CN')# 初始化 地址1234567891011121314fake.street_name()# '广州街fake.city_suffix()# '县'fake.street_address()# '香港路B座'fake.longitude()# -98.702031fake.district()# '璧山' 汽车12fake.license_plate()# HZL 767 银行12345678fake.bban()# 'KLUX5928618542924'fake.bank_country()# 'GB'fake.iban()# 'GB04BPNH0448315286040' 条形码12345678fake.ean(length=13)# '0994331656275'fake.ean8()# '51309350'fake.ean13()# '8336323543385' 公司1234567891011fake.company_prefix()# '鸿睿思博'fake.bs()# 'embrace strategic schemas'fake.company_suffix()# '科技有限公司'fake.company()# '昂歌信息网络有限公司' 信用卡1234567891011121314fake.credit_card_security_code(card_type=None)# '360'fake.credit_card_full(card_type=None)# 'Diners Club / Carte Blanche\\n林 莘\\n30311852484679 10/19\\nCVC: 388\\n'fake.credit_card_number(card_type=None)# '30240280288941'fake.credit_card_expire(start=\"now\", end=\"+10y\", date_format=\"%m/%y\")# '11/26'fake.credit_card_provider(card_type=None)# 'Maestro' 互联网1234567891011121314151617181920212223242526fake.domain_word(*args, **kwargs)# 'jin'fake.company_email(*args, **kwargs)# 'zoulei@hou.com'fake.free_email(*args, **kwargs)# 'vxu@yahoo.com'fake.ipv4_private(network=False, address_class=None)# '10.202.214.57'fake.ascii_safe_email(*args, **kwargs)# 'baiyan@example.net'fake.email(*args, **kwargs)# 'minggao@gmail.com'fake.image_url(width=None, height=None)# 'https://www.lorempixel.com/817/102'fake.uri_page()# 'category'fake.ipv4_network_class()# 'c' 姓名1234567891011121314151617181920212223242526272829303132333435363738394041fake.first_name_female()# '秀华'fake.name_male()# '郏杰'fake.suffix_female()# ''fake.first_name()# '东'fake.prefix_female()# ''fake.last_name_male()# '扶'fake.last_name()# '荣'fake.name_female()# '曹红'fake.suffix_male()# ''fake.last_name_female()# '辛'fake.last_romanized_name()# 'Zhang'fake.first_romanized_name()# 'Min'fake.romanized_name()# 'Xiuying Qiao'fake.name()# '钟想' 电话12345678fake.phone_number()# '18874465626'fake.msisdn()# '8086764507444'fake.phonenumber_prefix()# 155 user_agent这个大家应该很熟悉，常用的就是 fake-useragent这个库12345678910111213141516171819202122232425262728293031323334353637fake.mac_platform_token()# 'Macintosh; Intel Mac OS X 10_12_1'fake.firefox()# ('Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_9_4; rv:1.9.4.20) '# 'Gecko/2012-05-03 04:16:34 Firefox/3.6.10')fake.windows_platform_token()# 'Windows 95'fake.safari()# ('Mozilla/5.0 (iPod; U; CPU iPhone OS 3_1 like Mac OS X; sat-IN) '# 'AppleWebKit/533.2.4 (KHTML, like Gecko) Version/3.0.5 Mobile/8B113 '# 'Safari/6533.2.4')fake.chrome(version_from=13, version_to=63, build_from=800, build_to=899)# ('Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5331 (KHTML, like Gecko) '# 'Chrome/52.0.838.0 Safari/5331')fake.opera()# 'Opera/8.83.(X11; Linux i686; ce-RU) Presto/2.9.169 Version/10.00'fake.mac_processor()# 'Intel'fake.user_agent()# ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_9 rv:3.0; pa-IN) '# 'AppleWebKit/532.47.6 (KHTML, like Gecko) Version/4.0.1 Safari/532.47.6')fake.linux_platform_token()# 'X11; Linux x86_64'fake.linux_processor()# 'i686'fake.internet_explorer()# 'Mozilla/5.0 (compatible; MSIE 5.0; Windows NT 5.01; Trident/3.1)' 这里举例的都是中文的，当然也有其他语言的，小伙伴可以去官网看看。 最近在和小伙伴刷题，欢迎加入 Leetcode Solutions By All Language","tags":[{"name":"Faker","slug":"Faker","permalink":"https://zhangslob.github.io/tags/Faker/"}]},{"title":"强大的异步爬虫 with aiohttp","date":"2018-05-16T09:00:07.000Z","path":"2018/05/16/强大的异步爬虫-aiohttp/","text":"这是崔斯特的第四十七篇原创文章 异步了解下 (๑• . •๑) 看到现在网络上大多讲的都是requests、scrapy，却没有说到爬虫中的神器：aiohttp aiohttp 介绍aiohttp是什么，官网上有这样一句话介绍：Async HTTP client/server for asyncio and Python，翻译过来就是 asyncio和Python的异步HTTP客户端/服务器 主要特点是： 支持客户端和HTTP服务器。 无需使用Callback Hell即可支持Server WebSockets和Client WebSockets。 Web服务器具有中间件，信号和可插拔路由。 emmmm，好吧，还是来看代码吧 Client example: 1234567891011121314import aiohttpimport asyncioasync def fetch(session, url): async with session.get(url) as response: return await response.text()async def main(): async with aiohttp.ClientSession() as session: html = await fetch(session, 'http://httpbin.org/headers') print(html)loop = asyncio.get_event_loop()loop.run_until_complete(main()) output: 1&#123;\"headers\":&#123;\"Accept\":\"*/*\",\"Accept-Encoding\":\"gzip, deflate\",\"Connection\":\"close\",\"Host\":\"httpbin.org\",\"User-Agent\":\"Python/3.6 aiohttp/3.2.1\"&#125;&#125; Server example: 123456789101112from aiohttp import webasync def handle(request): name = request.match_info.get('name', \"Anonymous\") text = \"Hello, \" + name return web.Response(text=text)app = web.Application()app.add_routes([web.get('/', handle), web.get('/&#123;name&#125;', handle)])web.run_app(app) output: 12======== Running on http://0.0.0.0:8080 ========(Press CTRL+C to quit) aiohttp 与 requests去翻一下官方文档 Client Quickstart，让我感觉非常熟悉，很多用法和requests相似。 1234async with aiohttp.ClientSession() as session: async with session.get('http://httpbin.org/get') as resp: print(resp.status) print(await resp.text()) 首先，官方推荐使用ClientSession来管理会话，这不就是requests中的session吗？用法也类似，使用session.get()去发送get请求，返回的resp中就有我们所需要的数据了，用法也和requests一样，text（）文本，.json()直接打印返回的json数据，headers什么的也一样，更多内容参考官方文档Response object 既然已经有requests了，那为什么还要说aiohttp了？重点来了，aiohttp是异步的。在python3.5中，加入了asyncio/await 关键字，使得回调的写法更加直观和人性化。而aiohttp是一个提供异步web服务的库，asyncio可以实现单线程并发IO操作。 requests写爬虫是同步的，是等待网页下载好才会执行下面的解析、入库操作，如果在下载网页时间太长会导致阻塞，使用multiprocessing或者 threading加速爬虫也是一种方法。 我们现在使用的aiohttp是异步的，简单来说，就是不需要等待，你尽管去下载网页就好了，我不用傻傻的等待你完成才进行下一步，我还有别的活要干。这样就极大的提高了下载网页的效率。 另外，Scrapy也是异步的，是基于Twisted事件驱动的。在任何情况下，都不要写阻塞的代码。阻塞的代码包括： 访问文件、数据库或者Web 产生新的进程并需要处理新进程的输出，如运行shell命令 执行系统层次操作的代码，如等待系统队列 代码实例这里是使用aiohttp的一个爬虫实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import asyncioimport aiohttpfrom bs4 import BeautifulSoupimport loggingclass AsnycGrab(object): def __init__(self, url_list, max_threads): self.urls = url_list self.results = &#123;&#125; self.max_threads = max_threads def __parse_results(self, url, html): try: soup = BeautifulSoup(html, 'html.parser') title = soup.find('title').get_text() except Exception as e: raise e if title: self.results[url] = title async def get_body(self, url): async with aiohttp.ClientSession() as session: async with session.get(url, timeout=30) as response: assert response.status == 200 html = await response.read() return response.url, html async def get_results(self, url): url, html = await self.get_body(url) self.__parse_results(url, html) return 'Completed' async def handle_tasks(self, task_id, work_queue): while not work_queue.empty(): current_url = await work_queue.get() try: task_status = await self.get_results(current_url) except Exception as e: logging.exception('Error for &#123;&#125;'.format(current_url), exc_info=True) def eventloop(self): q = asyncio.Queue() [q.put_nowait(url) for url in self.urls] loop = asyncio.get_event_loop() tasks = [self.handle_tasks(task_id, q, ) for task_id in range(self.max_threads)] loop.run_until_complete(asyncio.wait(tasks)) loop.close()if __name__ == '__main__': async_example = AsnycGrab(['http://edmundmartin.com', 'https://www.udemy.com', 'https://github.com/', 'https://zhangslob.github.io/', 'https://www.zhihu.com/'], 5) async_example.eventloop() print(async_example.results) 需要注意的是，你需要时刻在你的代码中使用异步操作，你如果在代码中使用同步操作，爬虫并不会报错，但是速度可能会受影响。 其他异步库因为爬虫不仅仅只有下载这块，还会有操作数据库，这里提供两个异步库：aioredis、motor 123456789101112131415import asyncioimport aioredisloop = asyncio.get_event_loop()async def go(): conn = await aioredis.create_connection( 'redis://localhost', loop=loop) await conn.execute('set', 'my-key', 'value') val = await conn.execute('get', 'my-key') print(val) conn.close() await conn.wait_closed()loop.run_until_complete(go())# will print 'value' 文档：aioredis 123456789101112131415import motor.motor_asyncioclient = motor.motor_asyncio.AsyncIOMotorClient('mongodb://localhost:27017')db = client['test_database']collection = db['test_collection']async def do_insert(): document = &#123;'key': 'value'&#125; result = await db.test_collection.insert_one(document) print('result %s' % repr(result.inserted_id)) async def do_find_one(): document = await db.test_collection.find_one(&#123;'i': &#123;'$lt': 1&#125;&#125;) pprint.pprint(document) 文档：motor 本文仅仅介绍了aiohttp作为Client的用法， 有兴趣的朋友可以去研究下作为Server的用法，同样很强大。","tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://zhangslob.github.io/tags/Leetcode/"}]},{"title":"Leetcode Solutions（一） two-sum","date":"2018-05-15T10:22:26.000Z","path":"2018/05/15/Leetcode Solutions（一）/","text":"这是崔斯特的第四十六篇原创文章 开始刷题咯 (๑• . •๑) Two Sum 题目给定一个整数数组和一个目标值，找出数组中和为目标值的两个数。 你可以假设每个输入只对应一种答案，且同样的元素不能被重复利用。 示例:1234给定 nums = [2, 7, 11, 15], target = 9因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] 解题思路Goa + b = target 也可以看成是 a = target - b 在map[整数]整数的序号中，可以查询到a的序号。这样就不用嵌套两个for循环了。 12345678910func twoSum(nums []int, target int) []int &#123; m := make(map[int]int, len(nums)) for i, b := range nums &#123; if j, ok := m[target-b]; ok &#123; return []int&#123;j, i&#125; &#125; m[nums[i]] = i &#125; return nil&#125; Python 由于要找到符合题意的数组元素的下标，所以先要将原来的数组深拷贝一份，然后排序。 然后在排序后的数组中找两个数使它们相加为target。这个思路比较明显：使用两个指针，一个指向头，一个指向尾，两个指针向中间移动并检查两个指针指向的数的和是否为target。如果找到了这两个数，再将这两个数在原数组中的位置找出来就可以了。 要注意的一点是：在原来数组中找下标时，需要一个从头找，一个从尾找，要不无法通过。如这个例子：numbers=[0,1,2,0]; target=0。如果都从头开始找，就会有问题。 12345678910class Solution: def twoSum(self, nums, target): if len(nums) &lt;= 1: return False d = dict() for i in range(len(nums)): if nums[i] in d: return [d[nums[i]], i] else: d[target - nums[i]] = i","tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"https://zhangslob.github.io/tags/Leetcode/"}]},{"title":"告别win10，拥抱linux","date":"2018-05-12T05:49:33.000Z","path":"2018/05/12/告别win10，拥抱linux/","text":"这是崔斯特的第四十五篇原创文章 安装linux操作系统 (๑• . •๑) win10 升级先问你一个问题，你讨厌win10升级系统吗？ 我的回答：是，明明已经把自动更新关闭了，可是还是会有“易升”，win10易升一直卸载不掉。所以就想试试别的系统。 linux是最好的选择。黑苹果暂时不考虑。 喜欢linux的理由 深度桌面 深度终端（配合zsh超赞的） 除了颜值外，程序兼容性会更好，安装各种东西会很方便。作为一名程序员，熟悉linux下基本操作也是必要的。 我自己试过，爬虫会跑的更快。 手动滑稽 选择linux哪个版本目前我使用过deepin和ubuntu18，对于完全的小白来说，我推荐deepin也就是深度操作系统，深度商店收入的应用可以基本满足，ubuntu很多应用安装起来比较麻烦，如果你喜欢折腾，那就上手ubuntu吧。 如果你和我一样 喜欢xxx，那就试试deepin和ubuntu18共存。 我现在的开机界面（渣渣像素） 如何安装linux安装deepin使用U盘安装 先去下载： ISO文件 深度启动盘制作工具 然后安装启动盘制作工具，然后选择刚才下载的ISO文件，下一步选择你的U盘，然后就开始安装了。 下一步，重启电脑，一般情况下电脑默认是从硬盘启动，因此，在使用U盘安装系统之前，您需要先进入电脑的BIOS界面将U盘设置为第一启动项。 台式机一般为 Delete 键、笔记本一般为 F2 或 F10 或 F12 键，即可进入 BIOS 设置界面。 将深度操作系统光盘插入电脑光驱中。 启动电脑，将光盘设置为第一启动项。 进入安装界面，选择需要安装的语言。 如果还不会，这里有官方录制的视频哦 深度安装器+深度探索频道第七期+深度操作系统官方出品 还有一种更加简单的方式就是下载 深度系统安装器 然后就是傻瓜操作了，记得关闭下 安全启动 小歪并不推荐使用第二种方式安装，在笔记本上怎么都没有效果，在台式上一次成功。所以大家有U盘的尽量使用U盘吧 安装ubuntu需要用到上面提到过的深度启动盘制作工具，然后去下载 ISO文件，然后就是和上面安装方法一样的，进行操作即可。 有没有很简单。 我的ubuntu界面，用得少，所以没美化 感受我使用deepin有一个月了，写代码用deepin，家里的台式还是win7，因为deepin虽然有steam，但是吃鸡不支持在linux下运行。 deepin完全可以满足我的办公需求，Pycharm、sublime、typora、chrome、网易云音乐等等都有，用起来很舒服，至少现在是这样感觉。但是有时候deepin也会卡死。 强烈建议上手linux，可以学到很多命令行操作，安装deepin就好，到时候你的电脑会Windows与deepin共存，根据场景选择系统。 有时间写一篇deepin美化与安装应用相关的东西，看到这一定要点赞哦。","tags":[{"name":"linux","slug":"linux","permalink":"https://zhangslob.github.io/tags/linux/"}]},{"title":"linux下安装python3.6","date":"2018-05-11T14:26:54.000Z","path":"2018/05/11/linux下安装python3-6/","text":"这是崔斯特的第四十四篇原创文章 linux操作 (๑• . •๑) 1234567891011121314151617181920212223sudo sed -i 's\\archive.ubuntu.com\\mirrors.aliyun.com\\g' /etc/apt/sources.listsudo apt-get updatecd /home/sudo apt-get install gcc make zlib1g-dev -ysudo apt-get install libbz2-dev libsqlite3-dev libxml2-dev libffi-dev libssl-dev -ysudo apt install wget -ywget http://mirrors.sohu.com/python/3.6.2/Python-3.6.2.tgz # 可以换成你想要的版本tar -xvf Python-3.6.2.tgzcd Python-3.6.2./configure --prefix=/home/usr/python36/sudo makesudo make installcd ../usr/python36/bin/sudo mkdir -p ~/.pip/sudo cat &lt;&lt; EOF &gt; ~/.pip/pip.conf[global]trusted-host=mirrors.aliyun.comindex-url=http://mirrors.aliyun.com/pypi/simple/EOFsudo apt-get install libmysqlclient-dev -yln -s /home/usr/python36/bin/pip3 /usr/bin/pip36ln -s /home/usr/python36/bin/python3 /usr/bin/python36 以后就可以使用 pip36 python36来进行操作。 如果在命令行中输入scrapy提示没这个命令，可以试试python36 -m scrapy","tags":[{"name":"linux","slug":"linux","permalink":"https://zhangslob.github.io/tags/linux/"}]},{"title":"护眼神器了解下","date":"2018-05-08T14:05:24.000Z","path":"2018/05/08/护眼神器了解下/","text":"这是崔斯特的第四十三篇原创文章 保护眼睛 (๑• . •๑) 整天面对屏幕，护眼是必须的，下面推荐几款小歪使用过的软件。 flux官网：https://justgetflux.com/ 使用起来非常简单，是我之前非常喜欢的一款产品，会随着一天之内光线强弱改变屏幕的颜色。但是在linux上似乎不起作用，安装之后，屏幕颜色没有发生任何变化，于是我放弃了，选择另一款软件 == redshift网站：https://github.com/jonls/redshift Ubuntu下sudo apt-get install redshift就可以安装，打开方式是在命令行输入redshift -v -t 4500:2500，然后颜色就会变得非常舒服。 如果在你的电脑上 redshift 有时不工作，检查是否开启了多个 redshift。 Safe Eyes网站：http://slgobinath.github.io/SafeEyes/ 这款应用会自动关闭屏幕，间隔一段时间会有休息，这个是否好好放松下自己的眼睛。 这个好像只能安装在linux上 大家还有什么推荐的呢？欢迎评论指出","tags":[{"name":"护眼","slug":"护眼","permalink":"https://zhangslob.github.io/tags/护眼/"}]},{"title":"awesome_crawl(一)：腾讯新闻","date":"2018-05-01T13:52:11.000Z","path":"2018/05/01/awesome-crawl-一-：腾讯新闻/","text":"这是崔斯特的第四十二篇原创文章 awesome (๑• . •๑) 项目地址：https://github.com/zhangslob/awesome_crawl awesome_crawl（优美的爬虫）1、腾讯新闻的全站爬虫采集策略 从网站地图出发，找出所有子分类，从每个子分类中再寻找详情页面的链接。 首先寻找每条新闻的ID，然后移动端采集具体内容。 再去找一些推荐新闻的接口，做一个“泛爬虫”。 说明 整套系统中分为两部分，一套是生产者，专门去采集qq新闻的链接，然后存放到redis中，一套是消费者，从redis中读取这些链接，解析详情数据。所有配置文件都是爬虫中的custom_settings中，可以自定义。 如果需要设置代理，请在middlewares.ProxyMiddleware中设置。 qq_list: 这个爬虫是生产者。运行之后，在你的redis服务器中会出现qq_detail:start_urls，即种子链接 qq_detail: 这个爬虫是生消费者，运行之后会消费redis里面的数据，如下图： 你可以自行添加更多爬虫去采集种子链接，如从首页进入匹配，从推荐入口： 12345678910111213141516171819import requestsurl = \"https://pacaio.match.qq.com/xw/recommend\"querystring = &#123;\"num\":\"10^\",\"callback\":\"__jp0\"&#125;headers = &#123; 'accept-encoding': \"gzip, deflate, br\", 'accept-language': \"zh-CN,zh;q=0.9,en;q=0.8\", 'user-agent': \"Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1\", 'accept': \"*/*\", 'referer': \"https://xw.qq.com/m/recommend/\", 'authority': \"pacaio.match.qq.com\", 'cache-control': \"no-cache\", &#125;response = requests.request(\"GET\", url, headers=headers, params=querystring)print(response.text) 等等，你所需要做的仅仅是把这些抓到的种子链接塞到redis里面，也就是启用qq_news.pipelines.RedisStartUrlsPipeline这个中间件。 TODO 增加更多新闻链接的匹配，从推荐接口处获得更多种子链接 增加“泛爬虫”，采集种子链接 数据库字段检验 redis中数据为空爬虫自动关闭（目前redis数据被消费完之后爬虫并不会自动关闭，如下图）","tags":[{"name":"awesome_crawl","slug":"awesome-crawl","permalink":"https://zhangslob.github.io/tags/awesome-crawl/"},{"name":"scrapy","slug":"scrapy","permalink":"https://zhangslob.github.io/tags/scrapy/"}]},{"title":"scrapy-redis 和 scrapy 有什么区别？","date":"2018-04-21T10:03:14.000Z","path":"2018/04/21/有什么区别？/","text":"这是崔斯特的第四十一篇原创文章 分布式爬虫 (๑• . •๑) 最近在工作中一直使用 redis 来管理分发爬虫任务，让我对 scrapy-redis 有很深刻的理解，下面让我慢慢说来。 在所有的问题开始之前，要先有一个前提：你使用 Scrapy 框架做开发 结论scrapy-redis 与 Scrapy的关系就像电脑与固态硬盘一样，是电脑中的一个插件，能让电脑更快的运行。 Scrapy 是一个爬虫框架，scrapy-redis 则是这个框架上可以选择的插件，它可以让爬虫跑的更快。 为什么使用 scrapy-redis首先，在实际开发中，我们总会对爬虫速度表示不满，为啥这么慢，能不能跑快点。除了爬虫本身的优化，我们就要引入分布式爬虫的概念。 我自己对分布式爬虫的理解就是：多个爬虫执行同一个任务 这里说下，Scrapy本身是不支持分布式的，因为它的任务管理和去重全部是在机器内存中实现的。 在 Scrapy 中最出名的分布式插件就是scrapy-redis了，scrapy-redis的作用就是让你的爬虫快、更快、超级快。 scrapy-redis 如何工作最简单的方式是使用redis替换机器内存，那么具体如何操作呢？非常简单，你只需要在 settings.py 中加上三代码，就能让你的爬虫变为分布式。 12345SCHEDULER = \"scrapy_redis.scheduler.Scheduler\"DUPEFILTER_CLASS = \"scrapy_redis.dupefilter.RFPDupeFilter\"REDIS_START_URLS_AS_SET = True SCHEDULER 是任务分发与调度，把所有的爬虫开始的请求都放在redis里面，所有爬虫都去redis里面读取请求。DUPEFILTER_CLASS 是去重队列，负责所有请求的去重，REDIS_START_URLS_AS_SET指的是使用redis里面的set类型（简单完成去重），如果你没有设置，默认会选用list。 如果你现在运行你的爬虫，你可以在redis中看到出现了这两个key: 12spider_name:dupefilterspider_name:requests 格式是set，即不会有重复数据。前者就是redis的去重队列，对应DUPEFILTER_CLASS，后者是redis的请求调度，把里面的请求分发给爬虫，对应SCHEDULER。（里面的数据不会自动删除，如果你第二次跑，需要提前清空里面的数据） scrapy-redis 优点速度快scrapy-redis 使用redis这个速度非常快的非关系型（NoSQL）内存键值数据库，速度快是最重要原因（但是也会产生负面想过，下面会说到）。 为什么是scrapy-redis而不是scrapy-mongo呢，大家可以仔细想想。 用法简单前人已经造好轮子了，scrapy-redis。我们直接拿来用就好，而用法也像上面提到的在 settings.py 文件中配置。在文档中还有另一种用法，即Feeding a Spider from Redis run the spider:scrapy runspider myspider.py push urls to redis:redis-cli lpush myspider:start_urls http://google.com（建议把lpush换为zset） 其实这种用法就是先打开一个爬虫，他会一直在redis里面寻找key为 myspider:start_urls，如果存在，就提取里面的url。当然你也可以在爬虫中指定redis_key，默认的是爬虫的名字加上:start_urls 去重简单爬虫中去重是一件大事，使用了scrapy-redis后就很简单了。上面提到过使用redis的set类型就可以很容易达到这个目标了，即REDIS_START_URLS_AS_SET = True。 scrapy-redis 缺点内存问题为什么使用分布式爬虫，当然是因为会有很多链接需要跑，或者说会存放很多个myspider:start_urls到redis中，Redis是key-value数据库，面对key的内存搜索，优势明显，但是Redis吃的是纯内存，myspider:start_urls是一个有一个像https://www.zhihu.com/people/cuishite的链接，会占用大量的内存空间。之前就因为这个原因redis崩溃过无数次，那么如何优化？ 网络上有的方法是 scrapy_redis去重优化（已有7亿条数据），附Demo福利，可以参考下。如果你有好的解决方法，欢迎私信告诉我。（保密原因就不介绍我们的处理方法了） Usage这个其实不算做问题，只是官方文档上我觉得的小BUG，在这里 Usage 1234# Store scraped item in redis for post-processing.ITEM_PIPELINES = &#123; 'scrapy_redis.pipelines.RedisPipeline': 300&#125; Pipeline是这样写的1234567891011121314def _process_item(self, item, spider): key = self.item_key(item, spider) data = self.serialize(item) self.server.rpush(key, data) return itemdef item_key(self, item, spider): \"\"\"Returns redis key based on given spider. Override this function to use a different key depending on the item and/or spider. \"\"\" return self.key % &#123;'spider': spider.name&#125; 看不懂为什么要把数据储存在redis里面，这不又加大redis储存负担吗？对于新手来说真的不友好，或许可以考虑提一个pr。 redis可视化工具最后介绍两个redis可视化工具 RedisDesktopManager 比较出名的工具，但是经常会崩溃 kedis 国人开发的免费工具，这个界面还是可以的","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"scrapy","slug":"scrapy","permalink":"https://zhangslob.github.io/tags/scrapy/"}]},{"title":"来codewars与我一起玩耍吧","date":"2018-04-12T13:25:13.000Z","path":"2018/04/12/来codewars与我一起玩耍吧/","text":"这是崔斯特的第四十篇原创文章 并肩作战 (๑• . •๑) 先看一道题目如何使用代码表示“石头、剪刀、布”之间的关系。 即：石头 &gt; 剪刀，剪刀 &gt; 布， 剪刀 &gt; 布 当时我想了很多，构造一个字典，和数字对应，但是应该如何表示“大小”关系呢？想破脑袋都想不出来，最后看了答案，形如 dict = {&#39;a&#39;: &#39;b&#39;, &#39;b&#39;: &#39;c&#39;, &#39;c&#39;: &#39;a&#39;} 简直是妙啊！！！ 原题在这里，Rock Paper Scissors!，可以自己试试看。 我觉得很妙的解法 CodeWars这是CodeWars上的一题，我觉得挺有意思的。CodeWars其实和leetcode差不多，但是我更喜欢有这几点。 界面 看着挺舒服的，同时提供了测试代码。 够简单真的，CodeWars上有些题目真的很简单，适合我这种新手，哈哈，比如： 123456# Complete the solution so that it reverses the string value passed into it.## solution('world') # returns 'dlrow'def solution(string): return string[::-1] 还可以选择问题类型。 可以自己出题，还可以邀请队友www.codewars.com/r/UsAiUQcodewars 点一下，就可以成为我的盟友。 点一下，玩一年，装逼不花一分钱！ 可以上榜leaderboard 第二名竟然是国人唉，不知是哪位大佬。希望有更多中国人可以出现在上面。 后话目前我也还是一个萌新，希望大佬能带带我。 我在Github上开了一个仓库，codewars_python 里面都是用 python的解题方法，但是现在还只有几题而已，希望大家可以一起来参与，多提pr。 1234567891011121314151617181920212223# -*- coding: utf-8 -*-# ATM machines allow 4 or 6 digit PIN codes and PIN codes cannot contain anything but exactly 4 digits or exactly 6 digits.# If the function is passed a valid PIN string, return true, else return false.# eg:# validate_pin(\"1234\") == True# validate_pin(\"12345\") == False# validate_pin(\"a234\") == False# My Solutiuondef validate_pin(pin): #return true or false return pin.isdigit() if len(pin) == 4 or len(pin) == 6 else False # Wonderful Solutiuon def validate_pin(pin): return len(pin) in (4, 6) and pin.isdigit()","tags":[{"name":"算法","slug":"算法","permalink":"https://zhangslob.github.io/tags/算法/"},{"name":"codewars","slug":"codewars","permalink":"https://zhangslob.github.io/tags/codewars/"}]},{"title":"Scrapy中如何提高数据的插入速度","date":"2018-03-28T13:18:32.000Z","path":"2018/03/28/Scrapy中如何提高数据的插入速度/","text":"这是崔斯特的第三十九篇原创文章 长期更新 (๑• . •๑) 速度问题最近工作中遇到这么一个问题，全站抓取时采用分布式：爬虫A与爬虫B，爬虫A给爬虫B喂饼，爬虫B由于各种原因运行的比较慢，达不到预期效果，所以必须对爬虫B进行优化。 提升Scrapy运行速度有很多方法，国外有大佬说过 Speed up web scraper Here’s a collection of things to try: use latest scrapy version (if not using already) check if non-standard middlewares are used try to increase CONCURRENT_REQUESTS_PER_DOMAIN, CONCURRENT_REQUESTS settings (docs)turn off logging LOG_ENABLED = False (docs) try yielding an item in a loop instead of collecting items into the items list and returning themuse local cache DNS (see this thread) check if this site is using download threshold and limits your download speed (see this thread)log cpu and memory usage during the spider run - see if there are any problems there try run the same spider under scrapyd service see if grequests + lxml will perform better (ask if you need any help with implementing this solution) try running Scrapy on pypy, see Running Scrapy on PyPy 大致看了下，确实可以提高爬虫运行速度，但是对于海量数据（这里说的是百万级）还需要考虑一点的就是数据插入问题，这里我们使用的是 Mongo。 官方示例让我们先从官方文档开始 Write items to MongoDB 123456789101112131415161718192021222324252627import pymongoclass MongoPipeline(object): collection_name = 'scrapy_items' def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DATABASE', 'items') ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): self.db[self.collection_name].insert_one(dict(item)) return item 比较简单，这里插入使用的方法是 insert_one，继续文档： insert_one(document, bypass_document_validation=False, session=None) Insert a single document.1234567&gt;&gt;&gt; db.test.count(&#123;'x': 1&#125;)0&gt;&gt;&gt; result = db.test.insert_one(&#123;'x': 1&#125;)&gt;&gt;&gt; result.inserted_idObjectId('54f112defba522406c9cc208')&gt;&gt;&gt; db.test.find_one(&#123;'x': 1&#125;)&#123;u'x': 1, u'_id': ObjectId('54f112defba522406c9cc208')&#125; 以前经常使用的 insert 方法，已经不被赞同 insert(doc_or_docs, manipulate=True, check_keys=True, continue_on_error=False, **kwargs) Insert a document(s) into this collection.123DEPRECATED - Use insert_one() or insert_many() instead.Changed in version 3.0: Removed the safe parameter. Pass w=0 for unacknowledged write operations. insert 简单理解就是插入，把我们采集到的 item 插入到数据库，这样存在一个很严重的问题，就是去重 去重晚上有一种很流行的写法，使用 update命令，如： self.db[self.collection_name].update({&#39;id&#39;: item[&#39;id&#39;]}, {&#39;$set&#39;: dict(item)}, True) 解释为： 比较重要的一点就在于process_item，在这里使用了update方法，第一个参数传入查询条件，这里使用的是id，第二个参数传入字典类型的对象，就是我们的item，第三个参数传入True，这样就可以保证，如果查询数据存在的话就更新，不存在的话就插入。这样就可以保证去重了。 这确实是一种很简单的方法，其实原理很简单，就是在每次插入数据前，对数据库中查询，是否有该 ID，如果没有就插入，如果有就放弃。 对于数据量比较少的项目，这确实是一种很简单的方法，很简单就完成了目标。 但是，我们现在说的是百万级数据，如果每一条数据在插入前，都需要去查询该数据是否在数据库，那会多么耗时，效率会大大较低，那么还有什么好办法呢？ 索引MongoDB 索引索引能够实现高效地查询。没有索引，MongoDB 就必须扫描集合中的所有文档，才能找到匹配查询语句的文档。这种扫描毫无效率可言，需要处理大量的数据。 索引是一种特殊的数据结构，将一小块数据集保存为容易遍历的形式。索引能够存储某种特殊字段或字段集的值，并按照索引指定的方式将字段值进行排序。 我们可以借助索引，使用 insert_one方法提高效率。代码实现： 123456789101112131415161718class MongoDBPipeline(object): def open_spider(self, spider): self.client = mongodb_client self.db = self.client.get_database() self.collection = self.db['test'] # 添加唯一索引 self.collection.create_index('id', unique=True) def close_spider(self, spider): self.client.close() def process_item(self, item, spider): try: self.collection.insert_one(dict(item)) return item except DuplicateKeyError: spider.logger.debug(' duplicate key error collection') return item 其实很简单，就是在 open_spider先创建唯一索引，然后再插入数据。注意需要在process_item中使用异常处理，因为很有可能插入重复数据，到时候就会输出日志。 其他方法mongo 除了 insert_one方法还有一种，insert_many insert_many(documents, ordered=True, bypass_document_validation=False, session=None) Insert an iterable of documents.1234567&gt;&gt;&gt; db.test.count()0&gt;&gt;&gt; result = db.test.insert_many([&#123;'x': i&#125; for i in range(2)])&gt;&gt;&gt; result.inserted_ids[ObjectId('54f113fffba522406c9cc20e'), ObjectId('54f113fffba522406c9cc20f')]&gt;&gt;&gt; db.test.count()2 这样插入的数据不再是一条，而是很多， What’s the difference between insert(), insertOne() and insertMany() methods on MongoDB 大佬有写到，可以去看看。 同时插入多条数据，减轻数据库压力。但是这个“多”到底还是多少，目前不得而知。 结语除了更多机器和更多节点，还有很多方法可以提升 Scrapy运行速度。 今天说到的是管道阻塞问题，还有其他地方也可以优化，还需要努力。","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"Scrapy","slug":"Scrapy","permalink":"https://zhangslob.github.io/tags/Scrapy/"}]},{"title":"Hi，这里是我的爬虫笔记","date":"2018-03-25T13:37:15.000Z","path":"2018/03/25/Hi，这里是我的爬虫笔记/","text":"这是崔斯特的第三十八篇原创文章 长期更新 (๑• . •๑) 平时有个习惯，会把自己的笔记写在有道云里面，现在做个整理。会长期更新，因为我是BUG制造机。 解析xpath提取所有节点文本1&lt;div id=&quot;test3&quot;&gt;我左青龙，&lt;span id=&quot;tiger&quot;&gt;右白虎，&lt;ul&gt;上朱雀，&lt;li&gt;下玄武。&lt;/li&gt;&lt;/ul&gt;老牛在当中，&lt;/span&gt;龙头在胸口。&lt;div&gt; 使用xpath的string(.) 123456789101112#!/usr/bin/env python# -*- coding: utf-8 -*-from scrapy.selector import Selectortext = '&lt;div id=\"test3\"&gt;我左青龙，&lt;span id=\"tiger\"&gt;右白虎，&lt;ul&gt;上朱雀，&lt;li&gt;下玄武。&lt;/li&gt;&lt;/ul&gt;老牛在当中，&lt;/span&gt;龙头在胸口。&lt;div&gt;'s = Selector(text=text)data = s.xpath('//div[@id=\"test3\"]')info = data.xpath('string(.)').extract()[0]print(info)# output: 我左青龙，右白虎，上朱雀，下玄武。老牛在当中，龙头在胸口。 如何解决详情页面元素改变这个问题是这样产生的，在很多PC站，比如链家，这个页面有这些字段A，但是下个页面这个字段A没了，取而代之的是字段B，在xpath定位时就失效了。这个问题很常见，大体思路是这样的。 创建一个包含所有字段的dict: data = {}.fromkeys((&#39;url&#39;, &#39;price&#39;, &#39;address&#39;)) 然后根据网页中是否有字段来取值，例如，有’url’就取对应的value，没有则为空 这样就可以完美解决匹配不全问题 Scrapy 相关文件编写逻辑文件和解析部分分开写，匹配文件目录是utils/parse/，爬虫文件目录是spiders/ Scrapy 中文乱码在 setting 文件中设置：FEED_EXPORT_ENCODING = &#39;utf-8&#39; Scrapy 使用Mongopipelines.py 首先我们要从settings文件中读取数据的地址、端口、数据库名称。 拿到数据库的基本信息后进行连接。 将数据写入数据库（update制定唯一键） 关闭数据库 注意：只有打开和关闭是只执行一次，而写入操作会根据具体的写入次数而定。Redis 无需关闭12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import pymongo class MongoDBPipeline(object): \"\"\" 1、连接数据库操作 \"\"\" def __init__(self,mongourl,mongoport,mongodb): ''' 初始化mongodb数据的url、端口号、数据库名称 :param mongourl: :param mongoport: :param mongodb: ''' self.mongourl = mongourl self.mongoport = mongoport self.mongodb = mongodb @classmethod def from_crawler(cls,crawler): \"\"\" 1、读取settings里面的mongodb数据的url、port、DB。 :param crawler: :return: \"\"\" return cls( mongourl = crawler.settings.get(\"MONGO_URL\"), mongoport = crawler.settings.get(\"MONGO_PORT\"), mongodb = crawler.settings.get(\"MONGO_DB\") ) def open_spider(self,spider): ''' 1、连接mongodb数据 :param spider: :return: ''' self.client = pymongo.MongoClient(self.mongourl,self.mongoport) self.db = self.client[self.mongodb] def process_item(self,item,spider): ''' 1、将数据写入数据库 :param item: :param spider: :return: ''' name = item.__class__.__name__ # self.db[name].insert(dict(item)) self.db['user'].update(&#123;'url_token':item['url_token']&#125;,&#123;'$set':item&#125;,True) return item def close_spider(self,spider): ''' 1、关闭数据库连接 :param spider: :return: ''' self.client.close() scrapy图片下载12345678910111213141516import scrapyfrom scrapy.pipelines.images import ImagesPipelinefrom scrapy.exceptions import DropItemclass MyImagesPipeline(ImagesPipeline): def get_media_requests(self, item, info): for image_url in item['image_urls']: yield scrapy.Request(image_url) def item_completed(self, results, item, info): image_paths = [x['path'] for ok, x in results if ok] if not image_paths: raise DropItem(\"Item contains no images\") item['image_paths'] = image_paths return item scrapy 暂停爬虫scrapy crawl somespider -s JOBDIR=crawls/somespider-1 scrapy_redis 分布式使用队列与去重即可完成分布式需求，需要注意的是 Redis 格式，默认采用的是 list， 可以在 settings.py 文件中设置 REDIS_START_URLS_AS_SET = True，使用 Redis的 set类型（去重种子链接） 安装超时问题自定义超时时间 sudo pip3 --default-timeout=100 install -U scrapy 或者 使用其他源 sudo pip3 install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple 权限问题安装某模块时，报错：PermissionError: [WinError 5] 拒绝访问。: &#39;c:\\\\program files\\\\python35\\\\Lib\\\\sit e-packages\\\\lxml&#39; 最简单方法：pip install --user lxml Pycharm 相关.gitignore 文件安装插件： Preferences &gt; Plugins &gt; Browse repositories... &gt; Search for &quot;.ignore&quot; &gt; Install Plugin 然后就可以很方便的添加到 .gitignore 显示函数 点击 Show Members，查看目录，会显示相应的类和函数 激活码 http://idea.liyang.io http://xidea.online 数据Mongo导出命令λ mongoexport -d test -c set --type=csv -f name,age -o set.csv λ mongoexport -h 10.10.10.11 -d test -c test --type=csv -f url,id,title -o data.csv 其他requirements.txt 文件小提示：使用 pigar 可以一键生成 requirements.txt 文件 Installation：pip install pigar Usage：pigar 好了，今天先写这点，以后再补上。","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"}]},{"title":"学习Git（二）基本操作","date":"2018-03-19T13:11:39.000Z","path":"2018/03/19/学习Git（二）基本操作/","text":"这是崔斯特的第三十七篇原创文章 继续补基础 (๑• . •๑) Git 基础操作1. 创建版本库什么是版本库呢？版本库又名仓库，英文名 repository，你可以简单理解成一个目录，这个目录里面的所有文件都可以被 Git 管理起来，每个文件的修改、删除，Git 都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻可以“还原”。 所以，创建一个版本库非常简单，首先，选择一个合适的地方，创建一个空目录： 1234$ mkdir learngit$ cd learngit$ pwd/Users/learngit pwd命令用于显示当前目录。 如果你使用 Windows 系统，为了避免遇到各种莫名其妙的问题，请确保目录名（包括父目录）不包含中文。 第二步，通过git init命令把这个目录变成 Git 可以管理的仓库： 12$ git initInitialized empty Git repository in /Users/learngit/.git/ 瞬间 Git 就把仓库建好了，而且告诉你是一个空的仓库（empty Git repository） 用ls -ah命令就可以看见当前目录下多了一个.git的目录，这个目录是 Git 来跟踪管理版本库的，没事千万不要手动修改这个目录里面的文件，不然改乱了，就把 Git 仓库给破坏了。 一定要放到 learngit 目录下或子目录下 1$ git status # 随时用git status 查看文件状态 一个文件放到 Git 仓库只需要两步。 用命令git add告诉 Git，把文件添加到仓库： 1$ git add . # 把所有文件都添加到仓库 执行上面的命令，没有任何显示，这就对了，Unix 的哲学是“没有消息就是好消息”，说明添加成功。（因为没有添加任何文件，如果添加结果不同，可以使用 git status 随时查看 Git 状态） 用命令git commit告诉 Git，把文件提交到仓库： 1234$ git commit abc/aaa.py -m&quot;chore:wrote a readme file&quot;[master (root-commit) cb926e7] wrote a readme file 1 file changed, 2 insertions(+) create mode 100644 aaa.py commit 必须遵循commit规范123456789**git commit规范**- feat：新功能（feature）- fix：修补bug- docs：文档（documentation）- style： 格式（不影响代码运行的变动）- refactor：重构（即不是新增功能，也不是修改bug的代码变动）- test：增加测试- chore：构建过程或辅助工具的变动 2. 新机器配置 Git 创建 SSH Key。在用户主目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有id_rsa和id_rsa.pub这两个文件，如果已经有了，可直接跳到第2步。如果没有，打开 Shell（Windows 下打开 Git Bash），创建 SSH Key： 1$ ssh-keygen -t rsa -C &quot;你的github邮箱&quot; 把邮件地址换成你自己的邮件地址，一路回车，使用默认值即可，无需设置密码。 如果一切顺利的话，可以在用户主目录里找到.ssh目录，里面有 id_rsa 和 id_rsa.pub 两个文件，这两个就是 SSH Key 的秘钥对，id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。 登陆 GitHub，打开“Account settings”，“SSH Keys”页面： 然后，点“Add SSH Key”，填上任意 Title，在 Key 文本框里粘贴id_rsa.pub文件的内容： 点“Add Key”，你就应该看到已经添加的 Key 当然，GitHub 允许你添加多个 Key。假定你有若干电脑，你一会儿在公司提交，一会儿在家里提交，只要把每台电脑的 Key 都添加到 GitHub，就可以在每台电脑上往 GitHub 推送了。 3. 关联远程库 如果公司已创建该项目的远程库，本地还没有，clone 该项目地址: clone with ssh 1$ git clone git@github.com:xxxx/xxx.git SSH 警告 当你第一次使用 Git 的 clone 或者 push 命令连接 GitHub 时，会得到一个警告： 123The authenticity of host &apos;github.com (xx.xx.xx.xx)&apos; can&apos;t be established.RSA key fingerprint is xx.xx.xx.xx.xx.Are you sure you want to continue connecting (yes/no)? 这是因为 Git 使用 SSH 连接，而 SSH 连接在第一次验证 GitHub 服务器的 Key 时，需要你确认 GitHub 的 Key 的指纹信息是否真的来自 GitHub 的服务器，输入 yes 回车即可。 Git 会输出一个警告，告诉你已经把 GitHub 的 Key 添加到本机的一个信任列表里了： 1Warning: Permanently added &apos;github.com&apos; (RSA) to the list of known hosts. 这个警告只会出现一次，后面的操作就不会有任何警告了。 ​ 如果已经在本地创建了一个 Git 仓库后，公司也已在 GitHub 创建一个 Git 仓库， 实现让这两个仓库进行远程同步 1$ git remote add origin git@github.com:xxxx/xxxx.git 下一步，就可以把本地库的所有内容推送到远程库上 1$ git push -u origin master 把本地库的内容推送到远程，用git push命令，实际上是把当前分支 master 推送到远程。 由于远程库是空的，我们第一次推送 master 分支时，加上了-u参数，Git 不但会把本地的 master 分支内容推送的远程新的 master 分支，还会把本地的 master 分支和远程的 master 分支关联起来，在以后的推送或者拉取时就可以简化命令。 推送成功后，可以立刻在 GitHub 页面中看到远程库的内容已经和本地一模一样 从现在起，只要本地作了提交，就可以通过命令： $ git push origin master 把本地master分支的最新修改推送至GitHub 一般我们在develop分支开发 如果github上没有develop分支 首先，我们在本地创建 develop 分支，然后切换到 develop 分支： 12$ git checkout -b developSwitched to a new branch &apos;develop&apos; git checkout命令加上-b参数表示创建并切换，相当于以下两条命令： 123$ git branch develop$ git checkout developSwitched to branch &apos;develop&apos; 然后，用git branch命令查看当前分支： 123$ git branch* develop master git branch命令会列出所有分支，当前分支前面会标一个*号 发布develop分支 发布dev分支指的是同步develop分支的代码到远程服务器 12git push origin develop:develop # 这样远程仓库也有一个develop分支了 或者git push origin develop # 这两种应该都可以 如果github已经有master分支和develop分支 在本地 git checkout -b develop 新建并切换到本地develop分支 git pull origin develop 本地develop分支与远程develop分支相关联 Git 总结1234567891011121314151617git add . # 添加所有改动的文件到仓库git commit 文件路径 -m&apos;fix:修复xx bug&apos;# github上已经有master分支 和dev分支在本地git checkout -b dev # 创建+切换分支devgit pull origin dev # 本地分支与远程分支相关联dev# github无dev分支，在本地新建分支并推送到远程git checkout -b devgit push origin dev:dev # 这样远程仓库中也就创建了一个dev分支git branch # 查看本地有多少分支git branch 分支名字 # 创建分支git checkout dev # 切换到dev分支进行开发git push # 提交到远程git branch -d dev # 删除本地dev分支git merge dev # 合并dev到当前分支(master) git remote 深入研究 git-remote - Manage set of tracked repositories 12345678910111213git remote [-v | --verbose]git remote add [-t &lt;branch&gt;] [-m &lt;master&gt;] [-f] [--[no-]tags] [--mirror=&lt;fetch|push&gt;] &lt;name&gt; &lt;url&gt;git remote rename &lt;old&gt; &lt;new&gt;git remote remove &lt;name&gt;git remote set-head &lt;name&gt; (-a | --auto | -d | --delete | &lt;branch&gt;)git remote set-branches [--add] &lt;name&gt; &lt;branch&gt;…​git remote get-url [--push] [--all] &lt;name&gt;git remote set-url [--push] &lt;name&gt; &lt;newurl&gt; [&lt;oldurl&gt;]git remote set-url --add [--push] &lt;name&gt; &lt;newurl&gt;git remote set-url --delete [--push] &lt;name&gt; &lt;url&gt;git remote [-v | --verbose] show [-n] &lt;name&gt;…​git remote prune [-n | --dry-run] &lt;name&gt;…​git remote [-v | --verbose] update [-p | --prune] [(&lt;group&gt; | &lt;remote&gt;)…​] 查看远程仓库如果想查看你已经配置的远程仓库服务器，可以运行 git remote 命令。 它会列出你指定的每一个远程服务器的简写。 如果你已经克隆了自己的仓库，那么至少应该能看到 origin - 这是 Git 给你克隆的仓库服务器的默认名字： 12345678910$ git clone https://github.com/schacon/ticgitCloning into &apos;ticgit&apos;...remote: Reusing existing pack: 1857, done.remote: Total 1857 (delta 0), reused 0 (delta 0)Receiving objects: 100% (1857/1857), 374.35 KiB | 268.00 KiB/s, done.Resolving deltas: 100% (772/772), done.Checking connectivity... done.$ cd ticgit$ git remoteorigin 你也可以指定选项 -v，会显示需要读写远程仓库使用的 Git 保存的简写与其对应的 URL。123$ git remote -vorigin https://github.com/schacon/ticgit (fetch)origin https://github.com/schacon/ticgit (push) 如果你的远程仓库不止一个，该命令会将它们全部列出。 例如，与几个协作者合作的，拥有多个远程仓库的仓库看起来像下面这样： 123456789101112$ cd grit$ git remote -vbakkdoor https://github.com/bakkdoor/grit (fetch)bakkdoor https://github.com/bakkdoor/grit (push)cho45 https://github.com/cho45/grit (fetch)cho45 https://github.com/cho45/grit (push)defunkt https://github.com/defunkt/grit (fetch)defunkt https://github.com/defunkt/grit (push)koke git://github.com/koke/grit.git (fetch)koke git://github.com/koke/grit.git (push)origin git@github.com:mojombo/grit.git (fetch)origin git@github.com:mojombo/grit.git (push) 这样我们可以轻松拉取其中任何一个用户的贡献。 此外，我们大概还会有某些远程仓库的推送权限，虽然我们目前还不会在此介绍。 注意这些远程仓库使用了不同的协议；我们将会在 在服务器上搭建 Git 中了解关于它们的更多信息。 添加远程仓库运行 git remote add &lt;shortname&gt; &lt;url&gt; 添加一个新的远程 Git 仓库，同时指定一个你可以轻松引用的简写：12345678$ git remoteorigin$ git remote add pb https://github.com/paulboone/ticgit$ git remote -vorigin https://github.com/schacon/ticgit (fetch)origin https://github.com/schacon/ticgit (push)pb https://github.com/paulboone/ticgit (fetch)pb https://github.com/paulboone/ticgit (push) 现在你可以在命令行中使用字符串 pb 来代替整个 URL。 例如，如果你想拉取 Paul 的仓库中有但你没有的信息，可以运行 git fetch pb：12345678$ git fetch pbremote: Counting objects: 43, done.remote: Compressing objects: 100% (36/36), done.remote: Total 43 (delta 10), reused 31 (delta 5)Unpacking objects: 100% (43/43), done.From https://github.com/paulboone/ticgit * [new branch] master -&gt; pb/master * [new branch] ticgit -&gt; pb/ticgit 现在 Paul 的 master 分支可以在本地通过 pb/master 访问到 - 你可以将它合并到自己的某个分支中，或者如果你想要查看它的话，可以检出一个指向该点的本地分支。 ##从远程仓库中抓取与拉取就如刚才所见，从远程仓库中获得数据，可以执行： $ git fetch [remote-name]这个命令会访问远程仓库，从中拉取所有你还没有的数据。 执行完成后，你将会拥有那个远程仓库中所有分支的引用，可以随时合并或查看。 如果你使用 clone 命令克隆了一个仓库，命令会自动将其添加为远程仓库并默认以 “origin” 为简写。 所以，git fetch origin 会抓取克隆（或上一次抓取）后新推送的所有工作。 必须注意 git fetch 命令会将数据拉取到你的本地仓库 - 它并不会自动合并或修改你当前的工作。 当准备好时你必须手动将其合并入你的工作。 如果你有一个分支设置为跟踪一个远程分支，可以使用 git pull 命令来自动的抓取然后合并远程分支到当前分支。 这对你来说可能是一个更简单或更舒服的工作流程；默认情况下，git clone 命令会自动设置本地 master 分支跟踪克隆的远程仓库的 master 分支（或不管是什么名字的默认分支）。 运行 git pull 通常会从最初克隆的服务器上抓取数据并自动尝试合并到当前所在的分支。 推送到远程仓库当你想分享你的项目时，必须将其推送到上游。 这个命令很简单：git push [remote-name] [branch-name]。 当你想要将 master 分支推送到 origin 服务器时（再次说明，克隆时通常会自动帮你设置好那两个名字），那么运行这个命令就可以将你所做的备份到服务器： $ git push origin master只有当你有所克隆服务器的写入权限，并且之前没有人推送过时，这条命令才能生效。 当你和其他人在同一时间克隆，他们先推送到上游然后你再推送到上游，你的推送就会毫无疑问地被拒绝。 你必须先将他们的工作拉取下来并将其合并进你的工作后才能推送。 查看远程仓库如果想要查看某一个远程仓库的更多信息，可以使用 git remote show [remote-name] 命令。 如果想以一个特定的缩写名运行这个命令，例如 origin，会得到像下面类似的信息：123456789101112$ git remote show origin* remote origin Fetch URL: https://github.com/schacon/ticgit Push URL: https://github.com/schacon/ticgit HEAD branch: master Remote branches: master tracked dev-branch tracked Local branch configured for &apos;git pull&apos;: master merges with remote master Local ref configured for &apos;git push&apos;: master pushes to master (up to date) 它同样会列出远程仓库的 URL 与跟踪分支的信息。 这些信息非常有用，它告诉你正处于 master 分支，并且如果运行 git pull，就会抓取所有的远程引用，然后将远程 master 分支合并到本地 master 分支。 它也会列出拉取到的所有远程引用。 这是一个经常遇到的简单例子。 如果你是 Git 的重度使用者，那么还可以通过 git remote show 看到更多的信息。1234567891011121314151617181920$ git remote show origin* remote origin URL: https://github.com/my-org/complex-project Fetch URL: https://github.com/my-org/complex-project Push URL: https://github.com/my-org/complex-project HEAD branch: master Remote branches: master tracked dev-branch tracked markdown-strip tracked issue-43 new (next fetch will store in remotes/origin) issue-45 new (next fetch will store in remotes/origin) refs/remotes/origin/issue-11 stale (use &apos;git remote prune&apos; to remove) Local branches configured for &apos;git pull&apos;: dev-branch merges with remote dev-branch master merges with remote master Local refs configured for &apos;git push&apos;: dev-branch pushes to dev-branch (up to date) markdown-strip pushes to markdown-strip (up to date) master pushes to master (up to date) 这个命令列出了当你在特定的分支上执行 git push 会自动地推送到哪一个远程分支。 它也同样地列出了哪些远程分支不在你的本地，哪些远程分支已经从服务器上移除了，还有当你执行 git pull 时哪些分支会自动合并。 远程仓库的移除与重命名如果想要重命名引用的名字可以运行 git remote rename 去修改一个远程仓库的简写名。 例如，想要将 pb 重命名为 paul，可以用 git remote rename 这样做：1234$ git remote rename pb paul$ git remoteoriginpaul 值得注意的是这同样也会修改你的远程分支名字。 那些过去引用 pb/master 的现在会引用 paul/master。 如果因为一些原因想要移除一个远程仓库 - 你已经从服务器上搬走了或不再想使用某一个特定的镜像了，又或者某一个贡献者不再贡献了 - 可以使用 git remote rm ：123$ git remote rm paul$ git remoteorigin 回顾：学习Git（一）起步","tags":[{"name":"Git","slug":"Git","permalink":"https://zhangslob.github.io/tags/Git/"}]},{"title":"学习Git（一）起步","date":"2018-03-14T14:52:36.000Z","path":"2018/03/14/学习Git（一）起步/","text":"这是崔斯特的第三十六篇原创文章 开始补基础 (๑• . •๑) 什么是Git在Git官网上找到这样一段描述 Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. 重点是 version control system (VCS)，翻译过来也就是 版本控制系统。（Pycharm中有一个） 关于版本控制版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。 （我的理解就是一个东西会有不同的版本，就像吃鸡，常常更新，每更新一次就是一个新的版本，如果刚发行的版本出现毒圈不掉血，这个时候就需要版本回退。。版本控制系统就体现作用了） 版本控制系统又分为：本地版本控制系统、集中化的版本控制系统、分布式版本控制系统。 本地版本控制系统 集中化的版本控制系统 分布式版本控制系统 更多介绍可以来看 1.1 起步 - 关于版本控制 谁开发了 Git2002 年，Linux 内核开源项目组开始启用一个专有的分布式版本控制系统 BitKeeper 来管理和维护代码。 但是到了 2005 年，开发 BitKeeper 的商业公司同 Linux 内核开源社区的合作关系结束，他们收回了 Linux 内核社区免费使用 BitKeeper 的权力。 这就迫使 Linux 开源社区（特别是 Linux 的缔造者 Linus Torvalds）基于使用 BitKeeper 时的经验教训，开发出自己的版本系统。 他们对新的系统制订了若干目标： 速度 简单的设计 对非线性开发模式的强力支持（允许成千上万个并行开发的分支） 完全分布式 有能力高效管理类似 Linux 内核一样的超大规模项目（速度和数据量） Git 特点Git 工作流基本的 Git 工作流程如下： 克隆 Git 资源作为工作目录。 在克隆的资源上添加或修改文件。 如果其他人修改了，你可以更新资源。 在提交前查看修改。 提交修改。 在修改完成后，如果发现错误，可以撤回提交并再次修改并提交。 近乎所有操作都是本地执行在 Git 中的绝大多数操作都只需要访问本地文件和资源，一般不需要来自网络上其它计算机的信息。 举个例子，要浏览项目的历史，Git 不需外连到服务器去获取历史，然后再显示出来——它只需直接从本地数据库中读取。 你能立即看到项目历史。 如果你想查看当前版本与一个月前的版本之间引入的修改，Git 会查找到一个月前的文件做一次本地的差异计算，而不是由远程服务器处理或从远程服务器拉回旧版本文件再来本地处理。 Git 保证完整性Git 中所有数据在存储前都计算校验和，然后以校验和来引用。 这意味着不可能在 Git 不知情时更改任何文件内容或目录内容。 这个功能建构在 Git 底层，是构成 Git 哲学不可或缺的部分。 若你在传送过程中丢失信息或损坏文件，Git 就能发现。 Git 用以计算校验和的机制叫做 SHA-1 散列（hash，哈希）。 这是一个由 40 个十六进制字符（0-9 和 a-f）组成字符串，基于 Git 中文件的内容或目录结构计算出来。 SHA-1 哈希看起来是这样： 24b9da6552252987aa493b52f8696cd6d3b00373 Git 中使用这种哈希值的情况很多，你将经常看到这种哈希值。 实际上，Git 数据库中保存的信息都是以文件内容的哈希值来索引，而不是文件名。 Git 一般只添加数据你执行的 Git 操作，几乎只往 Git 数据库中增加数据。 很难让 Git 执行任何不可逆操作，或者让它以任何方式清除数据。 同别的 VCS 一样，未提交更新时有可能丢失或弄乱修改的内容；但是一旦你提交快照到 Git 中，就难以再丢失数据，特别是如果你定期的推送数据库到其它仓库的话。 Git 的三种状态 已提交（committed）：数据已经安全的保存在本地数据库中。 已修改（modified） ：修改了文件，但还没保存到数据库中。 已暂存（staged） ：对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。 由此引入 Git 项目的三个工作区域的概念：Git 仓库、工作目录以及暂存区域。 安装 Git说了那么多，现在开始动手，安装Git。 Linux$ sudo yum install git 或者 $ sudo apt-get install git Mac下载安装包 Downloading Git 或者 $ brew install git Windows直接下载文件 Downloading Git 或者 安装 GitHub for Windows 初次运行 Git 前的配置用户信息12$ git config --global user.name &quot;name&quot;$ git config --global user.email example@example.com 如果使用了 --global 选项，那么该命令只需要运行一次，因为之后无论你在该系统上做任何事情， Git 都会使用那些信息。 检查配置信息12345678$ git config --listuser.name=nameuser.email=example@example.comcolor.status=autocolor.branch=autocolor.interactive=autocolor.diff=auto... 获取帮助若你使用 Git 时需要获取帮助，有三种方法可以找到 Git 命令的使用手册：123$ git help &lt;verb&gt;$ git &lt;verb&gt; --help$ man git-&lt;verb&gt; 例如，要想获得 config 命令的手册，执行 $ git help config 总结以前只是了解 Git常见的push、commit等命令，从没有对 Git进性过系统学习，抓紧时间学习。 目标：两周后，也就是3月28日，能熟练使用 Git进性开发。 这一篇主要讲了些背景知识吧，下一篇就是实际的操作了。 主要从Git文档上截取自己认为重要的部分，有条件可以去仔细阅读。","tags":[{"name":"Git","slug":"Git","permalink":"https://zhangslob.github.io/tags/Git/"}]},{"title":"大佬，我代码哪错了？","date":"2018-03-13T13:30:03.000Z","path":"2018/03/13/大佬，我代码哪错了？/","text":"这是崔斯特的第三十五篇原创文章 到底哪错了 (๑• . •๑) 问题无处不在我： “大佬，帮我看看这个问题错在哪了？” 大佬： “你的代码呢、你的错误提示呢？” 我： “好的，我发给你” 大佬： “。。。 再见” 留下一脸懵逼的我 如何解决问题最简单的方法是根据错误提示，查看对应位置的代码，Pycharm会提示具体是哪一行代码有问题，并抛出错误。 找到错误首先应自己想办法解决，自己解决又分为几种：有经验的大佬看到Error就知道是哪里有问题了，没经验怎么办，那就自己去搜索了。 下个结论，你遇到的问题，前人一定遇到过。 所以你只需要把你的错误提示搜索就好了。 注意下面，你懂的。（给自己一个好点的环境） Google &gt; 百度 stackoverflow &gt; 知乎 github &gt; CSDN Github上有一个很有用的 trending ，可以显示今天或本周或本月最火的项目，例如： techGay/91porn Github 作为全球最大的同性交友网站，上面有很多值得我们好好学习的东西。 如何向别人提问如果你真的像上面哪样，发一张照片，估计大佬都要吐血了。 在这里推荐大家使用Github上的 gist ，很适合分享代码片段。 Instantly share code, notes, and snippets. 例如这样 Issue in FromRequest #3144 这样看起来是不是舒服多了 如何才能 永无bug1、佛祖保佑 永无bug12345678910111213141516171819202122232425262728293031// _ooOoo_// o8888888o// 88&quot; . &quot;88// (| -_- |)// O\\ = /O// ____/`---&apos;\\____// . &apos; \\\\| |// `.// / \\\\||| : |||// \\// / _||||| -:- |||||- \\// | | \\\\\\ - /// | |// | \\_| &apos;&apos;\\---/&apos;&apos; | |// \\ .-\\__ `-` ___/-. /// ___`. .&apos; /--.--\\ `. . __// .&quot;&quot; &apos;&lt; `.___\\_&lt;|&gt;_/___.&apos; &gt;&apos;&quot;&quot;.// | | : `- \\`.;`\\ _ /`;.`/ - ` : | |// \\ \\ `-. \\_ __\\ /__ _/ .-` / /// ======`-.____`-.___\\_____/___.-`____.-&apos;======// `=---=&apos;//// .............................................// 佛祖镇楼 BUG辟易// 佛曰:// 写字楼里写字间，写字间里程序员；// 程序人员写程序，又拿程序换酒钱。// 酒醒只在网上坐，酒醉还来网下眠；// 酒醉酒醒日复日，网上网下年复年。// 但愿老死电脑间，不愿鞠躬老板前；// 奔驰宝马贵者趣，公交自行程序员。// 别人笑我忒疯癫，我笑自己命太贱；// 不见满街漂亮妹，哪个归得程序员？ 2、佛系编程 永无bugNo CodeNo code is the best way to write secure and reliable applications. Write nothing; deploy nowhere. Getting StartedStart by not writing any code. 12 This is just an example application, but imagine it doing anything you want. Adding new features is easy too: 12 The possibilities are endless. Building the ApplicationNow that you have not done anything it’s time to build your application: 12 Yep. That’s it. You should see the following output: 12 DeployingWhile you still have not done anything it’s time to deploy your application. By running the following command you can deploy your application absolutely nowhere. 12 It’s that simple. And when it comes time to scale the application, all you have to do is: 12 I know right? ContributingYou don’t.","tags":[{"name":"编程","slug":"编程","permalink":"https://zhangslob.github.io/tags/编程/"}]},{"title":"学点算法之队列的学习及应用","date":"2018-03-06T12:54:00.000Z","path":"2018/03/06/学点算法之队列的学习及应用/","text":"这是崔斯特的第三十四篇原创文章 从约瑟夫问题开始说起 (๑• . •๑) 约瑟夫问题约瑟夫问题 有 n 个囚犯站成一个圆圈，准备处决。首先从一个人开始，越过k-2个人（因为第一个人已经被越过），并杀掉第k个人。接着，再越过 k-1个人，并杀掉第k个人。这个过程沿着圆圈一直进行，直到最终只剩下一个人留下，这个人就可以继续活着。 问题是，给定了n和k，一开始要站在什么地方才能避免被处决？ 这个问题是以弗拉维奥·约瑟夫命名的，它是1世纪的一名犹太历史学家。他在自己的日记中写道，他和他的40个战友被罗马军队包围在洞中。他们讨论是自杀还是被俘，最终决定自杀，并以抽签的方式决定谁杀掉谁。约瑟夫斯和另外一个人是最后两个留下的人。约瑟夫斯说服了那个人，他们将向罗马军队投降，不再自杀。约瑟夫斯把他的存活归因于运气或天意，他不知道是哪一个。 队列是什么这道题有多种解法，这里先不说别的，要引出今天的主角——队列。队列的定义很好理解： 队列是项的有序结合，其中添加新项的一端称为队尾，移除项的一端称为队首。当一个元素从队尾进入队列时，一直向队首移动，直到它成为下一个需要移除的元素为止。 队列抽象数据类型由以下结构和操作定义。如上所述，队列被构造为在队尾添加项的有序集合，并且从队首移除。队列保持 FIFO 排序属性。 队列操作如下。 Queue() 创建一个空的新队列。 它不需要参数，并返回一个空队列。 enqueue(item) 将新项添加到队尾。 它需要 item 作为参数，并不返回任何内容。 dequeue() 从队首移除项。它不需要参数并返回 item。 队列被修改。 isEmpty() 查看队列是否为空。它不需要参数，并返回布尔值。 size() 返回队列中的项数。它不需要参数，并返回一个整数。 队列的Python算法实现为了实现队列抽象数据类型创建一个新类 pythonds/basic/queue.py 123456789101112131415class Queue: def __init__(self): self.items = [] def isEmpty(self): return self.items == [] def enqueue(self, item): self.items.insert(0,item) def dequeue(self): return self.items.pop() def size(self): return len(self.items) 想明白了其实就是对 list 的简单操作 如何活到最后那我们回到上面的问题，如果是你，你要如何选择并活到最后呢？ 我们的程序将输入名称列表和一个称为 num 常量用于报数。它将返回以 num 为单位重复报数后剩余的最后一个人的姓名。 假设第一个人是a。从他开始计数，a将先出列再入队列，把他放在队列的最后。经过 num 次的出队入队后，前面的人将被永久移除队列。并且另一个周期开始，继续此过程，直到只剩下一个名字（队列的大小为 1）。 12345678910111213141516171819from pythonds.basic.queue import Queuedef hotPotato(namelist, num): simqueue = Queue() for name in namelist: simqueue.enqueue(name) while simqueue.size() &gt; 1: for i in range(num): simqueue.enqueue(simqueue.dequeue()) simqueue.dequeue() return simqueue.dequeue()print(hotPotato(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'], 7))# output: f 其他解法比较简单的做法是用循环单链表模拟整个过程，时间复杂度是O(n*m)。如果只是想求得最后剩下的人，则可以用数学推导的方式得出公式。先看看模拟过程的解法。 12345678910111213141516171819202122232425262728293031323334# -*- coding: utf-8 -*- class Node(object): def __init__(self, value): self.value = value self.next = Nonedef create_linkList(n): head = Node(1) pre = head for i in range(2, n+1): newNode = Node(i) pre.next= newNode pre = newNode pre.next = head return headn = 5 #总的个数m = 2 #数的数目if m == 1: #如果是1的话，特殊处理，直接输出 print(n) else: head = create_linkList(n) pre = None cur = head while cur.next != cur: #终止条件是节点的下一个节点指向本身 for i in range(m-1): pre = cur cur = cur.next print(cur.value) pre.next = cur.next cur.next = None cur = pre.next print(cur.value) 作业假设实验室里有一台打印机供学生共性。当学生向共享打印机发送打印任务时，任务被放置在队列中以便以先来先服务的方式被处理。如何才能通过python程序模拟的方式得到每次提交任务的平均等待时间呢？（平均等待时间不包括打印本身的时间，仅指在队列中排队的时间。）我们假定： 学生们每次打印的页数在1到20页之间。 打印机平均每小时会收到20个打印请求，即平均每180秒1个请求。 每秒新增任务的可能性相等，即任务的产生为独立同分布 打印机的打印速度恒定。 挖坑，要一起来填吗？","tags":[{"name":"算法","slug":"算法","permalink":"https://zhangslob.github.io/tags/算法/"},{"name":"队列","slug":"队列","permalink":"https://zhangslob.github.io/tags/队列/"}]},{"title":"Scrapy源码（2）——爬虫开始的地方","date":"2018-02-26T14:48:13.000Z","path":"2018/02/26/Scrapy源码（2）——爬虫开始的地方/","text":"这是崔斯特的第三十三篇原创文章 开始学习Scrapy源码 (๑• . •๑) Scrapy运行命令一般来说，运行Scrapy项目的写法有，（这里不考虑从脚本运行Scrapy） 1234567Usage examples:$ scrapy crawl myspider[ ... myspider starts crawling ... ]$ scrapy runspider myspider.py[ ... spider starts crawling ... ] 但是更好的写法是，新建一个Python文件，如下，（便于调试）123from scrapy import cmdlinecmdline.execute(\"scrapy crawl myspider\".split()) 很容易就发现，Scrapy运行文件是cmdline.py文件里面的execute()函数，下面学习下这个函数在做什么。 分析源码12345678910def execute(argv=None, settings=None): if argv is None: argv = sys.argv # --- backwards compatibility for scrapy.conf.settings singleton --- if settings is None and 'scrapy.conf' in sys.modules: from scrapy import conf if hasattr(conf, 'settings'): settings = conf.settings # ------------------------------------------------------------------ 寻找 scrapy.conf配置文件，argv直接取sys.argv 123456789101112131415161718if settings is None: settings = get_project_settings() # set EDITOR from environment if available try: editor = os.environ['EDITOR'] except KeyError: pass else: settings['EDITOR'] = editorcheck_deprecated_settings(settings)# --- backwards compatibility for scrapy.conf.settings singleton ---import warningsfrom scrapy.exceptions import ScrapyDeprecationWarningwith warnings.catch_warnings(): warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning) from scrapy import conf conf.settings = settings# ------------------------------------------------------------------ set EDITOR from environment if available 读取settings设置文件，导入项目，调用get_project_settings()函数，此处为utils文件夹下的project.py文件： 1234def get_project_settings(): if ENVVAR not in os.environ: project = os.environ.get('SCRAPY_PROJECT', 'default') init_env(project) project.py init_env() 函数如下： 12345678910111213def init_env(project='default', set_syspath=True): \"\"\"Initialize environment to use command-line tool from inside a project dir. This sets the Scrapy settings module and modifies the Python path to be able to locate the project module. \"\"\" cfg = get_config() if cfg.has_option('settings', project): os.environ['SCRAPY_SETTINGS_MODULE'] = cfg.get('settings', project) closest = closest_scrapy_cfg() if closest: projdir = os.path.dirname(closest) if set_syspath and projdir not in sys.path: sys.path.append(projdir) conf.py 如注释所说，初始化环境,循环递归找到用户项目中的配置文件settings.py,并且将其设置到环境变量Scrapy settings module中。然后修改Python路径，确保能找到项目模块。 123456789101112131415161718settings = Settings()settings_module_path = os.environ.get(ENVVAR)if settings_module_path: settings.setmodule(settings_module_path, priority='project')# XXX: remove this hackpickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\")if pickled_settings: settings.setdict(pickle.loads(pickled_settings), priority='project')# XXX: deprecate and remove this functionalityenv_overrides = &#123;k[7:]: v for k, v in os.environ.items() if k.startswith('SCRAPY_')&#125;if env_overrides: settings.setdict(env_overrides, priority='project')return settings project.py 至此，get_project_settings()该函数结束，如函数名字一样，最后返回项目配置，到此为止，接着往下看 1234567891011inproject = inside_project()cmds = _get_commands_dict(settings, inproject)cmdname = _pop_command_name(argv)parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), \\ conflict_handler='resolve')if not cmdname: _print_commands(settings, inproject) sys.exit(0)elif cmdname not in cmds: _print_unknown_command(settings, cmdname, inproject) sys.exit(2) 导入相应的module爬虫模块（inside_project） 执行环境是否在项目中，主要检查scrapy.cfg配置文件是否存在，读取commands文件夹，把所有的命令类转换为{cmd_name: cmd_instance}的字典 12345678cmd = cmds[cmdname]parser.usage = \"scrapy %s %s\" % (cmdname, cmd.syntax())parser.description = cmd.long_desc()settings.setdict(cmd.default_settings, priority='command')cmd.settings = settingscmd.add_options(parser)opts, args = parser.parse_args(args=argv[1:])_run_print_help(parser, cmd.process_options, args, opts) 根据命令名称找到对应的命令实例，设置项目配置和级别为command，添加解析规则，解析命令参数，并交由Scrapy命令实例处理。 最后，看看下面这段代码。 123cmd.crawler_process = CrawlerProcess(settings)_run_print_help(parser, _run_command, cmd, args, opts)sys.exit(cmd.exitcode) 初始化CrawlerProcess实例，将对应的命令执行，这里是crawl 12345def _run_command(cmd, args, opts): if opts.profile: _run_command_profiled(cmd, args, opts) else: cmd.run(args, opts) 看到这，想起了文档中的介绍 Run Scrapy from a script 123456789101112131415# Here’s an example showing how to run a single spider with it.import scrapyfrom scrapy.crawler import CrawlerProcessclass MySpider(scrapy.Spider): # Your spider definition ...process = CrawlerProcess(&#123; 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'&#125;)process.crawl(MySpider)process.start() # the script will block here until the crawling is finished 所以Scrapy爬虫运行都有用使用到CrawlerProcess，想要深入了解可以去看看源码 scrapy/scrapy/crawler.py 123456789101112131415161718192021\"\"\"A class to run multiple scrapy crawlers in a process simultaneously.This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding supportfor starting a Twisted `reactor`_ and handling shutdown signals, like thekeyboard interrupt command Ctrl-C. It also configures top-level logging.This utility should be a better fit than:class:`~scrapy.crawler.CrawlerRunner` if you aren't running anotherTwisted `reactor`_ within your application.The CrawlerProcess object must be instantiated with a:class:`~scrapy.settings.Settings` object.:param install_root_handler: whether to install root logging handler (default: True)This class shouldn't be needed (since Scrapy is responsible of using itaccordingly) unless writing scripts that manually handle the crawlingprocess. See :ref:`run-from-script` for an example.\"\"\" 最后，附上Scrapy的路径图 总结简单来说，有这么几步： 读取配置文件，应用到爬虫中 把所有的命令类转换名称与实例字典 初始化CrawlerProcess实例，运行爬虫 (看的头疼，好多函数名记不住) 回顾： Scrapy源码（1）——爬虫流程概览 Scrapy源码（2）——爬虫开始的地方","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"Scrapy","slug":"Scrapy","permalink":"https://zhangslob.github.io/tags/Scrapy/"}]},{"title":"Scrapy源码（1）——爬虫流程概览","date":"2018-02-24T13:16:23.000Z","path":"2018/02/24/Scrapy源码（1）——爬虫流程概览/","text":"这是崔斯特的第三十二篇原创文章 开始学习Scrapy源码 (๑• . •๑) 前言使用 Scrapy 已经有一段时间了，觉得自己有必要对源码好好的学习下了，所以写下记录，希望能加深自己的理解。 Scrapy | A Fast and Powerful Scraping and Web Crawling Framework 接下来说到的是最新版本： Scrapy 1.5，暂且把 Spider 称为 蜘蛛，而不是爬虫。 介绍Scrapy是一个开源爬虫框架，用于抓取网站并提取有用的结构化数据，如数据挖掘，信息处理或历史档案。 尽管Scrapy最初是为网页抓取设计的，但它也可以用于使用API（如Amazon Associates Web Services）或作为通用网络抓取工具提取数据。 一个最简单的例子，相信大家都写过 12345678910111213141516171819import scrapyclass QuotesSpider(scrapy.Spider): name = \"quotes\" start_urls = [ 'http://quotes.toscrape.com/tag/humor/', ] def parse(self, response): for quote in response.css('div.quote'): yield &#123; 'text': quote.css('span.text::text').extract_first(), 'author': quote.xpath('span/small/text()').extract_first(), &#125; next_page = response.css('li.next a::attr(\"href\")').extract_first() if next_page is not None: yield response.follow(next_page, self.parse) 一般来说，创建一个Scrapy项目需要如下流程： 使用scrapy startproject spider创建爬虫模板 爬虫类继承scrapy.Spider，重写parse方法和逻辑 parse方法中yield或return字典、Request、Item 自定义Item、Middlewares、Pipelines等 使用scrapy crawl &lt;spider_name&gt;或新建文件cmdline.execute(&quot;scrapy crawl spider_name&quot;.split())运行（便于调试） 其它 架构概述 这是一张非常经典的图，基本上说到Scrapy都会用到它，来源于Architecture overview 核心组件（Components） Scrapy Engine：引擎，负责控制系统所有组件之间的数据流，并在发生某些操作时触发事件； Scheduler：调度器，接收来自引擎的请求，并将它们排入队列，以便在引擎请求它们时将它们提供给它们（也提供给引擎）； Downloader：下载器，负责从网络上获取网页并将它们返回到引擎，然后引擎将它们返回给蜘蛛/spiders； Spiders：蜘蛛，是用户编写的自定义类，用于解析响应并从中提取项目（也称为抓取的项目）或追加其他请求； Item Pipeline：管道，负责输出结构化数据，可自定义输出位置，典型的任务包括清理，验证和持久性； Downloader middlewares：下载中间件，位于引擎和下载器之间的特定钩子/hooks，当它们从引擎传递到下载器时处理请求，以及从下载器传递到引擎的响应，常用于如下情况： 在将请求发送到下载器之前处理请求（即在Scrapy将请求发送到网站之前）; 在将其传递给蜘蛛之前改变接收到的响应; 发送新的请求，而不是将接收到的响应传递给蜘蛛; 向蜘蛛传递响应而不需要获取网页; 默默地放下一些请求。 Spider middlewares：Spider中间件，特定的钩子，位于引擎和蜘蛛之间，能够处理蜘蛛输入（响应）和输出（项目和请求），常用于如下情况： spider回调的后处理输出 更改/添加/删除请求或items; 后处理start_requests; 处理蜘蛛异常; 根据响应内容为一些请求调用errback而不是callback。 Event-driven networking：事件驱动的网络，Scrapy是用Twisted编写的，这是一个流行的事件驱动的Python网络框架。 因此，它使用非阻塞（又称异步）代码来实现并发。 Twisted is an event-driven networking engine written in Python and licensed under the open source ​MIT license. 数据流（Data flow）Scrapy中的数据流由执行引擎控制，如下所示： 引擎获取最初的请求从蜘蛛抓取（start_urls）。 引擎在调度程序中调度请求，并要求下一个请求进行采集。 调度器将下一个请求返回给引擎。 引擎将请求发送到下载器，通过下载器中间件。 一旦页面完成下载，Downloader会生成一个响应（包含该页面）并将其发送到引擎，并通过Downloader Middlewares。 引擎从Downloader收到响应并将其发送给Spider进行处理，并通过Spider Middleware传递。 Spider处理响应，并通过Spider中间件将抓取的项目和新的请求（后续）返回给引擎。 引擎将处理后的项目发送到项目管道，然后将处理后的请求发送到调度程序，并要求可能的下一个请求进行采集。 该过程重复（从第1步开始），直到调度器没有更多请求。 找到一张图，便于理解： 第一期差不多就到这了，没有说很多代码，主要是宏观上来观察 Scrapy 的架构，是如何运行。之后会更多的查看Scrapy的源代码，就近是如何采集数据的。 （内心有点小恐慌，不知道会写成什么样子。） 补充关于如何阅读项目源代码，找到一篇不错的文章，共享：如何阅读开源项目 主要是这几部分： 看：静态对代码进行分析，看相关资料，代码逻辑。 跑：将项目在IDE里面跑起来，通过IDE调试参数，加Log等。 查：阅读过程中肯定会遇到不懂的，这时候需要通过搜索引擎来解决你的疑惑。 回顾： Scrapy源码（1）——爬虫流程概览 Scrapy源码（2）——爬虫开始的地方","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"Scrapy","slug":"Scrapy","permalink":"https://zhangslob.github.io/tags/Scrapy/"}]},{"title":"PEP8规则及Pycharm应用","date":"2018-02-08T13:06:23.000Z","path":"2018/02/08/PEP8规则及Pycharm应用/","text":"这是崔斯特的第三十一篇原创文章 学习下 Python增强建议书 (๑• . •๑) PEP8 PEP是 Python Enhancement Proposal 的缩写，翻译过来就是 Python增强建议书 PEP8 是什么呢，简单说就是一种编码规范，是为了让代码“更好看”，更容易被阅读。 具体有这些规范，参考 PEP 8 – Style Guide for Python Code For example 1234567891011121314151617181920Yes: import os import sysNo: import sys, osYes:i = i + 1submitted += 1x = x*2 - 1hypot2 = x*x + y*yc = (a+b) * (a-b)No:i=i+1submitted +=1x = x * 2 - 1hypot2 = x * x + y * yc = (a + b) * (a - b) 对于初学者（例如我）来说，这些标准太多，在实际coding中可能很难记住所有的。之前招聘爬虫工程师，会PEP8也会加分，所以学习下PEP8还是有帮助的。 Pycharm 设置PyCharm 配置 PEP 8 代码提示 直接在右下角调整 Highlighting Level 为 Inspections 就能自动 PEP 8提示 (一般默认就是这个) For example 鼠标移到上方会提示： PEP 8: expected 2 blank lines, found 1，我们再增加一个空格就好了，你的代码中有任何不符合 PEP8 规范的地方都会有“~”提示 这是一个困扰了我很久的问题，我一直不知道是哪里出了错，看英文也不懂，最后去Google才知道，创建字典的写法不规范，应该这样写： item = dict() 其他工具除了Pycharm，还有其他工具可以提示 Google 开源的 Python 文件格式化工具：github.com/google/yapf pyflakes, pylint 等工具及各种编辑器的插件 正文好吧，这里才是正文，正确的规范如下： 缩进 不要使用 tab 缩进 使用任何编辑器写 Python，请把一个 tab 展开为 4 个空格 绝对不要混用 tab 和空格，否则容易出现 IndentationError 空格 在 list, dict, tuple, set, 参数列表的 , 后面加一个空格 在 dict 的 : 后面加一个空格 在注释符号 # 后面加一个空格，但是 #!/usr/bin/python 的 # 后不能有空格 操作符两端加一个空格，如 +, -, *, /, |, &amp;, = 接上一条，在参数列表里的 = 两端不需要空格 括号（(), {}, []）内的两端不需要空格 空行 function 和 class 顶上两个空行 class 的 method 之间一个空行 函数内逻辑无关的段落之间空一行，不要过度使用空行 不要把多个语句写在一行，然后用 ; 隔开 if/for/while 语句中，即使执行语句只有一句，也要另起一行 换行 每一行代码控制在 80 字符以内 使用 \\ 或 () 控制换行，举例：12345678def foo(first, second, third, fourth, fifth, sixth, and_some_other_very_long_param): user = User.objects.filter_by(first=first, second=second, third=third) \\ .skip(100).limit(100) \\ .all()text = ('Long strings can be made up ' 'of several shorter strings.') 命名 使用有意义的，英文单词或词组，绝对不要使用汉语拼音 package/module 名中不要出现 - 各种类型的命名规范： import 所有 import 尽量放在文件开头，在 docstring 下面，其他变量定义的上面 不要使用 from foo imort * import 需要分组，每组之间一个空行，每个分组内的顺序尽量采用字典序，分组顺序是： 标准库 第三方库 本项目的 package 和 module 不要使用隐式的相对导入（implicit relative imports），可是使用显示的相对导入（explicit relative imports），如 from ..utils import parse，最好使用全路径导入（absolute imports） 对于不同的 package，一个 import 单独一行，同一个 package/module 下的内容可以写一起： 12345678910# badimport sys, os, time# goodimport osimport sysimport time# okfrom flask import Flask, render_template, jsonify 为了避免可能出现的命名冲突，可以使用 as 或导入上一级命名空间 不要出现循环导入(cyclic import) 注释 文档字符串 docstring, 是 package, module, class, method, function 级别的注释，可以通过 doc 成员访问到，注释内容在一对 “”” 符号之间 function, method 的文档字符串应当描述其功能、输入参数、返回值，如果有复杂的算法和实现，也需要写清楚 不要写错误的注释，不要无谓的注释 12345# bad 无谓的注释x = x + 1 # increase x by 1# bad 错误的注释x = x - 1 # increase x by 1 优先使用英文写注释，英文不好全部写中文，否则更加看不懂 异常 不要轻易使用 try/except except 后面需要指定捕捉的异常，裸露的 except 会捕捉所有异常，意味着会隐藏潜在的问题 可以有多个 except 语句，捕捉多种异常，分别做异常处理 使用 finally 子句来处理一些收尾操作 try/except 里的内容不要太多，只在可能抛出异常的地方使用，如： 123456789101112131415161718192021222324252627282930# badtry: user = User() user.name = \"leon\" user.age = int(age) # 可能抛出异常 user.created_at = datetime.datetime.utcnow() db.session.add(user) db.session.commit() # 可能抛出异常except: db.session.rollback()# bettertry: age = int(age)except (TypeError, ValueError): return # 或别的操作user = User()user.name = \"leon\"user.age = ageuser.created_at = datetime.datetime.utcnow()db.session.add(user)try: db.session.commit()except sqlalchemy.exc.SQLAlchemyError: # 或者更具体的异常 db.session.rollback()finally: db.session.close() 从 Exception 而不是 BaseException 继承自定义的异常类 Class（类） 显示的写明父类，如果不是继承自别的类，就继承自 object 类 使用 super 调用父类的方法 支持多继承，即同时有多个父类，建议使用 Mixin 编码建议 字符串 使用字符串的 join 方法拼接字符串 使用字符串类型的方法，而不是 string 模块的方法 使用 startswith 和 endswith 方法比较前缀和后缀 使用 format 方法格式化字符串 比较 空的 list, str, tuple, set, dict 和 0, 0.0, None 都是 False 使用 if some_list 而不是 if len(some_list) 判断某个 list 是否为空，其他类型同理 使用 is 和 is not 与单例（如 None）进行比较，而不是用 == 和 != 使用 if a is not None 而不是 if not a is None 用 isinstance 而不是 type 判断类型 不要用 == 和 != 与 True 和 False 比较（除非有特殊情况，如在 sqlalchemy 中可能用到） 使用 in 操作： 用 key in dict 而不是 dict.has_key() 1234567# badif d.has_key(k): do_something()# goodif k in d: do_something() 2.用 set 加速 “存在性” 检查，list 的查找是线性的，复杂度 O(n)，set 底层是 hash table, 复杂度 O(1)，但用 set 需要比 list 更多内存空间 其他 使用列表表达式（list comprehension），字典表达式(dict comprehension, Python 2.7+) 和生成器(generator) dict 的 get 方法可以指定默认值，但有些时候应该用 [] 操作，使得可以抛出 KeyError 使用 for item in list 迭代 list, for index, item in enumerate(list) 迭代 list 并获取下标 使用内建函数 sorted 和 list.sort 进行排序 适量使用 map, reduce, filter 和 lambda，使用内建的 all, any 处理多个条件的判断 使用 defaultdict (Python 2.5+), Counter(Python 2.7+) 等 “冷门” 但好用的标准库算法和数据结构 使用装饰器(decorator) 使用 with 语句处理上下文 有些时候不要对类型做太过严格的限制，利用 Python 的鸭子类型（Duck Type）特性 使用 logging 记录日志，配置好格式和级别 了解 Python 的 Magic Method：A Guide to Python’s Magic Methods, Python 魔术方法指南 阅读优秀的开源代码，如 Flask 框架, Requests for Humans 不要重复造轮子，查看标准库、PyPi、Github、Google 等使用现有的优秀的解决方案 反思写程序的好习惯在于理解、坚持，然后就是运用自如！","tags":[{"name":"Pycharm","slug":"Pycharm","permalink":"https://zhangslob.github.io/tags/Pycharm/"},{"name":"PEP8","slug":"PEP8","permalink":"https://zhangslob.github.io/tags/PEP8/"}]},{"title":"学点算法之栈的学习与应用","date":"2018-02-05T14:08:13.000Z","path":"2018/02/05/学点算法之栈的学习与应用/","text":"这是崔斯特的第三十篇原创文章 在学习栈前，脑海中对这个词只有一个印象：客栈 (๑• . •๑) 栈是什么栈（有时称为“后进先出栈”）是一个项的有序集合，其中添加移除新项总发生在同一端。 这段话初学者是懵逼的，别急，往下看。 对栈的一般操作： Stack() 创建一个空的新栈。 它不需要参数，并返回一个空栈。 push(item)将一个新项添加到栈的顶部。它需要 item 做参数并不返回任何内容。 pop() 从栈中删除顶部项。它不需要参数并返回 item 。栈被修改。 peek() 从栈返回顶部项，但不会删除它。不需要参数。 不修改栈。 isEmpty() 测试栈是否为空。不需要参数，并返回布尔值。 size() 返回栈中的 item 数量。不需要参数，并返回一个整数。 例如，s 是已经创建的空栈，下图展示了栈操作序列的结果。栈中，顶部项列在最右边。 自己在心里过一遍就很好理解了 Python实现栈其实看到上面那张图，就想起了Python中 list 的一些用法，append、pop等，下面是使用 Python 来实现栈，也非常简单： 123456789101112131415161718class Stack: def __init__(self): self.items = [] def isEmpty(self): return self.items == [] def push(self, item): self.items.append(item) def pop(self): return self.items.pop() def peek(self): return self.items[len(self.items)-1] def size(self): return len(self.items) pythonds/basic/stack.py 栈的应用：简单括号匹配（一）有一些正确匹配的括号字符串： 12345(()()()())(((())))(()((())())) 对比那些不匹配的括号：12345((((((())()))(()()(() 具有挑战的是如何编写一个算法，能够从左到右读取一串符号，并决定符号是否平衡。 为了解决这个问题，我们需要做一个重要的观察。从左到右处理符号时，最近开始符号必须与下一个关闭符号相匹配。此外，处理的第一个开始符号必须等待直到其匹配最后一个符号。结束符号以相反的顺序匹配开始符号。他们从内到外匹配。这是一个可以用栈解决问题的线索。 12345678910111213141516171819202122232425from pythonds.basic.stack import Stackdef parChecker(symbolString): s = Stack() balanced = True index = 0 while index &lt; len(symbolString) and balanced: symbol = symbolString[index] if symbol == \"(\": s.push(symbol) else: if s.isEmpty(): balanced = False else: s.pop() index = index + 1 if balanced and s.isEmpty(): return True else: return Falseprint(parChecker('((()))'))print(parChecker('(()')) output12TrueFalse 一旦你认为栈是保存括号的恰当的数据结构，算法是很直接的。 从空栈开始，从左到右处理括号字符串。如果一个符号是一个开始符号，将其作为一个信号，对应的结束符号稍后会出现。另一方面，如果符号是结束符号，弹出栈，只要弹出栈的开始符号可以匹配每个结束符号，则括号保持匹配状态。如果任何时候栈上没有出现符合开始符号的结束符号，则字符串不匹配。最后，当所有符号都被处理后，栈应该是空的。 如果有和我一样不能很好理解的，使用pycharm的debug模式，可以一步步来，看看程序就近在做什么。 括号配对问题（二）来看看第二种匹配问题。Python程序里存在很多括号：如圆括号、方括号和花括号，每种括号都有开括号和闭括号。 123456789101112131415161718192021222324252627282930from pythonds.basic.stack import Stackpares = \"()[]&#123;&#125;\"def pare_theses(text): i, text_len = 0, len(text) while True: while i &lt; text_len and text[i] not in pares: i += 1 if i &gt;= text_len: return yield text[i], i i += 1def check_pares(text): open_pares = \"([&#123;\" opposite = &#123;')': '(', ']': '[', '&#125;': '&#123;'&#125; # 表示配对关系的字典 s = Stack() for pr, i in pare_theses(text): if pr in open_pares: # 开括号，压进栈并继续 s.push(pr) elif s.pop() != opposite[pr]: # 不匹配就是失败，退出 print('Unmatching is found at', i, 'for', pr) return False # else 是一次括号配对成功，什么也不做，继续 print(\"All paretheses are correctly matched.\") return Truecheck_pares('([]&#123;&#125;]')check_pares('([]&#123;&#125;)') output 12Unmatching is found at 5 for ]All paretheses are correctly matched. 生成器（回忆一下）： 用 yield 语句产生结果 可以用在需要迭代器的地方 函数结束导致迭代结束 参考 http://interactivepython.org/runestone/static/pythonds/BasicDS/TheStackAbstractDataType.html http://www.math.pku.edu.cn/teachers/qiuzy/ds_python/courseware/index.htm","tags":[{"name":"算法","slug":"算法","permalink":"https://zhangslob.github.io/tags/算法/"},{"name":"栈","slug":"栈","permalink":"https://zhangslob.github.io/tags/栈/"}]},{"title":"为什么不推荐Selenium写爬虫","date":"2018-02-02T12:35:02.000Z","path":"2018/02/02/为什么不推荐Selenium写爬虫/","text":"这是崔斯特的第二十九篇原创文章 如果可以使用 Requests 完成的，别用 Selenium (๑• . •๑) 最近在群里经常会看到有些朋友说，使用Selenium去采集网站，我看到其实内心是很难受的，哎！为什么要用Selenium呢？ 我想说下自己的看法，欢迎各位大佬批评。 观点如果可以使用 Requests 完成的，别用 Selenium 数据采集的顺序接到一个项目或者有一个采集需求时，第一步就是明确自己的需求。经常会遇到半路改需求的事情，真的很难受。 第二步就是去分析这个网站，这个在之前有提到过 采集方案策略之App抓包 : 首先大的地方，我们想抓取某个数据源，我们要知道大概有哪些路径可以获取到数据源，基本上无外乎三种：PC端网站针对移动设备响应式设计的网站（也就是很多人说的H5, 虽然不一定是H5）；移动App原则是能抓移动App的，最好抓移动App，如果有针对移动设备优化的网站，就抓针对移动设备优化的网站，最后考虑PC网站。因为移动App基本都是API很简单，而移动设备访问优化的网站一般来讲都是结构简单清晰的HTML，而PC网站自然是最复杂的了；针对PC端网站和移动网站的做法一样，分析思路可以一起讲，移动App单独分析。 这个时候可以借用 postman 来分析请求，参考 或许你应该学学 postman 然后下一步可能就是工程开始，各种配置，以及选择哪种采集方式，一般来说 Scrapy 是最好用、也是最常见的框架。当然你也可以使用 requests + xpath 或者 Selenium 。下面就我自己的看法来说说这三种采集方式。 三种采集差异Scrapy在 Scrapy 官网 上是这样写的： Scrapy | A Fast and Powerful Scraping and Web Crawling Framework 关键词是 Fast 和 Powerful，使用过确实感觉如此。我感觉 Scrapy 就是一个全家桶，它把爬虫所需要的大部分东西（为什么不是全部，下面会说到）都集成到这个框架中，如：下载器、中间件、调度器、Spider、调试、数据流等等所有功能全部都在这一个框架中，你所需要做的只是在命令行中输入：scrapy startproject yourproject Scrapy 的缺点也是显而易见的：不支持分布式。scrapy中scheduler是运行在队列中的，而队列是在单机内存中的，服务器上爬虫是无法利用内存的队列做任何处理。但是也有解决办法，参见rmax/scrapy-redis Requests来看看 Requests的文档 Requests 唯一的一个非转基因的 Python HTTP 库，人类可以安全享用。 警告：非专业使用其他 HTTP 库会导致危险的副作用，包括：安全缺陷症、冗余代码症、重新发明轮子症、啃文档症、抑郁、头疼、甚至死亡。 作者真幽默 urllib2 VS requests 12345678910111213141516171819202122232425#!/usr/bin/env python# -*- coding: utf-8 -*-import urllib2gh_url = 'https://api.github.com'req = urllib2.Request(gh_url)password_manager = urllib2.HTTPPasswordMgrWithDefaultRealm()password_manager.add_password(None, gh_url, 'user', 'pass')auth_manager = urllib2.HTTPBasicAuthHandler(password_manager)opener = urllib2.build_opener(auth_manager)urllib2.install_opener(opener)handler = urllib2.urlopen(req)print handler.getcode()print handler.headers.getheader('content-type')# ------# 200# 'application/json' 12345678910111213#!/usr/bin/env python# -*- coding: utf-8 -*-import requestsr = requests.get('https://api.github.com', auth=('user', 'pass'))print r.status_codeprint r.headers['content-type']# ------# 200# 'application/json' 简单对比发现 requests 的好用之处了，刚开始学习爬虫的时候也是从 urllib 开始，当看到 requests 果断抛弃，就像看到 xpath 抛弃 bs4 一样 所以如果你是初学者，那么请毫不犹豫的选择 requests Selenium最后来到今天的主角 Selenium， 首先看看官方怎么说 What is Selenium?Selenium automates browsers. That’s it! What you do with that power is entirely up to you. Primarily, it is for automating web applications for testing purposes, but is certainly not limited to just that. Boring web-based administration tasks can (and should!) be automated as well. Selenium has the support of some of the largest browser vendors who have taken (or are taking) steps to make Selenium a native part of their browser. It is also the core technology in countless other browser automation tools, APIs and frameworks. 重点是：it is for automating web applications for testing purposes, but is certainly not limited to just that，翻译过来就是：它是用于自动化Web应用程序的测试目的，但肯定不仅限于此，简单来说，Selenium 是web自动化测试工具集，如果你去Google上搜索 Selenium ，大多结果都是 利用Selenium 自动化web 测试相关内容，比较出名的有博客园的虫师，写的两本书也都是关于自动化测试方面的 至于为啥爬虫要用selenium，我在某些博客上找到有人这样说，我也不知道怎么说 对于一般网站来说scrapy、requests、beautifulsoup等都可以爬取，但是有些信息需要执行js才能显现，而且你肉眼所能看到的基本都能爬取下来，在学习中遇到了，就记录下来方便以后查看。 webdrive是selenium中一个函数： from selenium import webdriverdriver = webdriver.Chrome()driver.get(‘网址’) 其中PhantomJS同时可以换成Chrome、Firefox、Ie等等，但是PhantomJS是一个无头的浏览器，运行是不会跳出相应的浏览器，运行相对效率较高。在调试中可以先换成Chrome，方便调试，最后再换成PhantomJS即可。 下面是吐槽时间，说一说 Selenium 的缺点： 速度慢。每次运行爬虫都打开一个浏览器，如果没有设置，还会加载图片、JS等等一大堆东西； 占用资源太多。有人说，把Chrome换成无头浏览器PhantomJS，原理都是一样的，都是打开浏览器，而且很多网站会验证参数，如果对方看到你是以PhantomJS去访问，会BAN掉你的请求，然后你又要考虑更换请求头的事情，事情复杂程度不知道多了多少，为啥学Python？因为Python简单啊，如果有更快、更简单的库可以实现同样的功能，为什么不去使用呢？ 对网络的要求会更高。 Selenium 加载了很多可能对您没有价值的补充文件（如css，js和图像文件）。 与仅仅请求您真正需要的资源（使用单独的HTTP请求）相比，这可能会产生更多的流量。 爬取规模不能太大。你有看到哪家公司用Selenium作为生产环境吗？ 难。学习Selenium的成本太高，只有我一个人觉得Selenium比Requests难一百倍吗？ 我能想到的就这么多了，欢迎各位大佬补充。所以，如果可以使用 Requests 完成的，别用 Selenium，OK，洗脑完成。 之前面试爬虫工程师有一题就是：如何处理网站的登录系统？ A.浏览器模拟 B.HTTP请求 如果你想做测试工程师，那肯定需要学会 Selenium，公司一个妹子就是测试，现在学了 Selenium，工作轻松了好多。 最后，无耻的来个广告，本公司招聘爬虫工程师，希望和你成为队友!","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"Selenium","slug":"Selenium","permalink":"https://zhangslob.github.io/tags/Selenium/"}]},{"title":"或许你应该学学 postman","date":"2018-01-31T14:59:36.000Z","path":"2018/01/31/或许你应该学学-postman/","text":"这是崔斯特的第二十八篇原创文章 简单模拟请求的工具 (๑• . •๑) 使用最简单的方法就是直接在浏览器中复制 Copy as cURL ，然后把数据导入 postman，然后 send ，收工。 我们这里拿 知乎首页 举例 在对应的请求下复制 cURL 打开 postman ， 点击左上角的 Import ， 选择Paste Raw Text ，最后 Import，点击 send发送请求 发送请求之后就可以查看了，如下图，标箭头的地方可以打开看更多。比如可以预览web界面，查看 Headers 信息，查看状态，复制代码。 同时可以打开 Headers ，用来调试，哪些是需要的，哪些不需要 最方便的一点是，可以直接生成对应的编程语言，并复制，例如Python的requests方法： 好了，到这里 postman 的简单功能就说完了，他的全部功能当然不止这一点，更多的就去看 文档啦 问题在我的使用过程中，发现了 postman 的一些问题，如：导入错误，参数错误，请求失误。 导入错误例如知乎这个例子，如果我们复制的是 Copy as cURL (cmd) ，可能你会遇到下面的错误 这个时候选用 Copy as cURL (bush) 就好了，具体原因是啥，我也不清楚。我在这里找到了别人的描述 There is no difference between the two cURL command because there is a difference between ” and ‘. Refer : Use cURL to get the same results as a web browser 参数错误举个例子，今天在帮朋友查看 这个网站 的翻页，复制用postman打开， copy cURL 内容是 1234567891011121314151617curl &quot;https://www.crunchbase.com/v4/data/entities/organizations/56e40f50-97c7-2a77-255d-1d97d5f30646/overrides?field_ids=^%^5B^%^22identifier^%^22,^%^22layout_id^%^22,^%^22facet_ids^%^22,^%^22title^%^22,^%^22short_description^%^22,^%^22is_locked^%^22^%^5D^&amp;card_ids=^%^5B^%^22investments_list^%^22^%^5D&quot; -H &quot;cookie: _ga=GA1.2.35962729.1517412509; _gid=GA1.2.2072770006.1517412509; _vdl=1; _hp2_ses_props.973801186=^%^7B^%^22ts^%^22^%^3A1517412512548^%^2C^%^22d^%^22^%^3A^%^22www.crunchbase.com^%^22^%^2C^%^22h^%^22^%^3A^%^22^%^2Fsearch^%^2Fprincipal.investors^%^22^%^7D; __qca=P0-1969245879-1517412512628; D_IID=1B7344D2-1C8F-3327-8607-D786306444AE; D_UID=208F925B-3D1C-3491-A532-C82375EE187D; D_ZID=497DB63C-5101-3F49-BE35-1752A80F8DDA; D_ZUID=D89FCBAA-BF79-340C-BF55-B860768D0993; D_HID=57B19D5F-5069-3A82-94CB-D42821D1CD10; D_SID=123.120.141.63:bXaeU41PWi5vyYIflFmiShQiK1qwq/nC4G9IljWo+6A; AMCVS_6B25357E519160E40A490D44^%^40AdobeOrg=1; wcsid=KZbgLoopx4WnyMOW3F6pZ0H92JEzMrBd; hblid=cfw6lOKzm4FpCUou3F6pZ0H92JE6rBWB; s_cc=true; AMCV_6B25357E519160E40A490D44^%^40AdobeOrg=1099438348^%^7CMCMID^%^7C05859477990281579603868663655860142263^%^7CMCAAMLH-1518017313^%^7C11^%^7CMCAAMB-1518017313^%^7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y^%^7CMCOPTOUT-1517419713s^%^7CNONE^%^7CMCAID^%^7CNONE^%^7CMCSYNCSOP^%^7C411-17570^%^7CvVersion^%^7C2.1.0; _okdetect=^%^7B^%^22token^%^22^%^3A^%^2215174125149410^%^22^%^2C^%^22proto^%^22^%^3A^%^22https^%^3A^%^22^%^2C^%^22host^%^22^%^3A^%^22www.crunchbase.com^%^22^%^7D; olfsk=olfsk8562990481377502; _okbk=cd4^%^3Dtrue^%^2Cvi5^%^3D0^%^2Cvi4^%^3D1517412515909^%^2Cvi3^%^3Dactive^%^2Cvi2^%^3Dfalse^%^2Cvi1^%^3Dfalse^%^2Ccd8^%^3Dchat^%^2Ccd6^%^3D0^%^2Ccd5^%^3Daway^%^2Ccd3^%^3Dfalse^%^2Ccd2^%^3D0^%^2Ccd1^%^3D0^%^2C; _ok=1554-355-10-6773; _hp2_props.973801186=^%^7B^%^22Logged^%^20In^%^22^%^3Afalse^%^2C^%^22Pro^%^22^%^3Afalse^%^7D; _hp2_id.973801186=^%^7B^%^22userId^%^22^%^3A^%^228805156096536097^%^22^%^2C^%^22pageviewId^%^22^%^3A^%^221700148784936413^%^22^%^2C^%^22sessionId^%^22^%^3A^%^225929107734453151^%^22^%^2C^%^22identity^%^22^%^3Anull^%^2C^%^22trackerVersion^%^22^%^3A^%^223.0^%^22^%^7D; _oklv=1517412548852^%^2CKZbgLoopx4WnyMOW3F6pZ0H92JEzMrBd; s_pers=^%^20s_nrgvo^%^3DNew^%^7C1580484574965^%^3B&quot; -H &quot;origin: https://www.crunchbase.com&quot; -H &quot;accept-encoding: gzip, deflate, br&quot; -H &quot;x-distil-ajax: dfdvfavtsysazfberrtudvwabwe&quot; -H &quot;user-agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36&quot; -H &quot;content-type: application/json&quot; -H &quot;accept-language: zh-CN,zh;q=0.9,en;q=0.8&quot; -H &quot;accept: application/json, text/plain, */*&quot; -H &quot;referer: https://www.crunchbase.com/organization/500-startups/investments/investments_list&quot; -H &quot;authority: www.crunchbase.com&quot; -H &quot;x-requested-with: XMLHttpRequest&quot; --data-binary ^&quot;^&#123;^ ^\\^&quot;card_lookups^\\^&quot;: ^[^ ^&#123;^ ^\\^&quot;card_id^\\^&quot;: ^\\^&quot;investments_list^\\^&quot;,^ ^\\^&quot;limit^\\^&quot;: 100,^ ^\\^&quot;after_id^\\^&quot;: ^\\^&quot;07a9c686-4590-fa0f-3ac4-fc7b898c0b7a^\\^&quot;^ ^&#125;^ ^]^^&#125;^&quot; --compressed 导入之后，send，返回 400 错误。 postman 转义的code是： 1234567891011121314151617181920212223242526import requestsurl = \"https://www.crunchbase.com/v4/data/entities/organizations/56e40f50-97c7-2a77-255d-1d97d5f30646/overrides\"querystring = &#123;\"field_ids\":\"^%^5B^%^22identifier^%^22,^%^22layout_id^%^22,^%^22facet_ids^%^22,^%^22title^%^22,^%^22short_description^%^22,^%^22is_locked^%^22^%^5D^\",\"card_ids\":\"^%^5B^%^22investments_list^%^22^%^5D\"&#125;payload = \"^^&#123;^\\n\\n ^\\\\^card_lookups^^: ^[^\\n\\n ^&#123;^\\n\\n ^\\\\^card_id^^: ^\\\\^investments_list^^,^\\n\\n ^\\\\^limit^^: 100,^\\n\\n ^\\\\^after_id^^: ^\\\\^07a9c686-4590-fa0f-3ac4-fc7b898c0b7a^^^\\n\\n ^&#125;^\\n\\n ^]^\\n\\n^&#125;^\"headers = &#123; 'cookie': \"_ga=GA1.2.35962729.1517412509; _gid=GA1.2.2072770006.1517412509; _vdl=1; _hp2_ses_props.973801186=^%^7B^%^22ts^%^22^%^3A1517412512548^%^2C^%^22d^%^22^%^3A^%^22www.crunchbase.com^%^22^%^2C^%^22h^%^22^%^3A^%^22^%^2Fsearch^%^2Fprincipal.investors^%^22^%^7D; __qca=P0-1969245879-1517412512628; D_IID=1B7344D2-1C8F-3327-8607-D786306444AE; D_UID=208F925B-3D1C-3491-A532-C82375EE187D; D_ZID=497DB63C-5101-3F49-BE35-1752A80F8DDA; D_ZUID=D89FCBAA-BF79-340C-BF55-B860768D0993; D_HID=57B19D5F-5069-3A82-94CB-D42821D1CD10; D_SID=123.120.141.63:bXaeU41PWi5vyYIflFmiShQiK1qwq/nC4G9IljWo+6A; AMCVS_6B25357E519160E40A490D44^%^40AdobeOrg=1; wcsid=KZbgLoopx4WnyMOW3F6pZ0H92JEzMrBd; hblid=cfw6lOKzm4FpCUou3F6pZ0H92JE6rBWB; s_cc=true; AMCV_6B25357E519160E40A490D44^%^40AdobeOrg=1099438348^%^7CMCMID^%^7C05859477990281579603868663655860142263^%^7CMCAAMLH-1518017313^%^7C11^%^7CMCAAMB-1518017313^%^7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y^%^7CMCOPTOUT-1517419713s^%^7CNONE^%^7CMCAID^%^7CNONE^%^7CMCSYNCSOP^%^7C411-17570^%^7CvVersion^%^7C2.1.0; _okdetect=^%^7B^%^22token^%^22^%^3A^%^2215174125149410^%^22^%^2C^%^22proto^%^22^%^3A^%^22https^%^3A^%^22^%^2C^%^22host^%^22^%^3A^%^22www.crunchbase.com^%^22^%^7D; olfsk=olfsk8562990481377502; _okbk=cd4^%^3Dtrue^%^2Cvi5^%^3D0^%^2Cvi4^%^3D1517412515909^%^2Cvi3^%^3Dactive^%^2Cvi2^%^3Dfalse^%^2Cvi1^%^3Dfalse^%^2Ccd8^%^3Dchat^%^2Ccd6^%^3D0^%^2Ccd5^%^3Daway^%^2Ccd3^%^3Dfalse^%^2Ccd2^%^3D0^%^2Ccd1^%^3D0^%^2C; _ok=1554-355-10-6773; _hp2_props.973801186=^%^7B^%^22Logged^%^20In^%^22^%^3Afalse^%^2C^%^22Pro^%^22^%^3Afalse^%^7D; _hp2_id.973801186=^%^7B^%^22userId^%^22^%^3A^%^228805156096536097^%^22^%^2C^%^22pageviewId^%^22^%^3A^%^221700148784936413^%^22^%^2C^%^22sessionId^%^22^%^3A^%^225929107734453151^%^22^%^2C^%^22identity^%^22^%^3Anull^%^2C^%^22trackerVersion^%^22^%^3A^%^223.0^%^22^%^7D; _oklv=1517412548852^%^2CKZbgLoopx4WnyMOW3F6pZ0H92JEzMrBd; s_pers=^%^20s_nrgvo^%^3DNew^%^7C1580484574965^%^3B\", 'origin': \"https://www.crunchbase.com\", 'accept-encoding': \"gzip, deflate, br\", 'x-distil-ajax': \"dfdvfavtsysazfberrtudvwabwe\", 'user-agent': \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\", 'content-type': \"application/json\", 'accept-language': \"zh-CN,zh;q=0.9,en;q=0.8\", 'accept': \"application/json, text/plain, */*\", 'referer': \"https://www.crunchbase.com/organization/500-startups/investments/investments_list\", 'authority': \"www.crunchbase.com\", 'x-requested-with': \"XMLHttpRequest\", 'cache-control': \"no-cache\", 'postman-token': \"1df3b2b6-b682-edf7-4804-572ac5a03420\" &#125;response = requests.request(\"POST\", url, data=payload, headers=headers, params=querystring)print(response.text) 可以看到 加入了大量的 ^ 符号，这个在Python中是运算符1^ 按位异或运算符：当两对应的二进位相异时，结果为1 (a ^ b) 输出结果 49 ，二进制解释： 0011 0001 这也是 postman 的一个问题 请求失误这个问题，我也不是很懂，有的请求 postman 返回错误，但是复制代码到 Python 环境中运行是可以获得数据的，所以最好是多次验证。","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"postman","slug":"postman","permalink":"https://zhangslob.github.io/tags/postman/"}]},{"title":"有了她，谁敢阻止我学习","date":"2018-01-30T13:31:40.000Z","path":"2018/01/30/有了她，谁敢阻止我学习/","text":"这是崔斯特的第二十七篇原创文章 我爱学习 (๑• . •๑) 公众号Python爱好者社区，微信号：python_shequ 人生苦短，我用Python。分享Python相关的技术文章、工具资源、精选课程、视频教程、热点资讯、学习资料等。每天自动更新和推送。 Python爬虫分享，微信号：python_shequ 知乎专栏“爬虫从入门到放弃”作者。目前在知乎连载爬虫从入门到放弃到精通系列文章 张俊红，微信号：zhangjunhong0428 中国统计网专栏作者，数据分析路上的学习者与实践者，与你分享我的所见、所学、所想。 数据小魔方，微信号：datamofang 专注于数据可视化及商务智能的原创技能分享平台！ Python爬虫与算法进阶，微信号：zhangslob 分享Python知识，关注爬虫与算法,让我们一起从萌新变成大牛吧! 在线书籍 关于Python的面试题 草根学Python python数据结构和算法 Python最佳实践指南！ Python Cookbook 3rd Edition Documentation Stackoverflow about Python Python进阶 免费视频 零基础入门学习Python 话说我当时就看的小甲鱼视频入门的 Python语言程序设计 嵩天老师的一系列课程 Google机器学习 简单入门 算法基础 北大博士的课程。。 吴恩达机器学习 这个就不评价了，都知道 爬虫教程 爬虫教程 某位大佬所写，由浅入深 Scrapy分类 质量非常高 Python3网络爬虫实战 崔大的书，马上会出版 大数据之网络爬虫 主要是垂直型网络爬虫 使用Scrapy-redis实现分布式爬取 分布式资料 使用Docker部署scrapy-redis分布式爬虫 同样是分布式资料 最重要的一点是：不要光收藏，不去看","tags":[{"name":"Git","slug":"Git","permalink":"https://zhangslob.github.io/tags/Git/"}]},{"title":"烦人的Git ( ´•︵•` )","date":"2018-01-29T13:30:41.000Z","path":"2018/01/29/烦人的Git 终于学会了 -´•︵•/","text":"这是崔斯特的第二十六篇原创文章 终于会用Git了 (๑• . •๑) Git对于新手来说，真的很烦人哎，好在找到了好工具 —— PyCharm 使用PyCharm进行代码管理在VCS里有Git，里面有常用的操作，clone、pull、push等等。 更方便的是，在左下角，有Version Control，可以清晰的看到日志改变，图中另一个标记的位置可以直接进行commit，如下图所示 更多参考官方文档 Enabling Version Control 我修改了主题，可以在这里改 Material Theme UI emmm，简单的方法就是这样，还是需要了解下Git的基本操作的 创建新仓库创建新文件夹，打开，然后执行 git init 以创建新的 git 仓库。 或者按照官方推荐的例子 123456789git clone git@github.com:zhangslob/test_test.gitecho \"# test_test\" &gt;&gt; README.mdgit initgit add README.mdgit commit -m \"first commit\"git remote add origin git@github.com:zhangslob/test_test.gitgit push -u origin master# 其中的命令下面会讲到 克隆仓库执行如下命令以创建一个本地仓库的克隆版本： git clone /path/to/repository 如果是远端服务器上的仓库，你的命令会是这个样子： git clone username@host:/path/to/repository 添加和提交你可以提出更改（把它们添加到暂存区），使用如下命令： git add . 这是 git 基本工作流程的第一步；使用如下命令以实际提交改动： git commit -m &quot;代码提交信息&quot; 现在，你的改动已经提交到了 HEAD，但是还没到你的远端仓库。 推送改动你的改动现在已经在本地仓库的 HEAD 中了。执行如下命令以将这些改动提交到远端仓库： git push origin master 可以把 master 换成你想要推送的任何分支。 分支分支是用来将特性开发绝缘开来的。在你创建仓库的时候，master 是“默认的”分支。在其他分支上进行开发，完成后再将它们合并到主分支上。 创建一个叫做“develop”的分支，并切换过去： git checkout -b develop 切换回主分支： git checkout master 再把新建的分支删掉： git branch -d develop 除非你将分支推送到远端仓库，不然该分支就是 不为他人所见的： git push origin &lt;branch&gt; 更新与合并要更新你的本地仓库至最新改动，执行： git pull 以在你的工作目录中 获取（fetch） 并 合并（merge） 远端的改动。 要合并其他分支到你的当前分支（例如 master），执行： git merge &lt;branch&gt; 在这两种情况下，git 都会尝试去自动合并改动。遗憾的是，这可能并非每次都成功，并可能出现冲突（conflicts）。 这时候就需要你修改这些文件来手动合并这些冲突（conflicts）。改完之后，你需要执行如下命令以将它们标记为合并成功： git add &lt;filename&gt; 在合并改动之前，你可以使用如下命令预览差异： git diff &lt;source_branch&gt; &lt;target_branch&gt; 替换本地改动假如你操作失误，你可以使用如下命令替换掉本地改动： git checkout -- &lt;filename&gt; 此命令会使用 HEAD 中的最新内容替换掉你的工作目录中的文件。已添加到暂存区的改动以及新文件都不会受到影响。 假如你想丢弃你在本地的所有改动与提交，可以到服务器上获取最新的版本历史，并将你本地主分支指向它：12git fetch origingit reset --hard origin/master 小技巧 使用 git status 可以迅速查看当前状态，若添加文件，会显示绿色的文字 使用Github的客户端，可以事半功倍 https://desktop.github.com/ 更多可以参考 Git 文档 Git Commit Message 模版模版命名约定模版名称由以下部分组成 gitmessage-${语言}[-full] 以 -full 结尾的模版包含了 message 组成区域的解释以及书写的指导，推荐新手选择该模版上手，在熟悉书写规范后换成简单版本。 1234567# feat (新功能)# fix (问题修复)# refactor (代码重构)# style (代码风格改动、格式变化等，无实现改动)# docs (文档更新)# test (增加、重构测试，无实现改动)# chore (修改一些配置文件如 .gitignore 等，无实现改动)","tags":[{"name":"Git","slug":"Git","permalink":"https://zhangslob.github.io/tags/Git/"}]},{"title":"学点算法之字符串的乱序检查","date":"2018-01-27T10:52:56.000Z","path":"2018/01/27/学点算法之字符串的乱序检查/","text":"这是崔斯特的第二十五篇原创文章 老板，我要做算法工程师！ 问题字符串的乱序检查。 一个字符串是另一个字符串的乱序。如果第二个字符串只是第一个的重新排列，例如，’heart’ 和 ‘earth’ 就是乱序字符串。’python’ 和 ‘typhon’ 也是。为了简单起见，我们假设所讨论的两个字符串具有相等的长度，并且他们由 26 个小写字母集合组成。我们的目标是写一个布尔函数，它将两个字符串做参数并返回它们是不是回文。 解法1:检查我们对乱序问题的第一个解法是检查第一个字符串是不是出现在第二个字符串中。如果可以检验到每一个字符，那两个字符串一定是回文。可以通过用 None 替换字符来完成检查。但是，由于 Python 字符串是不可变的，所以第一步是将第二个字符串转换为列表。第一个字符串中的每个字符可以通过检查在第二个列表中检查元素是否存在，如果存在，替换成 None。 12345678910111213141516171819202122232425def anagramSolution1(s1,s2): alist = list(s2) pos1 = 0 stillOK = True while pos1 &lt; len(s1) and stillOK: pos2 = 0 found = False while pos2 &lt; len(alist) and not found: if s1[pos1] == alist[pos2]: found = True else: pos2 = pos2 + 1 if found: alist[pos2] = None else: stillOK = False pos1 = pos1 + 1 return stillOKprint(anagramSolution1('abcd','dcba')) s1 的每个字符都会在 s2 中进行最多 n 个字符的迭代 s2 列表中的 n 个位置将被访问一次来匹配来自 s1 的字符。访问次数可以写成 1 到 n 整数的和，可以写成 当 n 变大，n^2 这项占据主导，1/2 可以忽略。所以这个算法复杂度为 O(n^2 )。 解法2:排序和比较另一个解决方案是利用这么一个事实，即使 s1,s2 不同，它们只有由完全相同的字符组成，它们才是回文。所以，如果我们按照字母顺序排列每个字符串，从 a 到 z，如果两个字符串相同，则这两个字符串为回文。 12345678910111213def anagramSolution2(s1,s2): alist1 = list(s1) alist2 = list(s2) alist1.sort() alist2.sort() if alist1 == alist2: return True else: return Falseprint(anagramSolution2('abcde','edcba')) 这个算法比较简单，只用到了排序算法，那么排序算法的复杂度还是多少呢？ 在这里找到了答案 python中的sorted算法，网上有人撰文，说比较低级。其实不然，通过阅读官方文档，发现python中的sorted排序，真的是高大上，用的Timsort算法。什么是Timsort，请看 wiki的解释：http://en.wikipedia.org/wiki/Timsort 另外，国内有一个文档，适当翻译：http://blog.csdn.net/yangzhongblog/article/details/8184707，这里截取一个不同排序算法比较的图示，就明白sorted的威力了。 从时间复杂度来看，Timsort是威武的。 从空间复杂度来讲，需要的开销在数量大的时候会增大。 解法3: 穷举法解决这类问题的强力方法是穷举所有可能性。 对于回文检测，我们可以生成 s1 的所有乱序字符串列表，然后查看是不是有 s2。这种方法有一点困难。当 s1 生成所有可能的字符串时，第一个位置有 n 种可能，第二个位置有 n-1 种，第三个位置有 n-3 种，等等。总数为 n∗(n−1)∗(n−2)∗...∗3∗2∗1n∗(n−1)∗(n−2)∗...∗3∗2∗1， 即 n! 。 虽然一些字符串可能是重复的，程序也不可能提前知道这样，所以他仍然会生成 n! 个字符串。事实证明，n! 比 n^2 增长还快，事实上，如果 s1 有 20个字符长，则将有 20! = 2,432,902,008,176,640,000 个字符串产生。如果我们每秒处理一种可能字符串，那么需要 77,146,816,596 年才能过完整个列表。 所以当然不会采取这种方案了。 解法4: 计数和比较我们最终解决回文的方法是利用两个乱序字符串具有相同的 a, b, c 等等的事实。 我们首先计算的是每个字母出现的次数。由于有 26 个可能的字符，我们就用 一个长度为 26 的列表，每个可能的字符占一个位置。每次看到一个特定的字符，就增加该位置的计数器。最后如果两个列表的计数器一样，则字符串为乱序字符串。 1234567891011121314151617181920212223def anagramSolution4(s1,s2): c1 = [0]*26 c2 = [0]*26 for i in range(len(s1)): pos = ord(s1[i])-ord('a') c1[pos] = c1[pos] + 1 for i in range(len(s2)): pos = ord(s2[i])-ord('a') c2[pos] = c2[pos] + 1 j = 0 stillOK = True while j&lt;26 and stillOK: if c1[j]==c2[j]: j = j + 1 else: stillOK = False return stillOKprint(anagramSolution4('apple','pleap')) 同样，这个方案有多个迭代，但是和第一个解法不一样，它不是嵌套的。两个迭代都是 n, 第三个迭代，比较两个计数列表，需要 26 步，因为有 26 个字母。一共 T(n)=2n+26T(n)=2n+26，即 O(n)，我们找到了一个线性量级的算法解决这个问题。 如果让我自己来选择，我可能会选第二种，第二种最简单，也最好理解。但是最后的结论表明 解法4 才是最优解法，排序固然简单，但是但数量很大的时候，可能远不止我们想的那么简单。 在结束这个例子之前，我们来讨论下空间花费，虽然最后一个方案在线性时间执行，但它需要额外的存储来保存两个字符计数列表。换句话说，该算法牺牲了空间以获得时间。 很多情况下，你需要在空间和时间之间做出权衡。这种情况下，额外空间不重要，但是如果有数百万个字符，就需要关注下。作为一个计算机科学家，当给定一个特定的算法，将由你决定如何使用计算资源。 如有错误，请指出 图片来源 各位下期见，不聊了，又该搬砖了。。。","tags":[{"name":"算法","slug":"算法","permalink":"https://zhangslob.github.io/tags/算法/"},{"name":"Python","slug":"Python","permalink":"https://zhangslob.github.io/tags/Python/"},{"name":"排序","slug":"排序","permalink":"https://zhangslob.github.io/tags/排序/"}]},{"title":"使用logging管理爬虫","date":"2018-01-15T13:39:35.000Z","path":"2018/01/15/使用logging管理爬虫/","text":"这是崔斯特的第二十四篇原创文章 使用日志记录程序运行状态 文档 日志 模块自2.3版本开始便是Python标准库的一部分。它被简洁的描述在 PEP 282。 众所周知，除了 基础日志指南 部分，该文档并不容易阅读。 日志的两个目的： 诊断日志 记录与应用程序操作相关的日志。例如，用户遇到的报错信息， 可通过搜索诊断日志获得上下文信息。 审计日志 为商业分析而记录的日志。从审计日志中，可提取用户的交易信息， 并结合其他用户资料构成用户报告或者用来优化商业目标。 … 或者打印?当需要在命令行应用中显示帮助文档时， 打印 是一个相对于日志更好的选择。 而在其他时候，日志总能优于 打印 ，理由如下： 日志事件产生的 日志记录 ，包含清晰可用的诊断信息，如文件名称、路径、函数名和行号等。 包含日志模块的应用，默认可通过根记录器对应用的日志流进行访问，除非您将日志过滤了。 可通过 logging.Logger.setLevel() 方法有选择地记录日志， 或可通过设置 logging.Logger.disabled 属性为 True 来禁用。 库中的日志日志指南 中含 库日志配置 的说明。由于是 用户 ，而非库来指明如何响应日志事件， 因此这里有一个值得反复说明的忠告： 注解强烈建议不要向您的库日志中加入除NullHandler外的其它处理程序。 在库中，声明日志的最佳方式是通过 __name__ 全局变量： logging 模块通过点(dot)运算符创建层级排列的日志，因此，用 __name__ 可以避免名字冲突。 以下是一个来自 requests 资源 的最佳实践的例子 —— 把它放置在您的 __init__.py 文件中12import logginglogging.getLogger(__name__).addHandler(logging.NullHandler()) 应用程序中的日志应用程序开发的权威指南， 应用的12要素 ，也在其中一节描述了 日志的作用 。它特别强调将日志视为事件流， 并将其发送至由应用环境所处理的标准输出中。 配置日志至少有以下三种方式： 使用INI格式文件： 优点: 使用 logging.config.listen() 函数监听socket，可在运行过程中更新配置 缺点: 通过源码控制日志配置较少（ 例如 子类化定制的过滤器或记录器）。 使用字典或JSON格式文件： 优点: 除了可在运行时动态更新，在Python 2.6之后，还可通过 json 模块从其它文件中导入配置。 缺点: 很难通过源码控制日志配置。 使用源码： 优点: 对配置绝对的控制。 缺点: 对配置的更改需要对源码进行修改。 通过INI文件进行配置的例子我们假设文件名为 logging_config.ini 。关于文件格式的更多细节，请参见 日志指南 中的 日志配置 部分。 123456789101112131415161718192021[loggers]keys=root[handlers]keys=stream_handler[formatters]keys=formatter[logger_root]level=DEBUGhandlers=stream_handler[handler_stream_handler]class=StreamHandlerlevel=DEBUGformatter=formatterargs=(sys.stderr,)[formatter_formatter]format=%(asctime)s %(name)-12s %(levelname)-8s %(message)s 然后在源码中调用 logging.config.fileConfig() 方法：123456import loggingfrom logging.config import fileConfigfileConfig('logging_config.ini')logger = logging.getLogger()logger.debug('often makes a very good meal of %s', 'visiting tourists') 通过字典进行配置的例子Python 2.7中，您可以使用字典实现详细配置。PEP 391 包含了一系列字典配置的强制和 非强制的元素。123456789101112131415161718192021222324import loggingfrom logging.config import dictConfiglogging_config = dict( version = 1, formatters = &#123; 'f': &#123;'format': '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'&#125; &#125;, handlers = &#123; 'h': &#123;'class': 'logging.StreamHandler', 'formatter': 'f', 'level': logging.DEBUG&#125; &#125;, root = &#123; 'handlers': ['h'], 'level': logging.DEBUG, &#125;,)dictConfig(logging_config)logger = logging.getLogger()logger.debug('often makes a very good meal of %s', 'visiting tourists') 通过源码直接配置的例子 1234567891011import logginglogger = logging.getLogger()handler = logging.StreamHandler()formatter = logging.Formatter( '%(asctime)s %(name)-12s %(levelname)-8s %(message)s')handler.setFormatter(formatter)logger.addHandler(handler)logger.setLevel(logging.DEBUG)logger.debug('often makes a very good meal of %s', 'visiting tourists') 官方文档说完了，来看看具体的应用。 默认的日志级别设置为 WARNING （日志级别等级 CRITICAL &gt; ERROR &gt; WARNING &gt; INFO &gt; DEBUG &gt; NOTSET）, 小于 WARNING 级别的日志都不输出, 大于等于 WARNING 级别的日志都会输出。 简单的将日志打印到屏幕12345678910111213141516171819#!/usr/bin/env python# -*- coding: utf8 -*-import loggingif __name__ == '__main__': logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(filename)s [line:%(lineno)d] %(levelname)s %(message)s', datefmt='%a, %d %b %Y %H:%M:%S', filename='test.log', filemode='w') console = logging.StreamHandler() console.setLevel(logging.WARNING) formatter=logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s') console.setFormatter(formatter) logging.getLogger('').addHandler(console) logging.debug('This is DEBUG') logging.info('This is INFO') logging.warning('This is WARNING') 输出:1root : WARNING This is WARNING test.log文件中包含：123Mon, 15 Jan 2018 20:19:45 sfda.py [line:90] DEBUG This is DEBUGMon, 15 Jan 2018 20:19:45 sfda.py [line:91] INFO This is INFOMon, 15 Jan 2018 20:19:45 sfda.py [line:92] WARNING This is WARNING 注意：由于日志写入模式设置为 w ，因此重复运行时会将之前的日志清空。 logging.basicConfig 函数各参数: 123456789101112131415161718filename: 指定日志文件名filemode: 和file函数意义相同，指定日志文件的打开模式，’w’或’a’format: 指定输出的格式和内容，format可以输出很多有用信息，如上例所示:%(levelno)s: 打印日志级别的数值%(levelname)s: 打印日志级别名称%(pathname)s: 打印当前执行程序的路径，其实就是sys.argv[0]%(filename)s: 打印当前执行程序名%(funcName)s: 打印日志的当前函数%(lineno)d: 打印日志的当前行号%(asctime)s: 打印日志的时间%(thread)d: 打印线程ID%(threadName)s: 打印线程名称%(process)d: 打印进程ID%(message)s: 打印日志信息datefmt: 指定时间格式，同time.strftime()level: 设置日志级别，默认为logging.WARNINGstream: 指定将日志的输出流，可以指定输出到sys.stderr,sys.stdout或者文件，默认输出到sys.stderr，当stream和filename同时指定时，stream被忽略 logging 三大模块： Logger , Handler ， Formatlogger logger 通过 getLogger 函数得到， 可以在不同的模块中使用不同的 logger123import logginglogger = logging.getLogger(__name__)logger.debug('some infomation') Handler handler 有多种， 可以记录到 console， 或者到文件， 文件也可以自动 rotate， 常用的几个 handler StreamHandler 打印到终端 FileHandler 保存到文件 RotatingFileHandler 保存到文件， 达到一定大小之后备份文件。 TimedRotatingFileHandler 定时备份 Format Formatter 对象设置日志信息最后的规则、结构和内容，默认的时间格式为 %Y-%m-%d %H:%M:%S Scrapy 与 Logging 文档 Scrapy uses Python’s builtin logging system for event logging. We’ll provide some simple examples to get you started, but for more advanced use-cases it’s strongly suggested to read thoroughly its documentation. Scrapy使用Python的内置日志记录系统进行事件日志记录。 我们将提供一些简单的示例来帮助您开始，但对于更高级的用例，强烈建议您仔细阅读其文档。 Log levels logging.CRITICAL - for critical errors (highest severity) logging.ERROR - for regular errors logging.WARNING - for warning messages logging.INFO - for informational messages logging.DEBUG - for debugging messages (lowest severity) How to log messages quick example 123import logginglogger = logging.getLogger()logger.warning(\"This is a warning\") Logging from Spiders该记录器是使用Spider的名称创建的，但是您可以使用任何您想要的自定义Python记录器。 例如： 123456789101112import loggingimport scrapylogger = logging.getLogger('mycustomlogger')class MySpider(scrapy.Spider): name = 'myspider' start_urls = ['https://scrapinghub.com'] def parse(self, response): logger.info('Parse function called on %s', response.url) 在middlewares中应用12345678import logginglogger = logging.getLogger(__name__)class ProxyMiddleware(object): def process_request(self, request, spider): request.meta['proxy'] = random.choice(proxy_list) spider.logger.info('get ip: &#123;&#125;'.format(request.meta['proxy'])) 使用Scrapy记录爬取日志在 settings.py 中修改:12LOG_STDOUT = TrueLOG_FILE = 'scrapy_log.txt' 然后开始运行爬虫，日志不会打印，会保存到 scrapy_log.txt 文件中。 使用errbacks在请求处理中捕获异常请求的errback是在处理异常时被调用的函数。 它接收Twisted Failure实例作为第一个参数，可用于跟踪连接建立超时，DNS错误等。 这里有一个爬虫日志记录所有的错误和捕捉一些特定的错误，例子： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import scrapyfrom scrapy.spidermiddlewares.httperror import HttpErrorfrom twisted.internet.error import DNSLookupErrorfrom twisted.internet.error import TimeoutError, TCPTimedOutErrorclass ErrbackSpider(scrapy.Spider): name = \"errback_example\" start_urls = [ \"http://www.httpbin.org/\", # HTTP 200 expected \"http://www.httpbin.org/status/404\", # Not found error \"http://www.httpbin.org/status/500\", # server issue \"http://www.httpbin.org:12345/\", # non-responding host, timeout expected \"http://www.httphttpbinbin.org/\", # DNS error expected ] def start_requests(self): for u in self.start_urls: yield scrapy.Request(u, callback=self.parse_httpbin, errback=self.errback_httpbin, dont_filter=True) def parse_httpbin(self, response): self.logger.info('Got successful response from &#123;&#125;'.format(response.url)) # do something useful here... def errback_httpbin(self, failure): # log all failures self.logger.error(repr(failure)) # in case you want to do something special for some errors, # you may need the failure's type: if failure.check(HttpError): # these exceptions come from HttpError spider middleware # you can get the non-200 response response = failure.value.response self.logger.error('HttpError on %s', response.url) elif failure.check(DNSLookupError): # this is the original request request = failure.request self.logger.error('DNSLookupError on %s', request.url) elif failure.check(TimeoutError, TCPTimedOutError): request = failure.request self.logger.error('TimeoutError on %s', request.url)","tags":[{"name":"2017","slug":"2017","permalink":"https://zhangslob.github.io/tags/2017/"},{"name":"logging","slug":"logging","permalink":"https://zhangslob.github.io/tags/logging/"}]},{"title":"2017 梦醒时分","date":"2017-12-31T08:09:41.000Z","path":"2017/12/31/2017-梦醒时分/","text":"这是崔斯特的第二十三篇原创文章 首先说说自己吧，小歪、来自湖北、在武汉中南民族大学本科社会学专业，目前在北漂，职位是客户支持&amp;爬虫工程师。 先说说自己的大学吧，大学时基本是不知道自己想要什么，也没有思考过自己以后去做什么这个问题，可以说是玩了三年，到了大四，面临着毕业找工作的压力，出去找实习，可是自己啥也不会啊。就是在这个情况下，我接触到了Python。 翻看自己一年前写的博客 Python练习第一题，在图片上加入数字，我在里面曾经说过这样一句话： 最后，我想给自己定个目标，2017年利用Python找到一份工作，养活自己。 在今天，也就是2017年12月31日，我可以自豪地说，我完成了我的目标。 这其中发生了很多有趣的事，我现在都说一说吧。 第一个offer临近毕业的时候当然也有跑宣讲会什么的，各种机缘巧合下，拿到了第一个offer，是食品行业的，好像是什么质检员，我感觉自己还是想去互联网行业的，所以拒绝了。 第二个offer第二个是某派新闻，号称党媒旗下的媒体。刚去的时候，做的是审核类的工作，最后才发现里面的各种坑。好险自己发现的早，时间已经来到17年的3月份。 第三个offer 到3月份，我还没找到工作，但是自己心灰意冷，很害怕刚毕业就失业，但是在武汉根本很难找到合适的工作。恰好在这个时候，认识了天善的勇哥，大家可以去看看天善的课程，都是一线大牛人物，天善智能。然后我就去上海了，去上海的一个月时间，我自己改变是非常大的。认识了很多大佬，见识了很多新的想法。 在天善做的是新媒体这一块，但是就有写这样一篇文章，叫做为了找一份Python实习，我用爬虫收集数据，很巧的事情是，现在我的BOSS真好看到了这篇文章，然后就联系我了，然后我就来了。 第四个offer 我是6月份来到北京的，到现在已经有半年了，一直呆在造数。 做过运营、做过客户支持，最近开始接触项目，可以使用Python开发项目，虽然有很多做得不好的地方，但是对我来说这是一个开始。 我每天会花很多时间去学习（周末除外，因为我要吃鸡，haha），也会经常写博客，记录自己的学习经历，接触项目之后才发现自己以前的各种毛病，这些都不是问题，坚持学习就好。 这期间，我还配合造数的首席爬虫工程师小X制作了一期Python爬虫课程，课程还是挺精彩的，我学习进阶部分后，感觉对Scrapy框架入门了。 有兴趣的可以看看，但是我并不推荐，因为学Python或者爬虫最好的还是去看文档，文档是最好的选择。这种培训课程只有一个好处，就是及时的帮你解答疑惑。 好了，不扯远了，继续回到今天的主题。 总结2017年对我来说就是“梦醒时分”，总算是在这一年完成了自己的梦想，接下来就需要自己更加努力，去弥补自己所缺乏的。 对于我这个文科生来说，确实是挺不容易的，但是就像歌词里的： 新的风暴已经出现怎么能够停滞不前穿越时空竭尽全力我会来到你身边微笑面对危险梦想成真不会遥远鼓起勇气坚定向前奇迹一定会出现 非常欢迎大家与我交流，有什么疑问可以直接在评论中说出来，我定会知无不言。 也可以加我微信交流： zhang7350 最后，如果你也想要学Python找工作，你应该先问自己一个问题： 我喜欢Python吗？","tags":[{"name":"年度总结","slug":"年度总结","permalink":"https://zhangslob.github.io/tags/年度总结/"},{"name":"2017","slug":"2017","permalink":"https://zhangslob.github.io/tags/2017/"}]},{"title":"采集方案策略之App抓包","date":"2017-12-23T13:40:26.000Z","path":"2017/12/23/采集方案策略之App抓包/","text":"这是崔斯特的第二十二篇原创文章 采集方案策略设计在群里看到有人询问饿了么的参数，正好感兴趣，就来时间一番。 这里引用下大佬的一段话： 首先大的地方，我们想抓取某个数据源，我们要知道大概有哪些路径可以获取到数据源，基本上无外乎三种： PC端网站 针对移动设备响应式设计的网站（也就是很多人说的H5, 虽然不一定是H5）； 移动App 原则是能抓移动App的，最好抓移动App，如果有针对移动设备优化的网站，就抓针对移动设备优化的网站，最后考虑PC网站。因为移动App基本都是API很简单，而移动设备访问优化的网站一般来讲都是结构简单清晰的HTML，而PC网站自然是最复杂的了；针对PC端网站和移动网站的做法一样，分析思路可以一起讲，移动App单独分析。 其实很多网页都有移动端，像微博，我知道这三个： weibo.com weibo.cn m.weibo.cn 最简单的当然是第二种了，对于今天的受害者——饿了么来说，当然，首选也是移动端。 饿了么抓包分析这里抓包工具选择Fiddler，这里不讲如何配置，具体参考 用Fiddler对Android应用进行抓包 下面打开手机的饿了么，原本以为会有数据，结果，竟然是这样 去询问了专业人士，了解了有些应用不允许用户抓包，会有相应的限制。好吧，这就能难倒我了吗？？ 当我切换到发现类目下，发现有奇怪的提示 在疯狂点击继续访问后，我终于可以正常访问了。 那么就可以在Fiddler中查看对应的数据了。这里直接把接口展示出来：饿了么接口 浏览器直接打开，貌似没有啥验证 具体分析里面的参数 1234offset:20limit:40latitude:39.93245longitude:116.50097 有4个参数， offset 和 limit 就很常见了，翻页和每页的数据，至于 latitude 和 longitude 仔细观察就知道，经纬度嘛，把它该修改为你想采集的位置的经纬度就好。 抓包分析之后，接下来采集数据就很简单了，数据字段标识： food_id 是商品ID，接口是：https://www.ele.me/restapi/shopping/v1/foods?food_ids%5B%5D=712859937 打开此链接就是商品详情内容 可爱的小笼包 restaurant_id 是店铺ID，接口 https://www.ele.me/shop/157458556 打开就是店铺详情页，当然，也有移动端：https://h5.ele.me/shop/#id=157458556 这样进行商品采集就比较轻松了。 好饿，容我先点个外卖。 微信公众号抓包分析既然都看了饿了么，那也来看看微信吧。 使用Fiddler抓出来的curl命令 1curl -k -i --raw -o 0.dat &quot;https://mp.weixin.qq.com/mp/profile_ext?action=getmsg&amp;__biz=MzA4NzA1OTc5Nw==&amp;f=json&amp;offset=20&amp;count=10&amp;appmsg_token=936_iWFH%%252F9haOTPb6GApBj6wXjPGKg9eeU7slzmH2Q~~&quot; -H &quot;User-Agent: Mozilla/5.0 (Linux; Android 7.1.1; MI 6 Build/NMF26X; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/53.0.2785.49 Mobile MQQBrowser/6.2 TBS/043632 Safari/537.36 MicroMessenger/6.5.23.1180 NetType/WIFI Language/zh_CN&quot; -H &quot;Accept-Encoding: gzip, deflate&quot; -H &quot;Accept: */*&quot; -H &quot;Connection: keep-alive&quot; -H &quot;Host: mp.weixin.qq.com&quot; -H &quot;X-Requested-With: XMLHttpRequest&quot; -H &quot;Referer: https://mp.weixin.qq.com/mp/profile_ext?action=home&amp;__biz=MzA4NzA1OTc5Nw==&amp;scene=124&amp;devicetype=android-25&amp;version=26051732&amp;lang=zh_CN&amp;nettype=WIFI&amp;a8scene=3&amp;pass_ticket=lLCqBwwrZ581bGDqrEkRsgjKkWYNPdUBs9grSaFjd79hSX0mdvR8%%2BLUbHoWGGBEp&amp;wx_header=1&quot; -H &quot;Accept-Language: zh-CN,en-US;q=0.8&quot; -H &quot;Cookie: pgv_pvi=4831552512; pgv_si=s989715456; sd_userid=18991505459750403; sd_cookie_crttime=1505459750403; tvfe_boss_uuid=a8e4e4f1ab6cd93d; pgv_info=ssid=s8735681072; pgv_pvid=4201362299; rewardsn=8d8b49dfb1811092eefe; wxtokenkey=19643e9f2ee569a10857d365bba88556d220fd33c1a0666b5d028a72b5bcd901; wxuin=838107840; devicetype=android-25; version=26051732; lang=zh_CN; pass_ticket=lLCqBwwrZ581bGDqrEkRsgjKkWYNPdUBs9grSaFjd79hSX0mdvR8+LUbHoWGGBEp; wap_sid2=CMCF0o8DElxUVDVJR3o1ZldpbDlHWWdjQ0xMU3lxM3BWTUozTFFuZFhrUEJaanhoSmZ1aEVncnU0VzFIaWR3QkVVVXFuTUlMTlkxNFZjTnRCMEt1VHJjV3UzQVNOYWdEQUFBfjD6rvjRBTgMQJRO&quot; -H &quot;Q-UA2: QV=3&amp;PL=ADR&amp;PR=WX&amp;PP=com.tencent.mm&amp;PPVN=6.5.23&amp;TBSVC=43602&amp;CO=BK&amp;COVC=043632&amp;PB=GE&amp;VE=GA&amp;DE=PHONE&amp;CHID=0&amp;LCID=9422&amp;MO= MI6 &amp;RL=1080*1920&amp;OS=7.1.1&amp;API=25&quot; -H &quot;Q-GUID: 569ade09b5931656e4f49098113e88cb&quot; -H &quot;Q-Auth: 31045b957cf33acf31e40be2f3e71c5217597676a9729f1b&quot; -H &quot;Content-Type: application/json; charset=UTF-8&quot; -H &quot;Cache-Control: no-cache, must-revalidate&quot; -H &quot;RetKey: 14&quot; -H &quot;LogicRet: 0&quot; 直接在浏览器中打开，会提示错误 12345&#123;ret: -3,errmsg: &quot;no session&quot;,cookie_count: 0&#125; 使用postman分析，最后Python的代码是 1234567891011121314151617181920212223242526272829import requestsurl = \"https://mp.weixin.qq.com/mp/profile_ext\"querystring = &#123;\"action\":\"getmsg\",\"__biz\":\"MzA4NzA1OTc5Nw==\",\"f\":\"json\",\"offset\":\"20\",\"count\":\"10\",\"appmsg_token\":\"936_iWFH%2F9haOTPb6GApBj6wXjPGKg9eeU7slzmH2Q~~\"&#125;headers = &#123; 'user-agent': \"Mozilla/5.0 (Linux; Android 7.1.1; MI 6 Build/NMF26X; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/53.0.2785.49 Mobile MQQBrowser/6.2 TBS/043632 Safari/537.36 MicroMessenger/6.5.23.1180 NetType/WIFI Language/zh_CN\", 'accept-encoding': \"gzip, deflate\", 'accept': \"*/*\", 'connection': \"keep-alive\", 'host': \"mp.weixin.qq.com\", 'x-requested-with': \"XMLHttpRequest\", 'referer': \"https://mp.weixin.qq.com/mp/profile_ext?action=home&amp;__biz=MzA4NzA1OTc5Nw==&amp;scene=124&amp;devicetype=android-25&amp;version=26051732&amp;lang=zh_CN&amp;nettype=WIFI&amp;a8scene=3&amp;pass_ticket=lLCqBwwrZ581bGDqrEkRsgjKkWYNPdUBs9grSaFjd79hSX0mdvR8%%2BLUbHoWGGBEp&amp;wx_header=1\", 'accept-language': \"zh-CN,en-US;q=0.8\", 'cookie': \"pgv_pvi=4831552512; pgv_si=s989715456; sd_userid=18991505459750403; sd_cookie_crttime=1505459750403; tvfe_boss_uuid=a8e4e4f1ab6cd93d; pgv_info=ssid=s8735681072; pgv_pvid=4201362299; rewardsn=8d8b49dfb1811092eefe; wxtokenkey=19643e9f2ee569a10857d365bba88556d220fd33c1a0666b5d028a72b5bcd901; wxuin=838107840; devicetype=android-25; version=26051732; lang=zh_CN; pass_ticket=lLCqBwwrZ581bGDqrEkRsgjKkWYNPdUBs9grSaFjd79hSX0mdvR8+LUbHoWGGBEp; wap_sid2=CMCF0o8DElxUVDVJR3o1ZldpbDlHWWdjQ0xMU3lxM3BWTUozTFFuZFhrUEJaanhoSmZ1aEVncnU0VzFIaWR3QkVVVXFuTUlMTlkxNFZjTnRCMEt1VHJjV3UzQVNOYWdEQUFBfjD6rvjRBTgMQJRO\", 'q-ua2': \"QV=3&amp;PL=ADR&amp;PR=WX&amp;PP=com.tencent.mm&amp;PPVN=6.5.23&amp;TBSVC=43602&amp;CO=BK&amp;COVC=043632&amp;PB=GE&amp;VE=GA&amp;DE=PHONE&amp;CHID=0&amp;LCID=9422&amp;MO= MI6 &amp;RL=1080*1920&amp;OS=7.1.1&amp;API=25\", 'q-guid': \"569ade09b5931656e4f49098113e88cb\", 'q-auth': \"31045b957cf33acf31e40be2f3e71c5217597676a9729f1b\", 'content-type': \"application/json; charset=UTF-8\", 'cache-control': \"no-cache\", 'retkey': \"14\", 'logicret': \"0\", &#125;response = requests.request(\"GET\", url, headers=headers, params=querystring, verify=False)print(response.json()) 这个时候的参数有 123456action:getmsg__biz:MzA4NzA1OTc5Nw==f:jsonoffset:20count:10appmsg_token:936_iWFH%%252F9haOTPb6GApBj6wXjPGKg9eeU7slzmH2Q~~ 目前还不清楚这些参数的作用，再抓一个试试 123456action:getmsg__biz:MjM5NzI3NDg4MA==f:jsonoffset:10count:10appmsg_token:936_kFNdYU3DJ%%252B%%252BVfHfEGImXqB5DMbIeqtSR75ZFZQ~~ 估计就是 __biz 和 appmsg_token 这两个参数对应不同的公众号 对了，上面的代码会出现一个问题 12InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning) 解决方法 12from requests.packages import urllib3urllib3.disable_warnings() App的分析App类使用的工具是Fidder，手机和电脑在一个局域网内，先用Fidder配置好端口，然后手机设置代理，ip为电脑的ip，端口为设置的端口，然后如果手机上请求网络内容时，Fidder会显示相应地请求，那么就ok了，分析的大体逻辑基本一致，限制会相对少很多，但是也有几种情况需要注意： 加密，App有时候也有一些加密的字段，这个时候,一般来讲都会进行反编译进行分析，找到对应的代码片段，逆推出加密方法； gzip压缩或者base64编码，base64编码的辨别度较高，有时候数据被gzip压缩了，不过Charles都是有自动解密的； https证书，有的https请求会验证证书, Fidder提供了证书，可以在官网找到，手机访问，然后信任添加就可以。 最后，祝大家圣诞节快乐","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"抓包","slug":"抓包","permalink":"https://zhangslob.github.io/tags/抓包/"}]},{"title":"【RNG vs SKT】弹幕的自然语言的初步分析","date":"2017-12-20T14:17:35.000Z","path":"2017/12/20/【RNG-vs-SKT】弹幕的自然语言的初步分析/","text":"这是崔斯特的第二十一篇原创文章 前排 @皇族电子竞技俱乐部 ================================== S7中RNG对阵SKT，想必是全世界LOL玩家关注的重点。在比赛开始前，使用小葫芦把斗鱼S7直播间的弹幕都抓下来，想着做一小点分析，看看会得出什么结论。 因为数据量和分析深度等原因，以下内容仅供娱乐观赏 数据采集使用小葫芦采集2万多条弹幕数据，RNG对阵SKT斗鱼直播间的弹幕，最后得到约4万可用数据 中文分词使用jieba分词，算法如下 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG) 采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法 简单处理之后，看起来是这样 顺便做个统计，看看都在说什么。不加停用词是这样的，全是 “666” RNG和牛逼是出现最多的词语，同时也发现“卢本伟牛逼” 弱弱问一句“唐梦琼”是谁 下面是词云，Python的词云做不来不好看，所以我使用的工具 HTML5 Word Cloud 弹幕内容词云 用户昵称词云 情感分析这里使用的是 isnowfy/snownlp。SnowNLP是一个python写的类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的。 1234from snownlp import SnowNLPs = SnowNLP(u'这个东西真心很赞')s.sentiments # 0.9769663402895832 positive的概率 有点难看，直接看数据吧，得到的结果是在 [0，1] 之间的positive的概率 51659中有44705个大于0.5，占比86.54%，有 6954条弹幕低于0.5，占比13.46%。 弹幕中积极的概率还是相当高的，说明观众还是比较赞赏比赛的。 特征提取TF-IDF是信息检索领域非常重要的搜索词重要性度量；tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。 词频TF(Term Frequency) 词w在文档d中出现次数count(w, d)和文档d中总词数size(d)的比值： 基于 TF-IDF 算法的关键词抽取： RNG rng 贺电 加油 发来 666 6666 66666 恭喜 66666666666666 666666 66666666666 6666666 skt 李哥 66666666666666666666 SKT 66666666 666666666 马甲 在没有加上停用词的前提下，可以看到效果并不理想 TextRank是在Google的PageRank算法启发下，针对文本里的句子设计的权重算法，目标是自动摘要。它利用投票的原理，让每一个单词给它的邻居（术语称窗口）投赞成票，票的权重取决于自己的票数。 基于 TextRank 算法的关键词抽取： 发来 贺电 加油 学院 机器人 大学 职业 船长 技术 小炮 没有 解说 中国 经济 开始 无敌 香锅 垃圾 老鼠 科技 特征选择Word2vec是一个将单词表征成向量的形式，它可以把文本内容的处理简化为向量空间中的向量运算，计算出向量空间上的相似度，来表示文本语义上的相似度。 计算相似度：首先是RNG： rng 0.9893965721130371 加油 0.9829007983207703 必胜 0.9790929555892944 Rng 0.9743078947067261 恭喜 0.9733642339706421 中国队 0.9397183656692505 关 0.9283092021942139 &lt; 0.9278833866119385 燕尾港 0.9222617149353027 B 0.9143030643463135 RNG加油 看看SKT： skt 0.9911665320396423 皇族 0.972029983997345 ＾ 0.970970094203949 稳住 0.9653508067131042 干死 0.9643667340278625 牛比 0.9629441499710083 一起 0.9625348448753357 @ 0.9619969725608826 鸟巢 0.9608791470527649 冠军 0.9608250856399536 稳住，干死SKT？ 贺电： 学院 0.9811943769454956 大学 0.980491042137146 技术 0.9766334295272827 职业 0.9691535234451294 电子科技 0.9668481349945068 发来 0.9619022607803345 科技 0.9594647884368896 山东 0.9568137526512146 重庆 0.9440888166427612 哈尔滨 0.939836859703064 山东XX学院发来贺电 加油： 必胜 0.9876022934913635 RNG 0.9829007983207703 rng 0.960281252861023 Rng 0.9591789841651917 恭喜 0.9551880359649658 中国队 0.9436988830566406 关 0.94183349609375 ！ 0.921385645866394 ~ 0.9148629903793335 @ 0.9062787294387817 RNG加油，RNG必胜 小狗： 吹 0.9970275163650513 无敌 0.996719241142273 神超 0.996111273765564 后期 0.9959050416946411 厉害 0.9957337975502014 凶 0.9957261681556702 强 0.9955072402954102 一个 0.9954395890235901 干 0.99541175365448 起来 0.9952359199523926 狗吹？ 李哥： 还是 0.9825356602668762 电话 0.9700809717178345 承 0.9697628617286682 心脏 0.9686012864112854 陈文泽在 0.9681863188743591 麻痹 0.9680625200271606 响 0.9674116373062134 以为 0.9664229154586792 狗哥 0.9592204689979553 不 0.9589840769767761 你李哥还是你李哥 MLXG： 宣告 0.9958090782165527 mlxg 0.9953181147575378 死亡 0.995277464389801 b 0.9949076771736145 6666 0.9947425723075867 丑 0.9943945407867432 10 0.9943088293075562 辣鸡 0.9940722584724426 干死 0.9940391778945923 锤 0.9939616918563843 香锅和死亡宣告有啥关系 小虎： 笑笑 0.9971799850463867 看到 0.9967395663261414 解说 0.9961692690849304 不是 0.9959656000137329 中单 0.9951503872871399 假 0.9950063824653625 为什么 0.9944812655448914 又 0.9942663908004761 么 0.9938984513282776 里奥 0.9937981367111206 小虎与加里奥（： letme： 难受 0.9964221715927124 笑话 0.9959778785705566 哦 0.9958946108818054 世界 0.9958213567733765 毒奶 0.9957934021949768 KPL 0.9957884550094604 上单 0.9956253170967102 瓜皮 0.9955945014953613 快 0.9953423738479614 打团 0.9953156113624573 真难受啊 To Do 可以使用朴素贝叶斯做分类模型 使用机器学习性能评估指标预测精确率和准确率 欢迎补充 可参考资料 中文分词基本算法介绍 ICTCLAS 汉语词性标注集 文本分类技术 文本分类与SVM 基于贝叶斯算法的文本分类算法 基于libsvm的中文文本分类原型 LDA-math-文本建模 情感分析资源 面向情感分析的特征抽取技术研究 斯坦福大学自然语言处理第七课-情感分析 深度学习、自然语言处理和表征方法 Deep Learning in NLP （一）词向量和语言模型","tags":[{"name":"NLP","slug":"NLP","permalink":"https://zhangslob.github.io/tags/NLP/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"https://zhangslob.github.io/tags/自然语言处理/"}]},{"title":"scrapy学习实例（四）采集淘宝数据并展示","date":"2017-12-19T13:09:07.000Z","path":"2017/12/19/scrapy学习实例（四）采集淘宝数据并展示/","text":"这是崔斯特的第二十篇原创文章 本节代码 ： zhangslob/Taobao_duoshou 万水千山总是情，给个star行不行 铛铛铛，懒惰了一段时间，咱接着学Scrapy。这一期玩点新花样，使用Flask展示我们的数据。效果如下图： 有些重复的 ╮(╯Д╰)╭ 过程简单来说有两步： 使用Scrapy采集淘宝数据 使用Flask展示数据 采集数据抓包分析其实采集淘宝数据的方法真的很多很多，这里不讨论Selenium，只说如何抓包分析，先提供几个接口，供观众老爷观赏： https://s.taobao.com/list?seller_type=taobao&amp;json=on https://s.m.taobao.com/search?event_submit_do_new_search_auction=1&amp;_input_charset=utf-8&amp;topSearch=1&amp;atype=b&amp;searchfrom=1&amp;action=home%3Aredirect_app_action&amp;from=1&amp;q=%E6%B0%B4%E6%9E%9C&amp;sst=1&amp;n=44&amp;buying=buyitnow&amp;m=api4h5&amp;abtest=25&amp;wlsort=25&amp;page=1 http://h5.m.taobao.com/app/selectassistant/www/choiceness/index.html?m=select&amp;vm=nw&amp;ttid=null&amp;utd_id=null&amp;page=2&amp;n=44&amp;q=%E6%B0%B4%E6%9E%9C 这三种接口都可以采集数据，别问我怎么知道的，经历过千百次失败。这里选择的是第二种，大家可以试试这几种的区别。 使用第二种去采集数据时，返回的是json数据，数据量已经很多。 其中有几点坑，分享下。 URL不同。淘宝和天猫的链接是不同的，移动端和网页端是不同的。 这里显示的 commentCount （评论数量）并不是真实的，你可以打开详情页对比 评论数量的接口。淘宝和天猫的都可以使用 https://rate.taobao.com/detailCount.do?itemId=商品ID，每件商品都有自己的唯一ID 代码实战这里直接把主代码贴出来，使用 Mongo 保存数据。 taobao.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687# -*- coding: utf-8 -*-import copyimport scrapyimport jsonfrom ..items import TaobaoInfoItemimport refrom traceback import format_exckw = ['网络服务', '装潢', '护理', '速食', '运动鞋', '运动服', '男装', '配件', '蔬果', '干货', '攻略', '地毯', '文具', '书籍', '人偶', '饰品', '报纸', '时尚饰品', '美发', '运动包', '粮油', '吃喝玩乐折扣券', '工具', '彩妆', '演出', '童装', '个性定制', '数码相机', '日化', '游戏', '尿片', '安防', '摄像机', '厨房电器', '办公设备', '网店', 'ZIPPO', '杂志', '礼品', '摄影器材', '喂哺等用品', '软件', '笔记本电脑', '明星', '登山', '居家日用', '户外', '电脑硬件', '流行首饰', '娃娃', '收纳', '影视', '音乐', '电玩', '音像', '香水', '水产', '热销女包', '大家电', '其他保健营养品', '箱包皮具', '瑞士军刀', '3C数码配件市场', '电脑周边', '男包', '玩具', 'U盘', '模型', '孕妇装', '窗帘', '眼镜', '促销店铺', '五金', '旅行', '洗护', '清洁', '移动存储', '卫浴', '野营', '颈环配件', '童鞋', '家装饰品', '显示器', '闪存卡', '传统滋补品', '耗材', '灯具']class TaobaoSpider(scrapy.Spider): name = 'taobao' allowed_domains = ['taobao.com'] start_urls = ['https://s.m.taobao.com/search?event_submit_do_new_search_auction=1&amp;_input_charset=utf-8&amp;topSearch=1&amp;atype=b&amp;searchfrom=1&amp;action=home%3Aredirect_app_action&amp;from=1&amp;q=&#123;&#125;&amp;sst=1&amp;n=44&amp;buying=buyitnow&amp;m=api4h5&amp;abtest=25&amp;wlsort=25&amp;page=&#123;&#125;'.format(i, y) for i in range(1, 101) for y in kw] # 其他接口 http://h5.m.taobao.com/app/selectassistant/www/choiceness/index.html?m=select&amp;vm=nw&amp;ttid=null&amp;utd_id=null&amp;page=2&amp;n=44&amp;q=%E6%B0%B4%E6%9E%9C def parse(self, response): item = TaobaoInfoItem() ListItem = json.loads(response.text)['listItem'] for i in ListItem: item['name'] = i['name'] item['title'] = i['title'] item['area'] = i['area'] # 处理不同的URL url = [] if 'https:' not in i['url']: if 'detail.m.tmall.com' in i['url']: url.append('https:' + i['url'].replace('.m','')) else: url.append('https:' + i['url']) if 'https:' in i['url']: url.append(i['url']) item['url'] = url print(item['url']) # 评论网址 comment_url = [] ur = item['url'] comment_url.append(ur[0] + '#J_Reviews') item['comment_url'] = comment_url item['fastPostFee'] = i['fastPostFee'] item['sales'] = i['act'] item['price'] = i['price'] item['originalPrice'] = i['originalPrice'] item['nick'] = i['nick'] item['id'] = i['item_id'] item['loc'] = i['sellerLoc'] # 图片链接 img_url = [] img_url.append('http:' + i['img2']) item['img_url'] = img_url count_url = [] count_url.append('https://rate.taobao.com/detailCount.do?itemId=' + i['item_id']) for url in count_url: yield scrapy.Request( url, callback=self.detail_parse, meta=&#123;'item': copy.deepcopy(item)&#125;, # 使用copy.deepcopy深复制，否则数据不对啊 dont_filter=True, errback=self.error_back ) def detail_parse(self, response): item = response.meta['item'] pat_count = '&#123;\"count\":(.*?)&#125;' item['count'] = re.findall(pat_count, str(response.body)) yield item def error_back(self, e): _ = e self.logger.error(format_exc())'''['羽绒服', '毛呢大衣', '毛衣', '冬季外套', '新品', '裤子', '连衣裙', '腔调', '秋冬新品', '淘特莱斯', '淘先生', '拾货', '秋冬外套', '时尚套装', '潮牌', '爸爸装', '春新品', '性感诱惑', '甜美清新', '简约优雅', '奢华高贵', '运动风', '塑身', '基础内衣', '轻薄款', '长款', '短款', '毛领', '加厚', '被子', '鹅绒', '新品', '秋款', '夹克', '卫衣', '西装', '风衣', '皮衣', '毛呢外套', '薄羽绒', '无钢圈', '无痕文胸', '蕾丝内衣', '运动文胸', '聚拢文胸', '大码文胸', '抹胸式', '隐形', '廓形', '双面呢', '羊绒', '中长款', '短款', '毛领', '设计师款', '系带', 'T恤', '长袖T', '打底衫', '纯色', '衬衫', '长袖款', '商务款', '时尚款', '睡衣套装', '睡裙', '睡袍浴袍', '外穿家居', '女士睡衣', '男士睡衣', '情侣睡衣', '亲子睡衣', '马海毛', '貂绒', '羊绒', '羊毛', '开衫', '中长款', '短款', '卡通', '休闲裤', '工装裤', '运动裤', '长裤', '牛仔裤', '小脚裤', '哈伦裤', '直筒裤', '女士内裤', '男士内裤', '三角裤', '平角裤', '丁字裤', '阿罗裤', '星期裤', '低腰', '外套', '套装', '风衣', '卫衣', '真皮皮衣', '马甲', '小西装', '唐装', '中老年', '薄毛衣', '针织开衫', '圆领毛衣', 'V领毛衣', '纯色毛衣', '民族风', '羊毛衫', '羊绒衫', '船袜', '男人袜', '连裤袜', '隐形袜', '收腹裤', '塑身衣', '美体裤', '收腹带', '帆布鞋', '高帮', '低帮', '内增高', '懒人鞋', '厚底', '韩版', '系带', '情侣款', '运动风鞋', '厚底', '内增高', '星星鞋', '系带', '上新', '人气款', '单肩包', '斜挎包', '手提包', '迷你包', '手拿包', '小方包', '棒球帽', '鸭舌帽', '遮阳帽', '渔夫帽', '草帽', '平顶帽', '嘻哈帽', '贝雷帽', '牛仔帽', '爵士帽', '高跟', '平底', '厚底', '中跟', '粗跟', '坡跟', '浅口', '尖头', '圆头', '运动款', '头层牛皮', '内增高', '松糕鞋', '豆豆鞋', '商务', '休闲', '潮范', '胸包', '腰包', '单肩', '斜跨', '手提', '手拿', '帆布', '牛皮', '女士腰带', '男士皮带', '帆布腰带', '腰封', '腰链', '针扣头', '平滑扣', '自动扣', '真皮', '正品', '厚底', '内增高', '星星鞋', '系带', '一脚蹬', '魔术贴', '气垫', '网状', '印花', '铆钉', '水洗皮', '卡通', '原宿', '糖果色', '商务', '运动', '帆布', '牛皮', '女士围巾', '男士围巾', '披肩', '丝巾', '假领', '小方巾', '三角巾', '大方巾', '真丝', '雪纺', '棉质', '亚麻', '蕾丝', '青春潮流', '商务皮鞋', '休闲皮鞋', '正装皮鞋', '商务休闲', '布洛克', '内增高', '反绒皮', '真皮', '潮流低帮', '韩版', '英伦', '复古', '铆钉', '编织', '豹纹', '大头', '拉杆箱', '密码箱', '学生箱', '子母箱', '拉杆包', '万向轮', '飞机轮', '航空箱', '铝框', '女士手套', '男士手套', '真皮手套', '蕾丝手套', '防晒手套', '半指手套', '分指手套', '连指手套', '短款手套', '长款手套', '皮鞋', '低帮', '反绒皮', '大头鞋', '豆豆鞋', '帆船鞋', '懒人鞋', '帆布/板鞋', '高帮', '凉鞋/拖鞋', '沙滩鞋', '人字拖', '皮凉鞋', '洞洞鞋', '钱包', '潮包馆', '真皮包', '手机包', '大牌', 'coach', 'MK', 'MCM', '毛线', '鞋垫', '鞋带', '领带', '领结', '袖扣', '手帕', '布面料', '耳套', '领带夹', '婚纱配件', '皮带扣', '英国牛栏', '英国爱他美', '美赞臣', '雅培', '澳洲爱他美', '可瑞康', '惠氏', '贝因美', '推车', '驱蚊器', '婴儿床', '理发器', '奶瓶', '餐椅', '背带腰凳', '安全座椅', '内衣', '内裤', '喂奶枕', '收腹带', '妈咪包', '待产包', '防辐射服', '储奶袋', '米粉', '肉松', '磨牙棒', '果泥', '益生菌', '清火开胃', '钙铁锌', '维生素', '花王', 'moony', '大王', '帮宝适', '雀氏', '好奇', '妈咪宝贝', '安儿乐', '海淘奶粉', '海淘辅食', '海淘营养品', '直邮花王', '海淘洗护', '海淘奶瓶', '海淘餐具', '海淘孕产', 'T恤', '连衣裙', '泳装', '套装', '衬衫', '防晒服', '半身裙', '短裤', '凉鞋', '沙滩鞋', '洞洞鞋', '网鞋', '学步鞋', '拖鞋', '帆布鞋', '宝宝鞋', '母女裙', '父子装', '亲子T恤', '亲子衬衫', '亲子套装', '母女鞋', '父子鞋', '家庭鞋', '沙滩戏水', '早教启蒙', '拼插益智', '遥控模型', '运动户外', '学习爱好', '卡通公仔', '亲子互动', '电动车', '自行车', '学步车', '手推车', '三轮车', '滑板车', '扭扭车', '儿童轮滑', '早教机', '点读机', '健身架', '布书', '串/绕珠', '床/摇铃', '爬行垫', '木质拼图', '卸妆', '面膜', '洁面', '防晒', '面霜', '爽肤水', '眼霜', '乳液', '补水', '美白', '收缩毛孔', '控油', '祛痘', '祛斑', '去黑眼圈', '去黑头', 'BB霜', '粉底液', '唇膏', '隔离', '遮瑕', '指甲油', '粉饼', '彩妆套装', '女士香水', '男士香水', '中性香水', '淡香水', '古龙水', '香精', '复方精油', '香体乳', '洗发水', '护发素', '染发', '烫发', '造型', '假发', '洗护套装', '假发配件', '美胸', '纤体', '胸部护理', '身体护理', '塑身', '脱毛', '手部保养', '足部护理', '眼线', '睫毛膏', '眼影', '眉笔', '假睫毛', '眼霜', '双眼皮贴', '眼部护理', '劲能醒肤', '清洁面膜', '男性主义', '剃须膏', '男士套装', '男士防晒', '火山岩', '爽身走珠', '抗皱', '抗敏感', '保湿', '去眼袋', '滋润', '抗氧化', '深层清洁', '雅诗兰黛', '兰蔻', '资生堂', '自然乐园', 'SK-II', '悦诗风吟', '水宝宝', '契尔氏', '芦荟胶', '彩妆盘', '腮红', '香氛', '高光棒', '修容', 'V脸', '去角质', '洁面', '爽肤水', '精华', '乳液', '鼻贴', '马油', '牛肉干', '鲜花饼', '红枣', '糖果', '巧克力', '山核桃', '松子', '卤味', '饼干', '话梅', '蔓越莓', '薯片', '奇异果', '芒果', '樱桃', '橙子', '秋葵', '苹果', '番茄', '柠檬', '椰子', '榴莲', '大米', '橄榄油', '小米', '黄豆', '赤豆', '火腿', '香肠', '木耳', '香菇', '豆瓣酱', '海参', '龙虾', '瑶柱', '土鸡', '牛排', '三文鱼', '咸鸭蛋', '皮蛋', '五花肉', '北极贝', '鸡尾酒', '红酒', '啤酒', '白酒', '梅酒', '洋酒', '清酒', '滋补酒', '茅台', '五粮液', '麦片', '咖啡', '牛奶', '柚子茶', '酸梅汤', '矿泉水', '酵素', '藕粉', '姜茶', '酸奶粉', '铁观音', '红茶', '花草茶', '龙井', '普洱', '黑茶', '碧螺春', '毛峰', '袋泡茶', '白茶', '枸杞', '人参', '石斛', '燕窝', '雪蛤', '蜂蜜', '天麻', '花粉', '党参', '红花', '芒果干', '鱼子酱', '咖啡', '橄榄油', '薯片', '巧克力', '咖喱', '方便面', '红酒', '麦片', '项链', '手链', '戒指', '发饰', '银饰', '水晶', '耳饰', '手镯', '翡翠', '彩宝', '蜜蜡', '裸钻', '珍珠', '黄金', '钻石', '金条', '和田玉', '翡翠', '水晶/佛珠', '黄金', '手表', '眼镜', '瑞士表', '机械表', '时装表', '儿童表', '电子表', '情侣表', '石英表', '手表配件', '太阳镜', '偏光镜', '近视镜', '司机镜', '护目镜', '眼镜配件', '运动镜', '老花镜', 'zippo', '电子烟', '烟斗', '瑞士军刀', '绝美酒具', '风格男表', '佛珠', '水晶', '碧玺', '925银', '施华洛', '翡翠', '珍珠', '黄金', '银项链', '流行风格', '天然水晶', '锆石水晶', '佛珠项链', '人造水晶', '925银', '翡翠', '和田玉', '复古泰银', '粉晶手镯', '黄金手镯', '日韩', '甜美', '复古/宫廷', '欧美', '瑞丽', '波西米亚', '民族风', '发饰', '项链', '套装', '耳饰', '韩式', '头饰', '三件套', '合金配件', '银饰', '水晶配珠', '琉璃', '珍珠母贝', '有机玻璃', '人造水晶', '设计师', '半包装修', '全包装修', '全案装修', '装修监理', '清包施工', '局部装修', '验房量房', '装修空气质量检测', '装修污染治理', '整体橱柜', '定制衣柜', '定制吊顶', '定制淋浴房', '门', '窗', '定制柜', '楼梯', '榻榻米定制', '地暖', '吸顶灯', '吊灯', '吸吊两用灯', '筒灯', '射灯', '台灯', '落地灯', '室外灯', '壁灯', '小夜灯', '浴室柜', '普通马桶', '花洒套装', '一体智能马桶', '智能马桶盖板', '淋浴房', '面盆龙头', '地漏', '五金挂件', '浴霸', 'PVC墙纸', '无纺布墙纸', '纯纸墙纸', '墙布', '沙粒墙纸', '绒面墙纸', '定制壁画', '3D墙纸', '实木地板', '实木复合地板', '强化复合地板', '竹地板', '户外地板', 'PVC地板', '防静电地板', '防潮膜', '踢脚线', '地板龙骨', '仿古砖', '釉面砖', '玻化砖', '微晶石', '马赛克', '抛晶砖', '通体砖', '花片', '腰线', '瓷砖背景墙', '插座', '开关', '电线', '监控器材', '智能家居', '防盗报警器材', '消防报警设备', '接线板插头', '布线箱', '断路器', '涂料乳胶漆', '油漆', '水管', '板材', '木方', '阳光房', '线条', '天然大理石', '人造大理石', '防水涂料', '实木床', '布艺床', '皮艺床', '床垫', '衣柜', '斗柜', '梳妆台', '子母床', '床头柜', '儿童床', '皮艺沙发', '布艺沙发', '沙发床', '实木沙发', '懒人沙发', '电视柜', '茶几', '鞋柜', '玄关厅', '衣帽架', '餐桌', '折叠餐桌', '欧式餐桌', '实木餐桌', '大理石餐桌', '餐椅', '餐边柜', '换鞋凳', '角柜', '屏风', '餐桌', '折叠餐桌', '欧式餐桌', '实木餐桌', '大理石餐桌', '餐椅', '餐边柜', '换鞋凳', '角柜', '屏风', '蚊帐', '三开蚊帐', '凉席', '凉席套件', '冰丝席', '藤席', '牛皮席', '夏凉被', '空调被', '天丝套件', '床单', '床笠', '四件套', '全棉套件', '被套', '蚕丝被', '羽绒被', '枕头', '乳胶枕', '记忆枕', '床褥', '毛毯', '定制窗帘', '地毯', '沙发垫', '靠垫', '桌布桌旗', '飘窗垫', '地垫', '餐垫', '防尘罩', '椅垫', '成品窗帘', '沙发罩', '摆件', '花瓶', '仿真花', '台钟闹钟', '香薰炉', '储物罐', '装饰碗盘', '木雕', '烟灰缸', '纸巾盒', '蜡烛烛台', '仿真饰品', '现代装饰画', '无框画', '后现代画', '油画', '挂钟', '照片墙', '新中式', '北欧家饰', '美式乡村', '挂钩搁板', '装饰挂钩', '壁饰', '扇子', '毛巾', '浴巾', '口罩', '隔音耳塞', '竹炭包', '眼罩', '夏季凉拖', '居家鞋', '夏季清凉', '湿巾', '晴雨伞', '驱蚊灯', '驱蚊液', '冰格', '保鲜产品', '密封罐', '防潮制品', '电扇/冰垫', '5元小物', '被子防尘袋', '收纳盒', '收纳袋', '大衣/西服罩', '护洗袋', '收纳凳', '鞋柜', '置物架', '桌用收纳', '内衣收纳', '洗发护发', '沐浴露', '漱口水', '卫生巾', '洗手液', '牙膏', '纸巾', '香皂', '沐浴球/浴擦/浴刷', '指甲刀', '剃须刮毛刀', '沐浴球', '浴室角架', '浴帘杆', '拖把', '垃圾桶', '梳子镜子', '围裙', '百洁布', '海绵擦', '餐具', '锅具', '刀具', '炖锅', '蒸锅', '汤锅', '煎锅', '压力锅', '炒锅', '菜板砧板', '一次性餐桌用品', '酒杯酒具', '咖啡器具', '碗盘碟', '刀叉勺', '餐具瓷器套装', '餐桌小物', '饭盒', '厨房储物', '一次性餐桌用品', '茶具', '茶壶', '飘逸杯', '功夫茶杯', '玻璃杯', '杯垫', '保温杯', '马克杯', '保温壶', '情侣杯', '晒衣篮', '晾衣杆', '脏衣篮', '衣架', '家庭清洁剂', '蓝泡泡', '管道疏通器', '塑胶手套', '医药箱', '垃圾袋', '汽车首页', '新车先购', '车海淘', '二手车', '爱车估价', 'suv', '别克', '大众', '宝马', '座垫', '座套', '脚垫', '香水', '旅行床', '遮阳挡', '挂件摆件', '安全座椅', '专车专用座垫', '脚垫', '安全座椅', '香水', '钥匙包', '挂件', '座套', '后备箱垫', '置物箱', '智能车机', '后视镜', '安卓导航', '便携GPS', 'DVD导航', '电子狗', '流动测速', '导航软件', '记录仪', '预警仪', 'GPS', '车机', '倒车雷达', '智能后视镜', '蓝牙', '防盗器', 'MP3', '4S保养', '电瓶安装', '配件安装', '隔热膜', '洗车卡', '镀晶镀膜', '连锁保养', '上门服务', '行车记录仪', '逆变器', '跟踪器', '充电器', '充气泵', '胎压监测', '车载冰箱', '空气净化', '车衣', 'SUV踏板', '晴雨挡', '改色膜', '汽车车标', '车牌架', '轮胎', '雨刮器', '机油滤芯', '空气滤芯', '空调滤芯', '减震', '刹车片', '火花塞', '轮胎', '雨刮', '机油', '高亮大灯', '挡泥板', '保险杠', '车顶架', '轮眉', '轮毂', '排气', '保险杠', '汽车包围', '氙气灯', '车顶架', '脚踏板', '大灯总成', '尾翼', '轮毂', '汽车装饰灯', '排气筒', '尾喉', '车身饰条', '添加剂', '防冻液', '玻璃水', '车蜡', '补漆笔', '洗车机', '洗车水枪', '车掸蜡拖', '车蜡', '洗车机', '补漆笔', '抛光机', '打蜡海绵', '车用水桶', '擦车巾', '车刷', '装饰条', '车贴', '尾喉', '改色膜', '防爆膜', '晴雨挡', '日行灯', '车衣', '夏季座垫', '遮阳挡', '防眩蓝镜', '防晒手套', 'iPhone', '小米', '华为', '三星', '魅族', '纽扣', '酷派', 'VIVO', 'iPad', '小米', '三星', '10寸', '台电', 'win8', '蓝魔', '华为', 'DIY电脑', '一体机', '路由器', '显示器', '学生', 'CPU', '移动硬盘', '无线鼠标', '苹果', '联想', 'Thinkpad', '戴尔', '华硕', 'Acer', '神州', '三星', '单反', '自拍神器', '拍立得', '佳能', '微单', '镜头', '卡西欧', '尼康', '充电宝', '智能穿戴', '蓝牙耳机', 'iPhone6壳', '电脑包', '手机贴膜', '手机壳套', '三脚架', '保护壳套', '炫彩贴膜', '移动电源', '相机配件', '手机零件', '自拍神器', '移动POS支付', '电池', '儿童手表', 'Apple Watch', '智能手表', '智能手环', '智能配饰', '智能健康', '智能排插', '智能眼镜', '游戏掌机', '家用游戏机', '游戏手柄', 'PS主机', 'XBOX', '任天堂配件', 'PS主机配件', 'XBOX配件', '路由器', '网关', '交换机', '光纤设备', '网络存储设备', '无线上网卡', 'TP-LINK', '小米路由器', 'MP3', 'MP4', '录音笔', '索尼', '飞利浦', 'ipod', '爱国者', '耳机', 'U盘', '闪存卡', '记忆棒', '移动硬盘', '希捷', '三星', 'Sandisk', '金士顿', '电磁炉', '电水壶', '料理机', '电饭煲', '榨汁机', '净水器', '豆浆机', '烤箱', '电风扇', '空调扇', '挂烫机', '扫地机', '吸尘器', '加湿器', '除湿机', '对讲机', '空气净化', '理发器', '电子称', '美容仪', '按摩椅', '按摩披肩', '血压计', '足浴器', '电动牙刷', '剃须刀', '耳机', '音响', '网络机顶盒', '麦克风', '扩音器', 'HiFi套装', '蓝光DVD', '低音炮', '打印机', '投影仪', '硒鼓墨盒', 'A4纸', '一体机', '学生文具', '保险柜', '电纸书', '学习机', '冰箱', '空调', '平板电视', '油烟机', '燃气灶', '消毒柜', '厨电套装', '热水器', '洗衣机', '包装设备', '包装纸箱', '塑料袋', '包装胶带', '铭牌', '快递袋', '气泡膜', '真空机', '笔记本', '文件袋', '钢笔', '胶粘用品', '铅笔', '计算器', '白板', '台历', '设计定制', '企业用品定制', 'T恤印制', '杯子定制', 'ppt模板', '班服定制', '洗照片', '人偶定制', '电子电工', '气动元件', '水泵', '阀门', '电钻', '焊接设备', '万用表', '雕刻机', '办公家具', '商业设施', '办公桌', '陈列柜', '货架', '广告牌', '文件柜', '沙发', '网络设备', '电子元器件', '路由器', '交换机', '光纤设备', '视频会议', '无线安全保密', '机柜', '餐饮美食', '冰淇淋', '火锅', '购物卡券', '体检配镜', '美容美甲', '保险理财', '婚纱摄影', '旅行团购', '住在帝都', '住在魔都', '住在杭州', '住在南京', '住在广州', '住在青岛', '住在宁波', '住在成都', '少儿英语', '小学教育', '潜能开发', '家长训练', '孕产育儿', '少儿绘画', '婴幼早教', '音乐', 'Q币充值', '点卡充值', '充游戏币', '游戏代练', '超值账号', '手游充值', '电竞比赛', '游戏帮派', '潇洒一室', '靠谱二室', '舒适三房', '大四室', '私藏别墅', '景观居所', '轨道沿线', '学区房', '实用英语', '网站制作', 'IT技能', '会计职称', '一对一', '办公软件', '日语', '编程', '英雄联盟', '剑侠情缘3', '征途2', '魔域', '我叫MT', '刀塔传奇', 'DOTA2', 'DNF', '魔兽世界', '自助餐', '个性写真', '儿童写真', '电影票团购', '上门服务', '周边旅游', '境外旅游', '基金理财', '魅力健身', '时尚美妆', '手工DIY', '舞蹈', '减肥瑜伽', '个人形象', '美剧英语', '摄影', '美女陪练', '轻松甩肉', '基金理财', '淘宝美工', '办公技能', '婚纱摄影', '婚礼策划', '三亚婚拍', '厦门婚拍', '青岛婚拍', '北京婚拍', '杭州婚拍', '上海婚拍', '新娘跟妆', '婚礼跟拍', '婚礼司仪', '婚车租赁', '任意洗', '洗外套', '洗西装', '洗鞋', '洗四件套', '洗烫衬衫', '皮包护理', '洗窗帘', '洗地毯', '在线洗衣', '洗礼服', '洗玩具', '开荒保洁', '厨房保洁', '公司保洁', '家电清洗', '空调清洗', '洗油烟机', '冰箱清洗', '擦玻璃', '家政服务', '家庭保洁', '保洁服务', '钟点工', '洗衣机清洗', '卫生间保洁', '上门养车', '洗车', '封釉镀膜', '内饰清洗', '空调清洗', '汽车维修', '充加油卡', '年检代办', '玻璃贴膜', '汽车装饰', '底盘装甲', '四轮定位', '汽车改装', '违章代办', '汽车隔音', '上门按摩', '常规体检', '入职体检', '老人体检', '四维彩超', '孕前检查', '体检报告', '专业洗牙', '烤瓷牙', '胃部检测', '月嫂', '催乳师', '育儿嫂', '营养师', '普通保姆', '涉外保姆', '产后陪护', '临时看护', '管家', '烧饭阿姨', '宠物寄养', '宠物美容', '宠物配种', '宠物洗澡', '宠物摄影', '宠物托运', '宠物训练', '宠物医疗', '水族服务', '宠物绝育', '宠物洗牙', '宠物造型', '宠物体检', '居家搬家', '公司搬运', '空调拆装', '家电搬运', '家具搬运', '打孔', '电路维修', '甲醛测试', '开锁换锁', '杀虫消毒', '高空清洁', '除尘除螨', '网上办事', '代缴费', '代排队', '交罚单', '叫醒服务', '宝宝起名', '学车报名', '代邮代取', '话费充值', '代送鲜花', '水电煤缴费', '同城速递', '代办档案', '宽带费', '机场停车', '专利申请', '法律咨询', '专业翻译', '开发建站', '图片处理', '视频制作', '名片制作', '商标转让', '打印', '复印', '商标注册', '私人律师', '合同文书', '出国翻译', '手机维修', 'pad维修', '修台式机', '相机维修', '修笔记本', '修复印机', '修游戏机', '修导航仪', '软件服务', '延保服务', '硬件维修', '苹果维修', '小米维修', '三星维修', '安卓刷机', '数据恢复', '电脑维修', 'ipad维修', '华为维修', '重装系统', '家电维修', '相机维修', '硬盘维修', '苹果换屏', '换主板', '名企招聘', '高薪岗位', '文案编辑', '网店推广', '开发技术', '活动策划', '美工设计', '金牌客服', '大促客服', '网页设计', '人才认证', '图片设计', '摄影师', '店长', '运营主管', '客服主管', '美工主管', '跑步鞋', '篮球鞋', '休闲鞋', '足球鞋', '帆布鞋', '训练鞋', '徒步鞋', '登山鞋', '限量版', '板鞋', 'Rosherun', '运动套装', '运动卫衣', '长裤', '皮肤风衣', '健身服', '球服', '耐克', '阿迪达斯', '三叶草', '美津浓', '彪马', '狼爪', '山地车', '公路车', '骑行服', '头盔', '装备', '零件', '工具', '护具', '折叠车', '死飞', '水壶架', '行李架', '羽毛球拍', '羽毛球服', '羽毛球', '网球拍', '篮球', '篮球服', '足球', '足球服', '乒乓球拍', '橄榄球', '台球', '高尔夫', '吊床', '头灯', '遮阳棚', '望远镜', '照明', '野营帐篷', '野外照明', '烧烤炉', '望远镜', '潜水镜', '防潮垫', '皮划艇', '皮肤衣', '防晒衣', '冲锋衣', '探路者', '速干裤', '迷彩服', '战术靴', '登山鞋', 'crocs', '溯溪鞋', '户外鞋', '麻将机', '轮滑', '麻将', '象棋', '雀友', '飞镖', '桌上足球', '风筝', '陀螺', '空竹', '沙袋', '太极服', '甩脂机', '轮滑装备', '跑步机', '舞蹈', '瑜伽', '哑铃', '仰卧板', '踏步机', '划船机', '卧推器', '健身车', '呼啦圈', '舞蹈', '瑜伽', '广场舞', '舞蹈鞋', '拉丁鞋', '广场舞套装', '肚皮舞服装', '瑜伽垫', '瑜伽球', '瑜伽服', '鱼饵', '套装', '路亚', '附件', '鱼钩', '钓鱼工具', '船/艇', '台钓竿', '海钓竿', '溪流竿', '路亚竿', '矶钓杆', '单肩背包', '旅行包', '双肩背包', '挎包', '户外摄影包', '头巾', '运动水壶', '防水包', '电池', '电自行车', '平衡车', '滑板车', '头盔', '摩托车', '老年代步', '独轮车', '遮阳伞', '扭扭车', '折叠车', '仿真植物', '干花', 'DIY花', '手捧花', '鲜果蓝', '仿真蔬果', '开业花篮', '花瓶', '绿植同城', '园艺方案', '多肉植物', '桌面盆栽', '蔬菜种子', '水培花卉', '苔藓景观', '空气凤梨', '肥料', '花盆花器', '花卉药剂', '营养土', '园艺工具', '洒水壶', '花架', '铺面石', '热带鱼', '孔雀鱼', '底栖鱼', '虾螺', '龙鱼', '罗汉鱼', '锦鲤', '金鱼', '水母', '灯科鱼', '乌龟', '水草', '底砂', '水草泥', '沉木', '仿真水草', '假山', '氧气泵', '过滤器', '水草灯', '加热棒', '鱼粮', '水质维护', '硝化细菌', '除藻剂', '龟粮', '兔兔', '仓鼠', '龙猫', '雪貂', '粮食零食', '医疗保健', '笼子', '鹦鹉', '鸟笼', '观赏鸟', '蚂蚁工坊', '蜘蛛', '蚕', '大牌狗粮', '宠物服饰', '狗厕所', '宠物窝', '航空箱', '海藻粉', '羊奶粉', '宠物笼', '储粮桶', '剃毛器', '营养膏', '上门服务', '全新钢琴', '智能钢琴', '中古钢琴', '尤克里里', '民谣吉他', '萨克斯风', '口琴', '小提琴', '高达', '手办', '盒蛋', '兵人', '变形金刚', '圣衣神话', '钢铁侠', 'BJD', '拼装', '人偶', '猫砂', '猫粮', '猫爬架', '猫窝', '猫砂盆', '化毛膏', '猫罐头', '喂食器', '折耳猫', '猫抓板', '猫玩具', '猫笼', '古筝', '二胡', '葫芦丝', '战鼓', '古琴', '陶笛', '琵琶', '笛子', '动漫T恤', '动漫抱枕', 'COS', '背包', '项链', '颜文字', '哆啦A梦', '大白', '手表', '盗墓笔记', '海贼', '火影', 'LOL', '杀菌剂', '杀虫剂', '除草剂', '调节剂', '杀螨剂', '杀鼠剂', '敌敌畏', '草甘膦', '园林种苗', '动物种苗', '蔬菜种苗', '水果种苗', '粮油种子', '药材种苗', '食用菌种', '辣木籽', '氮肥', '磷肥', '钾肥', '叶面肥', '新型肥料', '复合肥', '生物肥料', '有机肥', '耕种机械', '收割机械', '农机配件', '植保机械', '拖拉机', '施肥机械', '粮油设备', '微耕机', '塑料薄膜', '大棚膜', '防渗膜', '鱼塘专用', '薄膜', '遮阳网', '篷布', '防虫网', '镰刀', '锹', '高压水枪', '锨', '镐', '耙子', '锄头', '叉', '猪饲料', '羊饲料', '牛饲料', '预混料', '饲料原料', '全价料', '饲料添加剂', '浓缩料', '加工设备', '养殖器械', '渔业用具', '养殖服务', '配种服务', '养鸡设备', '挤奶机', '母猪产床', '化学药', '中兽药', '抗生素', '驱虫', '消毒剂', '疫苗', '阿莫西林', '氟苯尼考']''' 说明几点： 在使用 meta 传递数据的时候，要使用 copy.deepcopy 深复制，详情查阅 【scrapy】item传递出错 关于搜索词。因为淘宝对搜索结果只会返回100页，所以我们这里增加索引词，来获得更多数据。可以在代码末尾发现更多关键词，有1000多个，这个list是从这里获得 淘宝首页行业市场 反爬处理呃呃，有意思的是，自己跑了一会获得了几万条数据，并没有任何的异常，没有在 middlewares.py 加代理，连 UA 都是固定的，好奇怪。 可能长得帅的人品都比较好吧。 展示数据mongo 中的数据 数据并不直观，所以我们选择展示出来，做一个小小的聚合搜索 同样看看主文件 server.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# -*- coding: UTF-8 -*-from flask import Flask, request, session, g, redirect, url_for, \\ abort, render_template, flashfrom bson import json_utilfrom bson.objectid import ObjectIdimport jsonimport pymongoconn = pymongo.MongoClient('localhost', 27017)db = conn['taobao']goods_coll = db['search']cate_coll = db['categories']app = Flask(__name__)def toJson(data): return json.dumps( data, default=json_util.default, ensure_ascii=False )@app.errorhandler(404)def page_not_found(e): return render_template('404.html'), 404@app.route('/', methods=['GET'])def index(): if request.method == 'GET': total = goods_coll.count() return render_template('index.html', total=total) #if request.form['key']: # key = request.form['key'] # return redirect(url_for('get_goods', key=key, page=1))@app.route('/search', methods=['GET'])@app.route('/search/&lt;item&gt;', methods=['GET'])def get_goods(item=None): if request.method == 'GET': page = request.args.get('page', 1, type=int) limit = request.args.get('limit', 30, type=int) p = (page - 1) * limit offset = request.args.get('offset', p, type=int) catid = request.args.get('catid', None, type=str) jsons = request.args.get('json', 'off') keyword = request.args.get('key', '') if not keyword: keyword = item if catid: cursor = goods_coll.find(&#123;'categories.catid': catid&#125;) else: cursor = goods_coll.find(&#123;'title': &#123;'$regex': keyword&#125; &#125;) #total = cursor.count() #flash('已查询到 %d 个结果.'%total) results = cursor.skip(offset).limit(limit) resultList = [] for result in results: resultList.append(result) if jsons == 'off': return render_template('search.html', entries=resultList) else: return toJson(resultList)if __name__ == '__main__': #app.run(host='0.0.0.0') app.run(debug=True) 这里是借鉴了别人的项目，地址在这 1dot75cm/taobaobao 您可以自定义在 \\templates目录下修改对应的html文件 12λ ls404.html categories.html index.html layout.html search.html 直接运行 server.py 12345λ piython server.py * Restarting with stat * Debugger is active! * Debugger PIN: 503-651-032 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) 打开 http://127.0.0.1:5000/ 即可查看结果 To Do 数据去重 价格及销量可视化展示 速度太慢，应使用分布式 评论采集 加强爬虫的反爬措施 Github ： zhangslob/Taobao_duoshou 万水千山总是情，给个star行不行 参考 1dot75cm/taobaobao 【scrapy】item传递出错","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"Scrapy","slug":"Scrapy","permalink":"https://zhangslob.github.io/tags/Scrapy/"},{"name":"淘宝","slug":"淘宝","permalink":"https://zhangslob.github.io/tags/淘宝/"},{"name":"Flask","slug":"Flask","permalink":"https://zhangslob.github.io/tags/Flask/"}]},{"title":"Scrapy学习实例（三）采集批量网页","date":"2017-12-12T13:11:17.000Z","path":"2017/12/12/Scrapy学习实例（三）采集批量网页/","text":"这是崔斯特的第十九篇原创文章 先来首火影压压惊 (｡・`ω´･) 最开始接触 Rules是在Scrapy的文档上看到的，但是并看读懂这是什么意思。接下来看别人的案例，有使用到Rules，便花了很多时间去了解。 解释：Rule是在定义抽取链接的规则，上面的两条规则分别对应列表页的各个分页页面和详情页，关键点在于通过restrict_xpath来限定只从页面特定的部分来抽取接下来将要爬取的链接。 其实用我的话来说就是，一个是可以便捷的进行翻页操作，二是可以采集二级页面，相当于打开获得详情页内容。所以若使用了 Rules，可以便捷的帮助我们采集批量网页。 官方文档CrawlSpider示例 12345678910111213141516171819202122232425import scrapyfrom scrapy.spiders import CrawlSpider, Rulefrom scrapy.linkextractors import LinkExtractorclass MySpider(CrawlSpider): name = 'example.com' allowed_domains = ['example.com'] start_urls = ['http://www.example.com'] rules = ( # Extract links matching 'category.php' (but not matching 'subsection.php') # and follow links from them (since no callback means follow=True by default). Rule(LinkExtractor(allow=('category\\.php', ), deny=('subsection\\.php', ))), # Extract links matching 'item.php' and parse them with the spider's method parse_item Rule(LinkExtractor(allow=('item\\.php', )), callback='parse_item'), ) def parse_item(self, response): self.logger.info('Hi, this is an item page! %s', response.url) item = scrapy.Item() item['id'] = response.xpath('//td[@id=\"item_id\"]/text()').re(r'ID: (\\d+)') item['name'] = response.xpath('//td[@id=\"item_name\"]/text()').extract() item['description'] = response.xpath('//td[@id=\"item_description\"]/text()').extract() return item 该spider将从example.com的首页开始爬取，获取category以及item的链接并对后者使用 parse_item 方法。 对于每个item response，将使用XPath从HTML中提取一些数据，并使用它填充Item。 实际应用为了更好的理解，我们来看看实际案例中Rules如何使用 豆瓣应用12345rules = [Rule(LinkExtractor(allow=(r'https://movie.douban.com/top250\\?start=\\d+.*'))), Rule(LinkExtractor(allow=(r'https://movie.douban.com/subject/\\d+')), callback='parse_item', follow=False)] 如果接触过django，那么可以发现这个规则与django的路由系统十分相似（django都已经忘完了 -_-！），其实这里使用的正则匹配。 使用 r&#39;https://movie.douban.com/top250\\?start=\\d+.*&#39;来匹配翻页链接，如： https://movie.douban.com/top250?start=25&amp;filter= https://movie.douban.com/top250?start=50&amp;filter= 使用https://movie.douban.com/subject/\\d+来匹配具体电影的链接，如： https://movie.douban.com/subject/1292052/ https://movie.douban.com/subject/1291546/ 链家应用爬虫的通常需要在一个网页里面爬去其他的链接，然后一层一层往下爬，scrapy提供了LinkExtractor类用于对网页链接的提取，使用LinkExtractor需要使用CrawlSpider爬虫类中，CrawlSpider与Spider相比主要是多了rules，可以添加一些规则，先看下面这个例子，爬取链家网的链接 1234567891011121314151617181920212223from scrapy.spiders import CrawlSpider, Rulefrom scrapy.linkextractors import LinkExtractorclass LianjiaSpider(CrawlSpider): name = \"lianjia\" allowed_domains = [\"lianjia.com\"] start_urls = [ \"http://bj.lianjia.com/ershoufang/\" ] rules = [ # 匹配正则表达式,处理下一页 Rule(LinkExtractor(allow=(r'http://bj.lianjia.com/ershoufang/pg\\s+$',)), callback='parse_item'), # 匹配正则表达式,结果加到url列表中,设置请求预处理函数 # Rule(FangLinkExtractor(allow=('http://www.lianjia.com/client/', )), follow=True, process_request='add_cookie') ] def parse_item(self, response): # 这里与之前的parse方法一样，处理 pass 同样的，使用r&#39;http://bj.lianjia.com/ershoufang/pg\\s+$&#39;来匹配下一页链接，如： https://bj.lianjia.com/ershoufang/pg2/ https://bj.lianjia.com/ershoufang/pg3/ 还可以使用 r&#39;https://bj.lianjia.com/ershoufang/\\d+.html&#39;来匹配详情页链接，如： https://bj.lianjia.com/ershoufang/101102126888.html https://bj.lianjia.com/ershoufang/101100845676.html 学习参数Rule对象Role对象有下面参数 link_extractor：链接提取规则 callback：link_extractor提取的链接的请求结果的回调 cb_kwargs：附加参数，可以在回调函数中获取到 follow：表示提取的链接请求完成后是否还要应用当前规则（boolean），如果为False则不会对提取出来的网页进行进一步提取，默认为False process_links：处理所有的链接的回调，用于处理从response提取的links，通常用于过滤（参数为link列表） process_request：链接请求预处理（添加header或cookie等） LinkExtractorLinkExtractor常用的参数有： allow：提取满足正则表达式的链接 deny：排除正则表达式匹配的链接（优先级高于allow） allow_domains：允许的域名（可以是str或list） deny_domains：排除的域名（可以是str或list） restrict_xpaths：提取满足XPath选择条件的链接（可以是str或list） restrict_css：提取满足css选择条件的链接（可以是str或list） tags：提取指定标签下的链接，默认从a和area中提取（可以是str或list） attrs：提取满足拥有属性的链接，默认为href（类型为list） unique：链接是否去重（类型为boolean） process_value：值处理函数（优先级大于allow） 关于LinkExtractor的详细参数介绍见官网 注意：在编写抓取Spider规则时，避免使用parse作为回调，因为CrawlSpider使用parse方法自己实现其逻辑。因此，如果你覆盖parse方法，爬行Spider将不再工作。 最后说一个自己犯过的低级错误，我用Scrapy有个习惯，创建一个项目之后，直接cd目录，然后使用genspider命令，然后。。 12345678910111213141516D:\\Backup\\桌面λ scrapy startproject exampleNew Scrapy project &apos;example&apos;, using template directory &apos;c:\\\\users\\\\administrator\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\site-packages\\\\scrapy\\\\templates\\\\project&apos;, created in: D:\\Backup\\桌面\\exampleYou can start your first spider with: cd example scrapy genspider example example.comD:\\Backup\\桌面λ cd exampleD:\\Backup\\桌面\\exampleλ scrapy genspider em example.comCreated spider &apos;em&apos; using template &apos;basic&apos; in module: example.spiders.em 然后我的em.py就变成了这样： 1234567891011# -*- coding: utf-8 -*-import scrapyclass EmSpider(scrapy.Spider): name = 'em' allowed_domains = ['example.com'] start_urls = ['http://example.com/'] def parse(self, response): pass 注意，这个时候是不能使用Rules方法的，因为object不对，应该是 class EmSpider(CrawlSpider) 而不是class EmSpider(scrapy.Spider): 共勉！！！ 下一节应该会讲到Scrapy中各个组件的作用，以及这张神图 参考： CrawlSpider示例 scrapy学习笔记","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"Scrapy","slug":"Scrapy","permalink":"https://zhangslob.github.io/tags/Scrapy/"},{"name":"Rules","slug":"Rules","permalink":"https://zhangslob.github.io/tags/Rules/"}]},{"title":"统计学学习笔记（二）数据整理与展示","date":"2017-12-06T14:19:18.000Z","path":"2017/12/06/统计学学习笔记（二）数据整理与展示/","text":"这是崔斯特的第十八篇原创文章 这是统计学的第二篇笔记，主要记录了如何整理数据与展示数据，书本上是这样说，但是我觉得现在人们会更多的叫做数据清洗与数据可视化。命名无所谓，掌握方法就好。 下面是正文。 可以接着这里看哦 统计学学习笔记（一） 数据整理与展示3.1 数据的预处理3.1.1 数据审核 概念：检查数据是否有错误 对于通过调查取得的原始数据，主要从完整性和准确性两个方面去审核 对于通过其他渠道获得的二手数据，主要审核数据的适用性和时效性 3.1.2 数据筛选 删除某些不符合要求的数据和有明显错误的数据 将符合某种特定条件的数据筛选出来，而将不符合特定条件的数据予以剔除 Excel举例 3.1.3 数据排序 概念：按一定顺序将数据排列，以便于研究者通过数据发现一些明显的特征或趋势，找到解决问题的线索 通过数据类型选择排序方式：字母型数据、汉字型数据、数值型数据 这个好像也叫做数据清洗 3.2 品质数据的整理与展示3.2.1 频数与频数分布 落在某一特定类别中的数据个数，成为频数 把各个类别及落在其中的相应频数全部列出，并用表格形式表现出来，成为频数分布 好像不止表格吧 一个总体（或样本）中各个部分的数据与全部数据之比，成为比例 将比例乘以100得到的数值，成为百分比或百分数用%表示 总体（或样本）中各不同类别数值之间的比值，成为比率 3.2.2 品质数据的展示 条形图 饼图 环形图 3.3 数值型数据的整理与展示3.3.1 数据分组 根据统计分析的需要，将原始数据按照某种标准划分成不同的组别，成为数据分组 在组距分组中，一个组的最小值称为下限，一个组的最大值称为上限 每一组的上限和下限之间的中间值称为组中值 3.3.2 数值型数据的图示 分组数据看分布：直方图（histogram） 未分组数据看分布：茎叶图和箱型图 多变量数据的展示：雷达图 3.4 使用图表的注意事项优秀图表特征： 显示数据 让读者把注意力放在图表内容上 避免歪曲 强调数据之间的比较 服务于一个明确的目的 有对图形的描述统计和文字说明 优秀图形应当： 精心设计，有助于洞察问题的实质 使复杂的观点得到简明、确切、高效的阐述 能在最短的时间内、以最少的笔墨给读者提供大量的信息 是多维的 表述数据的真实情况 品质数据您应该会和我一样提问，什么是品质数据？ 品质数据:对产品或商品进行各种化学、物理、力学等试验后所得出的数据。 品质数据：对产品或商品进行各种化学、物理、力学等试验后所得出的数据。 品质型数据：按品质标志分组所得到数据，包括分类数据和顺序数据，他们在整理和图形展示上的方法大体相同。 本文中提到的品质数据应该是后者 数据可视化这里重点说下数据图表的选择。 就我自己的工作中，比较常用的就是直方图、折线图与饼图，词云图（如果算的话）。教材中也说了很多没用过的图，这个东西还是要根据自己的具体业务来操作。 这里推荐一个很好的网站，图表使用 这个网站有多好，你一看便知，不多解释。 数据清洗有人说：一个分析项目基本八成时间在洗数据。那么什么是清洗数据。 数据清洗是指发现并纠正数据文件中可识别的错误的最后一道程序，包括检查数据一致性，处理无效值和缺失值等。与问卷审核不同，录入后的数据清理一般是由计算机而不是人工完成。 数据清洗的方法 解决不完整数据（ 即值缺失）的方法 错误值的检测及解决方法 重复记录的检测及消除方法 不一致性（ 数据源内部及数据源之间）的检测及解决方法 转换构造 数据压缩 老规矩，还是放一点代码 123456789101112131415161718192021222324252627282930313233343536373839import numpy as npimport matplotlib.pyplot as pltimport matplotlib.patches as patchesimport matplotlib.path as pathfig, ax = plt.subplots()# Fixing random state for reproducibilitynp.random.seed(19680801)# histogram our data with numpydata = np.random.randn(1000)n, bins = np.histogram(data, 50)# get the corners of the rectangles for the histogramleft = np.array(bins[:-1])right = np.array(bins[1:])bottom = np.zeros(len(left))top = bottom + n# we need a (numrects x numsides x 2) numpy array for the path helper# function to build a compound pathXY = np.array([[left, left, right, right], [bottom, top, top, bottom]]).T# get the Path objectbarpath = path.Path.make_compound_path_from_polys(XY)# make a patch out of itpatch = patches.PathPatch(barpath)ax.add_patch(patch)# update the view limitsax.set_xlim(left[0], right[-1])ax.set_ylim(bottom.min(), top.max())plt.show() 参考 数据清洗 数据挖掘中常用的数据清洗方法有哪些？","tags":[{"name":"统计学","slug":"统计学","permalink":"https://zhangslob.github.io/tags/统计学/"},{"name":"数据","slug":"数据","permalink":"https://zhangslob.github.io/tags/数据/"}]},{"title":"Scrapy学习实例（二）采集无限滚动页面","date":"2017-12-03T11:48:53.000Z","path":"2017/12/03/Scrapy学习实例（二）采集无限滚动页面/","text":"这是崔斯特的第十七篇原创文章 上一篇写的是采集虎嗅网首页的新闻数据，有朋友对我说，采集多页试试看。后来研究下，虎嗅网首页是POST加载，Form Data中携带参数，所以只需要带上一个循环就好了。这是我最初的想法，先让我们看看Scrapy中如何采集无限滚动页面？ 先举个栗子，采集网站是quotes 分析网页 下拉时，会发现更多新的请求，观察这些请求，返回的都是json数据，也就是我们所需的，再看看他们的不同，也就是参数的改变，完整链接是： 123http://spidyquotes.herokuapp.com/api/quotes?page=2http://spidyquotes.herokuapp.com/api/quotes?page=3http://spidyquotes.herokuapp.com/api/quotes?page=4 这就很清晰了。 返回的是json，我们需要解析，然后提取数据，那我们如何知道最多有多少条json呢，文件已经告诉我们了： has_next:true 创建项目12345scrapy startproject quotecd quotescrapy genspider spiderquote http://spidyquotes.herokuapp.com/scroll 定义Item查看网站，采集text、author和tags这三个 123456789import scrapyclass QuoteItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() text = scrapy.Field() author = scrapy.Field() tag = scrapy.Field() 编写spider1234567891011121314151617181920212223# -*- coding: utf-8 -*-import scrapyimport jsonclass SpiderquoteSpider(scrapy.Spider): name = 'spiderquote' quotes_base_url = 'http://spidyquotes.herokuapp.com/api/quotes?page=%s' start_urls = [quotes_base_url % 1] download_delay = 1.5 def parse(self, response): data = json.loads(response.body) for item in data.get('quotes', []): yield &#123; 'text': item.get('text'), 'author': item.get('author', &#123;&#125;).get('name'), 'tags': item.get('tags'), &#125; if data['has_next']: next_page = data['page'] + 1 yield scrapy.Request(self.quotes_base_url % next_page) 运行爬虫，然后就可以看到结果了。 应用到虎嗅网那么如何应用到虎嗅网呢？首先还是要去分析网页。 虎嗅网的参数有3个： 123huxiu_hash_code:13a3a353c52d424e1e263dda4d594e59page:3last_dateline:1512026700 我们知道page就是翻页页码，huxiu_hash_code是一个不变的字符，last_dateline看起来像unix时间戳，验证确实如此。这个时间戳有必要带上吗，我想验证试试看。 在postman中测试，不带上last_dateline也是可以返回数据，并且这个json中已经告诉我们一共有多少页： &quot;total_page&quot;: 1654 在主函数中我们可以依葫芦画瓢 12345678910111213141516171819202122232425262728293031# -*- coding: utf-8 -*-import scrapyfrom huxiu.items import HuxiuItemimport jsonfrom lxml import etreeclass HuxiuSpider(scrapy.Spider): name = 'HuXiu' def start_requests(self): url = 'https://www.huxiu.com/v2_action/article_list' for i in range(1, 10): # FormRequest 是Scrapy发送POST请求的方法 yield scrapy.FormRequest( url = url, formdata = &#123;\"huxiu_hash_code\" : \"13a3a353c52d424e1e263dda4d594e59\", \"page\" : str(i)&#125;, callback = self.parse ) def parse(self, response): item = HuxiuItem() data = json.loads(response.text) s = etree.HTML(data['data']) item['title'] = s.xpath('//a[@class=\"transition msubstr-row2\"]/text()') item['link'] = s.xpath('//a[@class=\"transition msubstr-row2\"]/@href') item['author'] = s.xpath('//span[@class=\"author-name\"]/text()') item['introduction'] = s.xpath('//div[@class=\"mob-sub\"]/text()') yield item 输出的数据有点难看，是一段一段的。。 因为data[&#39;data&#39;]是一段html文件，所以这里选择的是xpath，不清楚这里是否直接使用Scrapy的xpath解析工具，如果可以，欢迎在评论中告诉我。 本篇收获 Scrapy采集动态网站：分析网页 使用Scrapy模拟post请求方法，文档在这 刘亦菲好漂亮 待做事宜 完善文件保存与解析 全站抓取大概用了3分钟，速度有点慢 若想评论，先翻长城","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"Scrapy","slug":"Scrapy","permalink":"https://zhangslob.github.io/tags/Scrapy/"},{"name":"虎嗅","slug":"虎嗅","permalink":"https://zhangslob.github.io/tags/虎嗅/"}]},{"title":"统计学学习笔记（一）","date":"2017-11-30T15:45:14.000Z","path":"2017/11/30/统计学学习笔记（一）/","text":"这是崔斯特的第十六篇原创文章 这是学习统计学的第一篇笔记，以后尽量都放在这里吧。 发现使用hexo发文章的快捷键： hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 下面是正文： 1、统计中的基本概念 总体和样本。总体：所研究的全部个体；样本：总体中的一部分 参数和统计量。参数：用来描述总体特征的概括性数字度量；统计量：用来描述样本特征的概括性数字度量。 变量。变量、分类变量（事物类别的一个名称）、顺序变量（事物有序类别的一个名称）、数值型变量、离散型变量（只能取可数值的变量）、连续性变量。 2、数据的收集1、数据的间接来源二手数据：公开出版的或公开报道的数据。 2、数据的直接来源（1）统计调查方式 抽样调查：经济性、时效性强、适应面广、准确性高。 普查：一次性或周期性、规定调查时间、数据比较准确、范围比较狭窄。 （2）数据的收集方法 询问调查：访问调查、邮寄调查、电话调查、计算机辅助调查、座谈会、个别深度访问。 观察与实验：观察法、实验法。 竟然没网络爬虫，嘤嘤嘤 3、调查设计（1）调查方案设计 调查目的 调查对象和调查单位 调查项目和调查表 （2）调查问卷设计a.调查问卷的基本结构： 开头部分（问候语、填表说明、问卷编号 ） 甄别部分：过滤——筛选掉不需要的部分——针对特定人群 主体部分：调查的全部问题 背景部分：被调查者的背景资料 b.提问项目的设计： 提问的内容尽可能短 用词要确切、通俗 一个项目只包含一项内容 避免诱导性提问 避免否定式提问 避免敏感性问题 c.回答项目的设计 开放性问题：灵活；整理资料困难 封闭性问题：两项选择法、多项选择法（单项选择型、多项选择型、限制选择型） 顺序选择法：按顺序排列 评定尺度法：和NPS有点像哦 双向列联法：表格表现 d.问题顺序的设计 问题的安排硬具有逻辑性 问题的顺序应先难后易 能引起被调查者兴趣的问题放在最前面 开放性问题放在后面 （3）统计数据的质量a.统计数据的误差b.统计数据的误差 精度 准确性 关联性 及时性 一致性 最低成本 这是统计学基础 第三版 (贾俊平)的记录，看了前两章，感觉受益匪浅，尤其是问卷的设计，比较系统、完整，可以应用在以后的工作中。 书名：统计学基础 第三版 作者：贾俊平 出版社：中国人民大学出版社 最后说一说为什么要学统计学最直接原因是工资高。可以去拉勾上看看“数据分析”、“数据挖掘”、“数据科学家”等职位，他们对学历的要求基本上都会有“统计学”。 对于我这种文科生来说，学习统计学是必经之路。敲门砖啊！ 最后记录下最近学习的数据科学的流程： 业务理解 分析方法 数据要求 收集数据 数据理解 数据准备 建模（use and test） 模型评估 部署与反馈 自己判断，缺少的是业务理解，对相关的业务知识了解太少；分析方法知道的太少了，接下来会着重学习一些常见的算法；数据准备也是一个大坑，不过好在自己有一些Python基础；建模才是最难的，慢慢来吧。 最近几天需要个自己定一个学习任务，内容主要包括：统计学基础、常见算法、pandas处理数据及可视化、业务理解、Scrapy框架学习、前端（没错，学点前端很有必要） 欢迎加我微信，一起来学习，嘤嘤嘤 下面是常见的分析方法","tags":[{"name":"统计学","slug":"统计学","permalink":"https://zhangslob.github.io/tags/统计学/"},{"name":"数据","slug":"数据","permalink":"https://zhangslob.github.io/tags/数据/"}]},{"title":"Scrapy学习实例（一）","date":"2017-11-29T13:52:32.000Z","path":"2017/11/29/Scrapy学习实例（一）/","text":"这是崔斯特的第十五篇原创文章 Hello，我又回来啦。以后就在这发文章吧，记录自己的学习历程。 举头卖竹鼠，低头嘤嘤嘤。 我会记录自己对Scrapy的学历经历，更重要的是理解。下面就开始吧，首先当然是创建一个项目啦！ 我选择爬取虎嗅网首页的新闻列表。 1、创建项目1234567891011121314151617F:\\Python\\huxiu&gt;scrapy startproject huxiuNew Scrapy project 'huxiu', using template directory 'c:\\\\users\\\\administrator\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\site-packages\\\\scrapy\\\\templates\\\\project', created in: F:\\Python\\huxiu\\huxiuYou can start your first spider with: cd huxiu scrapy genspider example example.comF:\\Python\\huxiu&gt;cd huxiuF:\\Python\\huxiu\\huxiu&gt;scrapy genspider huxiu huxiu.comCannot create a spider with the same name as your projectF:\\Python\\huxiu\\huxiu&gt;scrapy genspider HuXiu huxiu.comCreated spider 'HuXiu' using template 'basic' in module: huxiu.spiders.HuXiu 记住爬虫和项目命名不一样 2、定义Item在item.py中创建scrapy.Item类，并定义它的类型为scrapy.Field的属性。 123456789101112import scrapyclass HuxiuItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() title = scrapy.Field() #标题 link = scrapy.Field() #链接 author = scrapy.Field() #作者 introduction = scrapy.Field() #简介 time = scrapy.Field() #时间 3、编写Spider 一目了然 在huxiu/spider/HuXiu.py中编写代码 12345678910111213141516171819202122# -*- coding: utf-8 -*-import scrapyfrom huxiu.items import HuxiuItemclass HuxiuSpider(scrapy.Spider): name = 'HuXiu' allowed_domains = ['huxiu.com'] start_urls = ['http://huxiu.com/'] def parse(self, response): for s in response.xpath('//div[@class=\"mod-info-flow\"]/div/div[@class=\"mob-ctt\"]'): item = HuxiuItem() item['title'] = s.xpath('h2/a/text()')[0].extract() item['link'] = s.xpath('h2/a/@href')[0].extract() url = response.urljoin(item['link']) item['author'] = s.xpath('div/a/span/text()')[0].extract() item['introduction'] = s.xpath('div[2]/text()')[0].extract() item['time'] = s.xpath('div/span/text()')[0].extract() print(item) 在终端输入命令 scrapy crawl HuXiu 部分输出 4、深度爬取哈哈，这里借用造数的命名了。其实就是爬取新闻详情页。1234567891011121314151617181920212223242526272829303132333435# -*- coding: utf-8 -*-import scrapyfrom huxiu.items import HuxiuItemclass HuxiuSpider(scrapy.Spider): name = 'HuXiu' allowed_domains = ['huxiu.com'] start_urls = ['http://huxiu.com/'] def parse(self, response): for s in response.xpath('//div[@class=\"mod-info-flow\"]/div/div[@class=\"mob-ctt\"]'): item = HuxiuItem() item['title'] = s.xpath('h2/a/text()')[0].extract() item['link'] = s.xpath('h2/a/@href')[0].extract() url = response.urljoin(item['link']) item['author'] = s.xpath('div/a/span/text()')[0].extract() item['introduction'] = s.xpath('div[2]/text()')[0].extract() item['time'] = s.xpath('div/span/text()')[0].extract() #print(item) yield scrapy.Request(url, callback=self.parse_article) def parse_article(self, response): item = HuxiuItem() detail = response.xpath('//div[@class=\"article-wrap\"]') item['title'] = detail.xpath('h1/text()')[0].extract().strip() item['link'] = response.url item['author'] = detail.xpath('div[@class=\"article-author\"]/span/a/text()')[0].extract() item['time'] = detail.xpath('div[@class=\"article-author\"]/div[@class=\"column-link-box\"]/span/text()')[0].extract() print(item) word = detail.xpath('div[5]') print(word[0].xpath('string(.)').extract()[0]) yield item 输出结果 说明一点，如何使用xpath获得多个标签下的文本，这里参考了解决：xpath取出指定多标签内所有文字text，把文章详细内容打印出来，但是会遇到一些错误，可以使用goose来试试看。 Python-Goose - Article Extractor1234567891011&gt;&gt;&gt; from goose import Goose&gt;&gt;&gt; from goose.text import StopWordsChinese&gt;&gt;&gt; url = 'http://www.bbc.co.uk/zhongwen/simp/chinese_news/2012/12/121210_hongkong_politics.shtml'&gt;&gt;&gt; g = Goose(&#123;'stopwords_class': StopWordsChinese&#125;)&gt;&gt;&gt; article = g.extract(url=url)&gt;&gt;&gt; print article.cleaned_text[:150]香港行政长官梁振英在各方压力下就其大宅的违章建筑（僭建）问题到立法会接受质询，并向香港民众道歉。梁振英在星期二（12月10日）的答问大会开始之际在其演说中道歉，但强调他在违章建筑问题上没有隐瞒的意图和动机。一些亲北京阵营议员欢迎梁振英道歉，且认为应能获得香港民众接受，但这些议员也质问梁振英有 参考文章： Scrapy笔记02- 完整示例 解决：xpath取出指定多标签内所有文字text 若想评论，先翻长城","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"Scrapy","slug":"Scrapy","permalink":"https://zhangslob.github.io/tags/Scrapy/"},{"name":"虎嗅","slug":"虎嗅","permalink":"https://zhangslob.github.io/tags/虎嗅/"}]},{"title":"爬虫三步走（二）解析源码","date":"2017-05-26T12:15:20.000Z","path":"2017/05/26/爬虫三步走（二）解析源码/","text":"这是崔斯特的第十四篇原创文章 爬虫三步走：获取源码、解析源码、数据储存 上一期讲了如何获取网页源码的方法，这一期说一说怎么从其中获得我们需要的和数据。 解析网页的方法很多，最常见的就是BeautifulSoup和正则了，其他的像xpath、PyQuery等等，其中我觉得最好用的就是xpath了，xpath真的超级简单好用，学了之后再也不想取用美丽汤了。下面介绍xpath的使用方法。 首先需要安装lxml，windows下安装lxml是个大坑，知乎上有人给出了解决方法Python LXML模块死活安装不了怎么办？ 详细的用法可以参考爬虫入门到精通-网页的解析（xpath） 在这里我们尝试使用xpath来迅速获取数据。 例如想要获熊猫直播虎牙直播下主播的ID import requests from lxml import etree url = &apos;http://www.huya.com/g/lol&apos; headers = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&apos;} res = requests.get(url,headers=headers).text s = etree.HTML(res) print(s.xpath(&apos;//i[@class=&quot;nick&quot;]/text()&apos;)) 输出： 下面一步步讲解为什么这样做。 import requests from lxml import etree 首先是导入模块，requests很常见，但是xpath需要 from lxml import etree，你肯点想问为什么这样写，回答是“我也不知道”，就像是约定俗成的东西一样。 url = &apos;http://www.huya.com/g/lol&apos; headers = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&apos;} res = requests.get(url,headers=headers).text 这三步就是平常获取源码的过程，很简单。 s = etree.HTML(res) 给一个html，返回xml结构，为什么这样写？？答案和上面一样。最重要的就是下面的这一步： s.xpath(&apos;//i[@class=&quot;nick&quot;]/text()&apos;) 按下F12看到“爱拍-古手羽”在i标签下，接着我们右键打开“查看网页源代码”，搜索“爱拍-古手羽” 确实找到了“爱拍-古手羽”就在i标签下，那我们就把他提出来吧！ s.xpath(&#39;//i[@class=&quot;nick&quot;]/text()&#39;) 这个段代码意思是，找到class为“nick”的i标签，返回其中的文本信息，当然你也可以返回i标签中的title，写法如下： s.xpath(&#39;//i[@class=&quot;nick&quot;]/@title&#39;) text()返回的是文本信息，@title则是标签里面的具体属性的值，例如我想知道观众人数 import requests from lxml import etree url = &apos;http://www.huya.com/g/lol&apos; headers = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&apos;} res = requests.get(url,headers=headers).text s = etree.HTML(res) print(s.xpath(&apos;//i[@class=&quot;js-num&quot;]/text()&apos;)) 只需在原来基础上修改一个属性，i标签class为“js-num”里面的值 print(s.xpath(&apos;//i[@class=&quot;js-num&quot;]/text()&apos;)) 返回结果是： 说明：在运行代码中，发现虎牙反爬虫做得挺好的，瞬间就识别爬虫身份并封了IP，所以我换了IP去访问，至于如何设置代理，在我的上一篇文章中有说到，去看看吧。 在实际操作中，你可能会遇到更加复杂的情况，所以一定记得去看看详细的教程。爬虫入门到精通-网页的解析（xpath） 小广告：喜欢爬虫、数据的可以关注一下我的微信公众号（zhangslob），多多交流。","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"Python入门","slug":"Python入门","permalink":"https://zhangslob.github.io/tags/Python入门/"}]},{"title":"爬虫三步走（一）获取源码","date":"2017-05-25T07:21:06.000Z","path":"2017/05/25/爬虫三步走（一）获取源码/","text":"这是崔斯特的第十三篇原创文章 爬虫三步走：获取源码、解析源码、数据储存 举个例子，爬一爬知乎日报的相关数据 知乎日报 - 每天 3 次，每次 7 分钟 1、获取源码12345import requests url = 'http://daily.zhihu.com/'res = requests.get(url).textprint(res) 个人喜欢requests，直接访问，发现返回500错误 C:\\Python35\\python.exe F:/PyCharm/爬虫/daily.py &lt;html&gt;&lt;body&gt;&lt;h1&gt;500 Server Error&lt;/h1&gt; An internal server error occured. &lt;/body&gt;&lt;/html&gt; Process finished with exit code 0 根据经验判断，是知乎禁止爬虫，需要加上一些伪装，让我们看看加上浏览器伪装效果 import requests headers = {&apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&apos;} url = &apos;http://daily.zhihu.com/&apos; res = requests.get(url,headers=headers).text print(res) 看看结果，已经返回我们需要的数据 C:\\Python35\\python.exe F:/PyCharm/爬虫/daily.py &lt;!DOCTYPE html&gt;&lt;html lang=&quot;zh-CN&quot;&gt;&lt;head&gt;&lt;title&gt;知乎日报 - 每天 3 次，每次 7 分钟&lt;/title&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot;&gt;&lt;meta name=&quot;description&quot; content=&quot;在中国，资讯类移动应用的人均阅读时长是 5 分钟，而在知乎日报，这个数字是 21。以独有的方式为你提供最高质、最深度、最有收获的阅读体验。&quot;&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;/css/base.auto.css&quot;&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;/css/new_home_v3.auto.css&quot;&gt;&lt;script src=&quot;/js/jquery.1.9.1.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;/js/new_index_v3/home.js&quot;&gt;&lt;/script&gt;&lt;link rel=&quot;shortcut icon&quot; href=&quot;/favicon.ico&quot; type=&quot;image/x-icon&quot;&gt;&lt;base target=&quot;_blank&quot;&gt;&lt;style&gt;h1,h2,h3 {padding: 0;margin:0}&lt;/style&gt;&lt;base target=&quot;_blank&quot;&gt;&lt;/head&gt;&lt;body class=&quot;home&quot;&gt;&lt;a href=&quot;javascript:;&quot; title=&quot;回到顶部&quot; class=&quot;back-to-top&quot;&gt;&lt;/a&gt;&lt;div class=&quot;header navbar-fixed-top&quot;&gt;&lt;div class=&quot;container-fixed-width clearfix&quot;&gt;&lt;div class=&quot;top-nav-link&quot;&gt;&lt;a href=&quot;javascript:;&quot; data-offset=&quot;470&quot;&gt;&lt;span&gt;浏览内容&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;javascript:;&quot; data-offset=&quot;0&quot; class=&quot;active&quot;&gt;&lt;span&gt;App 下载&lt;/span&gt;&lt;/a&gt;&lt;/div&gt;&lt;h1 class=&quot;logo&quot;&gt;&lt;a href=&quot;http://daily.zhihu.com/&quot; title=&quot;知乎日报&quot; class=&quot;link-logo&quot;&gt;知乎日报&lt;/a&gt;&lt;/h1&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;download&quot;&gt; ... 但是这种写法是否可以应用到所有的网站，答案是“不” 2、代理设置有时候同一个IP去爬取同一网站上的内容，久了之后就会被该网站服务器屏蔽。解决方法就是更换IP。这个时候，在对方网站上，显示的不是我们真实地IP地址，而是代理服务器的IP地址。 国内高匿免费HTTP代理IP_国内高匿 西刺代理提供了很多可用的国内IP，可以直接拿来使用。 那么如何在爬虫里加入代理呢，看看requests的官方文档怎么说。高级用法 - Requests 2.10.0 文档 如果需要使用代理，你可以通过为任意请求方法提供 proxies 参数来配置单个请求: import requests proxies = { &quot;http&quot;: &quot;http://10.10.1.10:3128&quot;, &quot;https&quot;: &quot;http://10.10.1.10:1080&quot;, } requests.get(&quot;http://example.org&quot;, proxies=proxies) 用法很简单，加入proxies参数即可 import requests proxies = { &quot;http&quot;: &quot;http://121.201.24.248：8088&quot;, &quot;https&quot;: &quot;http://36.249.194.52：8118&quot;, } headers = {&apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&apos;} url = &apos;http://daily.zhihu.com/&apos; res = requests.get(url,headers=headers，proxies=proxies).text print(len(res)) 为了便于测试，只打印出返回数据的长度 C:\\Python35\\python.exe F:/PyCharm/爬虫/daily.py 10830 Process finished with exit code 0 发现代理服务器成功爬取知乎日报的信息，内容是10830，故意把代理IP写错一位数，看看结果 import requests proxies = { &quot;http&quot;: &quot;http://121.201.24.248：8088&quot;, &quot;https&quot;: &quot;http://36.249.194.52: 222&quot;, } headers = {&apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&apos;} url = &apos;http://daily.zhihu.com/&apos; res = requests.get(url,headers=headers,proxies=proxies).text print(len(res)) 我们把”https”: “http://36.249.194.52：8118&quot;修改为&quot;https“: “http://36.249.194.52: 222”，此时返回的结果如下，发现不能获取网页数据。所以，在使用代理服务器爬去网站时，如果出现异常，要考虑代理IP是否失效了。 当然你也可以写一个爬虫，实时抓取最新的代理IP用来爬取。 Traceback (most recent call last): File &quot;F:/PyCharm/爬虫/daily.py&quot;, line 9, in &lt;module&gt; res = requests.get(url,headers=headers,proxies=proxies).text File &quot;C:\\Python35\\lib\\site-packages\\requests\\api.py&quot;, line 70, in get return request(&apos;get&apos;, url, params=params, **kwargs) File &quot;C:\\Python35\\lib\\site-packages\\requests\\api.py&quot;, line 56, in request return session.request(method=method, url=url, **kwargs) File &quot;C:\\Python35\\lib\\site-packages\\requests\\sessions.py&quot;, line 488, in request resp = self.send(prep, **send_kwargs) File &quot;C:\\Python35\\lib\\site-packages\\requests\\sessions.py&quot;, line 609, in send r = adapter.send(request, **kwargs) File &quot;C:\\Python35\\lib\\site-packages\\requests\\adapters.py&quot;, line 485, in send raise ProxyError(e, request=request) requests.exceptions.ProxyError: HTTPConnectionPool(host=&apos;121.201.24.248：8088&apos;, port=80): Max retries exceeded with url: http://daily.zhihu.com/ (Caused by ProxyError(&apos;Cannot connect to proxy.&apos;, NewConnectionError(&apos;&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x0000000003860DA0&gt;: Failed to establish a new connection: [Errno 11004] getaddrinfo failed&apos;,))) 3、模拟登录有些网站是需要登录才能看到信息的，例如知乎，直接用requests获取知乎首页信息，返回数据是需要你登录的，只有登录了才能看到数据。 &lt;button type=&quot;button&quot; class=&quot;signin-switch-button&quot;&gt;手机验证码登录&lt;/button&gt; &lt;a class=&quot;unable-login&quot; href=&quot;#&quot;&gt;无法登录？&lt;/a&gt; &lt;/div&gt; &lt;div class=&quot;social-signup-wrapper&quot; data-za-module=&quot;SNSSignIn&quot;&gt; &lt;span class=&quot;name js-toggle-sns-buttons&quot;&gt;社交帐号登录&lt;/span&gt; &lt;div class=&quot;sns-buttons&quot;&gt; &lt;a title=&quot;微信登录&quot; class=&quot;js-bindwechat&quot; href=&quot;#&quot;&gt;&lt;i class=&quot;sprite-index-icon-wechat&quot;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a title=&quot;微博登录&quot; class=&quot;js-bindweibo&quot; href=&quot;#&quot;&gt;&lt;i class=&quot;sprite-index-icon-weibo&quot;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a title=&quot;QQ 登录&quot; class=&quot;js-bindqq&quot; href=&quot;#&quot;&gt;&lt;i class=&quot;sprite-index-icon-qq&quot;&gt;&lt;/i&gt;&lt;/a&gt; &lt;/div&gt; 再次回到官方文档快速上手 - Requests 2.10.0 文档 如果某个响应中包含一些 cookie，你可以快速访问它们： &gt;&gt;&gt; url = &apos;http://example.com/some/cookie/setting/url&apos; &gt;&gt;&gt; r = requests.get(url) &gt;&gt;&gt; r.cookies[&apos;example_cookie_name&apos;] &apos;example_cookie_value&apos; 要想发送你的cookies到服务器，可以使用 cookies 参数： &gt;&gt;&gt; url = &apos;http://httpbin.org/cookies&apos; &gt;&gt;&gt; cookies = dict(cookies_are=&apos;working&apos;) &gt;&gt;&gt; r = requests.get(url, cookies=cookies) &gt;&gt;&gt; r.text &apos;{&quot;cookies&quot;: {&quot;cookies_are&quot;: &quot;working&quot;}}&apos; 具体的分析过程可以参考xchaoinfo所写的文章和视频，讲解十分清晰Python 模拟登录哪些事儿 - 知乎专栏 下面是代码 import requests from bs4 import BeautifulSoup import os, time import re # import http.cookiejar as cookielib # 构造 Request headers agent = &apos;Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Mobile Safari/537.36&apos; headers = { &quot;Host&quot;: &quot;www.zhihu.com&quot;, &quot;Referer&quot;: &quot;https://www.zhihu.com/&quot;, &apos;User-Agent&apos;: agent } ######### 构造用于网络请求的session session = requests.Session() # session.cookies = cookielib.LWPCookieJar(filename=&apos;zhihucookie&apos;) # try: # session.cookies.load(ignore_discard=True) # except: # print(&apos;cookie 文件未能加载&apos;) ############ 获取xsrf_token homeurl = &apos;https://www.zhihu.com&apos; homeresponse = session.get(url=homeurl, headers=headers) homesoup = BeautifulSoup(homeresponse.text, &apos;html.parser&apos;) xsrfinput = homesoup.find(&apos;input&apos;, {&apos;name&apos;: &apos;_xsrf&apos;}) xsrf_token = xsrfinput[&apos;value&apos;] print(&quot;获取到的xsrf_token为： &quot;, xsrf_token) ########## 获取验证码文件 randomtime = str(int(time.time() * 1000)) captchaurl = &apos;https://www.zhihu.com/captcha.gif?r=&apos;+\\ randomtime+&quot;&amp;type=login&quot; captcharesponse = session.get(url=captchaurl, headers=headers) with open(&apos;checkcode.gif&apos;, &apos;wb&apos;) as f: f.write(captcharesponse.content) f.close() # os.startfile(&apos;checkcode.gif&apos;) captcha = input(&apos;请输入验证码：&apos;) print(captcha) ########### 开始登陆 headers[&apos;X-Xsrftoken&apos;] = xsrf_token headers[&apos;X-Requested-With&apos;] = &apos;XMLHttpRequest&apos; loginurl = &apos;https://www.zhihu.com/login/email&apos; postdata = { &apos;_xsrf&apos;: xsrf_token, &apos;email&apos;: &apos;邮箱@qq.com&apos;, &apos;password&apos;: &apos;密码&apos; } loginresponse = session.post(url=loginurl, headers=headers, data=postdata) print(&apos;服务器端返回响应码：&apos;, loginresponse.status_code) print(loginresponse.json()) # 验证码问题输入导致失败: 猜测这个问题是由于session中对于验证码的请求过期导致 if loginresponse.json()[&apos;r&apos;]==1: # 重新输入验证码，再次运行代码则正常。也就是说可以再第一次不输入验证码，或者输入一个错误的验证码，只有第二次才是有效的 randomtime = str(int(time.time() * 1000)) captchaurl = &apos;https://www.zhihu.com/captcha.gif?r=&apos; + \\ randomtime + &quot;&amp;type=login&quot; captcharesponse = session.get(url=captchaurl, headers=headers) with open(&apos;checkcode.gif&apos;, &apos;wb&apos;) as f: f.write(captcharesponse.content) f.close() os.startfile(&apos;checkcode.gif&apos;) captcha = input(&apos;请输入验证码：&apos;) print(captcha) postdata[&apos;captcha&apos;] = captcha loginresponse = session.post(url=loginurl, headers=headers, data=postdata) print(&apos;服务器端返回响应码：&apos;, loginresponse.status_code) print(loginresponse.json()) ##########################保存登陆后的cookie信息 # session.cookies.save() ############################判断是否登录成功 profileurl = &apos;https://www.zhihu.com/settings/profile&apos; profileresponse = session.get(url=profileurl, headers=headers) print(&apos;profile页面响应码：&apos;, profileresponse.status_code) profilesoup = BeautifulSoup(profileresponse.text, &apos;html.parser&apos;) div = profilesoup.find(&apos;div&apos;, {&apos;id&apos;: &apos;rename-section&apos;}) print(div) 好了关于爬虫的第一步，获取源码这一节讲了很多，其实大多数网站加上User-Agent和代理IP就可以正常爬取。下一节会讲讲如何利用xpath来解析网页，获取我们想要的数据。 喜欢爬虫、数据的可以关注一下我的微信公众号，多多交流。","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"Python入门","slug":"Python入门","permalink":"https://zhangslob.github.io/tags/Python入门/"}]},{"title":"知乎上妹子都爱取啥名？","date":"2017-05-20T09:45:48.000Z","path":"2017/05/20/知乎上妹子都爱取啥名？/","text":"这是崔斯特的第十二篇原创文章 闲来无事上知乎，看到好多妹子，于是抓取一波。 目标网址：轮子哥带我逛知乎 - 收藏夹 - 知乎 目标：抓取妹子的ID 1、抓取这次并没有使用Python做爬虫，使用工具来，速度更快。造数 - 新一代智能云爬虫 不到十分钟就完成了，遗失了部分数据，但是无所谓啦。保存下来，分析分析。 2、分析 使用pandas操作文件 import pandas as pd fp = pd.read_excel(&apos;D:\\Backup\\桌面\\lunzige.xlsx&apos;) fp name = fp[&apos;name&apos;].tolist() li1 = list(set(name)) li1 [&apos;阿蕾&apos;, &apos;杨面&apos;, &apos;陈10&apos;, &apos;杨顺顺&apos;, &apos;霧橤&apos;, &apos;真顺顺真&apos;, &apos;谢椿明&apos;, &apos;刀刀&apos;, &apos;水枪大帝&apos;, &apos;倾浅&apos;, &apos;Listening&apos;, &apos;小火龙&apos;, &apos;包子琛&apos;, &apos;杨笋笋&apos;, &apos;蜉蝣&apos;, &apos;十元&apos;, &apos;靡靡之音&apos;, &apos;Real机智张&apos;, &apos;陈梓小童鞋&apos;, &apos;花甲&apos;, &apos;窗里窗外&apos;, &apos;刘梓乔&apos;, &apos;璇璇97&apos;, &apos;Olivia菊香小姐姐&apos;, &apos;牛奶小夏目&apos;, &apos;周依宁&apos;, &apos;万阿咸&apos;, &apos;一蓑烟雨任平生&apos;, &apos;来都来了&apos;, &apos;就像周一&apos;, &apos;Mc蛋蛋&apos;, &apos;秉剑侯&apos;, &apos;李大梦Lee&apos;, &apos;Diss锐雯&apos;, &apos;雨音眞白&apos;, &apos;半仙幺幺&apos;, &apos;Natsuki是只蠢兔纸&apos;, &apos;夏冰莹&apos;, &apos;guuweihai&apos;, &apos;阿舞&apos;, &apos;肖柚妮&apos;, &apos;墨脱要开&apos;, &apos;芷珞&apos;, &apos;舒西婷&apos;, &apos;Childe0Q&apos;, &apos;被压扁的海螺&apos;, &apos;snow arc&apos;, &apos;灰灰灰灰灰plus&apos;, &apos;小兔子菲呀&apos;, &apos;士多啤梨羊咩咩&apos;, &apos;李小可可&apos;, &apos;谁来拽我的尾巴&apos;, &apos;飞鸽之舞&apos;, &apos;小美&apos;, &apos;樱雪绫sama&apos;, &apos;zshiyao&apos;, &apos;王漠里&apos;, &apos;Slivan&apos;, &apos;喵小虾&apos;, &apos;SUSAN苏&apos;, &apos;上官兰颜&apos;, &apos;这个杀手不太冷&apos;, &apos;看朱成碧纷思君&apos;, &apos;情绪&apos;, &apos;我系小忌廉&apos;, &apos;一只兔&apos;, &apos;June&apos;, &apos;我就想改名而已&apos;, &apos;温柔的大猫Leo&apos;, &apos;猫芙琳&apos;, &apos;以太&apos;, &apos;博丽魔理沙&apos;, &apos;洛丽塔&apos;, &apos;羽小团&apos;, &apos;娄良&apos;, &apos;Rosi&apos;, &apos;叶以北&apos;, &apos;吃不胖的小猫&apos;, &apos;Lina&apos;, &apos;ingrid&apos;, &apos;itttttx&apos;, &apos;胡杨&apos;, &apos;孙阿童&apos;, &apos;林美珍&apos;, &apos;赫蘿Taiga&apos;, &apos;宫曼曼&apos;, &apos;Yoonyicc&apos;, &apos;ZW711&apos;, &apos;笙箫&apos;, &apos;KIKI.Liu&apos;, &apos;另一只袜子&apos;, &apos;荒野大嫖客&apos;, &apos;少女诗&apos;, &apos;芸豆豆豆豆&apos;, &apos;璐璐噜&apos;, &apos;棹歌&apos;, &apos;梦里有只独角兽&apos;, &apos;Oo澄子oO&apos;, &apos;雷梅苔丝&apos;, &apos;CherryZhao&apos;, &apos;李萬一&apos;, &apos;琴脂&apos;, &apos;鹿斑比&apos;, &apos;Chris姬-云烟&apos;, &apos;hyoram&apos;, &apos;蔗蔗蔗&apos;, &apos;柚子Ruby&apos;, &apos;Sheena&apos;, &apos;孟德尔&apos;, &apos;kaka小师妹&apos;, &apos;桢视明&apos;, &apos;大豆苗&apos;, &apos;少女开膛手&apos;, &apos;陈诗茗&apos;] 那么，下一步就是对名字进行分词了，jieba分词，你值得拥有。fxsjy/jieba li2 = &apos;&apos;.join(li1) li2 &apos;阿蕾杨面陈10杨顺顺霧橤真顺顺真谢椿明刀刀水枪大帝倾浅Listening小火龙包子琛杨笋笋蜉蝣十元靡靡之音Real机智张陈梓小童鞋花甲窗里窗外刘梓乔璇璇97Olivia菊香小姐姐牛奶小夏目周依宁万阿咸一蓑烟雨任平生来都来了就像周一Mc蛋蛋秉剑侯李大梦LeeDiss锐雯雨音眞白半仙幺幺Natsuki是只蠢兔纸夏冰莹guuweihai阿舞肖柚妮墨脱要开芷珞舒西婷Childe0Q被压扁的海螺snow arc灰灰灰灰灰plus小兔子菲呀士多啤梨羊咩咩李小可可谁来拽我的尾巴飞鸽之舞小美樱雪绫samazshiyao王漠里Slivan喵小虾SUSAN苏上官兰颜这个杀手不太冷看朱成碧纷思君情绪我系小忌廉一只兔June我就想改名而已温柔的大猫Leo猫芙琳以太博丽魔理沙洛丽塔羽小团娄良Rosi叶以北吃不胖的小猫Linaingriditttttx胡杨孙阿童林美珍赫蘿Taiga宫曼曼YoonyiccZW711笙箫KIKI.Liu另一只袜子荒野大嫖客少女诗芸豆豆豆豆璐璐噜棹歌梦里有只独角兽Oo澄子oO雷梅苔丝CherryZhao李萬一琴脂鹿斑比Chris姬-云烟hyoram蔗蔗蔗柚子RubySheena孟德尔kaka小师妹桢视明大豆苗少女开膛手陈诗茗&apos; 有何感想？？ 下一步就是分词制作图云了 import jieba seg_list = jieba.cut(li2) word = &quot;/&quot;.join(seg_list) print(&quot;Full Mode: &quot; + &quot;/ &quot;.join(seg_list)) Building prefix dict from the default dictionary ... Dumping model to file cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache Loading model cost 1.148 seconds. Prefix dict has been built succesfully. Full Mode: 阿蕾/ 杨/ 面陈/ 10/ 杨/ 顺顺/ 霧/ 橤/ 真/ 顺顺/ 真/ 谢椿明/ 刀刀/ 水枪/ 大帝/ 倾浅/ Listening/ 小/ 火龙/ 包子/ 琛/ 杨笋/ 笋/ 蜉蝣/ 十元/ 靡靡之音/ Real/ 机智/ 张/ 陈梓/ 小/ 童鞋/ 花甲/ 窗里/ 窗外/ 刘梓乔/ 璇/ 璇/ 97Olivia/ 菊香/ 小姐姐/ 牛奶/ 小夏目/ 周依宁/ 万/ 阿/ 咸一/ 蓑/ 烟雨任/ 平生/ 来/ 都/ 来/ 了/ 就/ 像/ 周一/ Mc/ 蛋蛋/ 秉剑侯/ 李大梦/ LeeDiss/ 锐雯雨/ 音眞白/ 半仙/ 幺/ 幺/ Natsuki/ 是/ 只/ 蠢/ 兔纸/ 夏/ 冰莹/ guuweihai/ 阿舞/ 肖柚妮/ 墨脱/ 要/ 开芷/ 珞/ 舒西婷/ Childe0Q/ 被/ 压扁/ 的/ 海螺/ snow/ / arc/ 灰灰/ 灰灰/ 灰/ plus/ 小兔子/ 菲/ 呀/ 士多啤梨/ 羊/ 咩/ 咩/ 李小/ 可可/ 谁/ 来/ 拽/ 我/ 的/ 尾巴/ 飞鸽/ 之舞/ 小美/ 樱雪/ 绫/ samazshiyao/ 王漠/ 里/ Slivan/ 喵/ 小虾/ SUSAN/ 苏/ 上官/ 兰颜/ 这个/ 杀手/ 不/ 太冷/ 看朱成碧/ 纷思君/ 情绪/ 我系/ 小忌廉/ 一只/ 兔/ June/ 我/ 就/ 想/ 改名/ 而已/ 温柔/ 的/ 大猫/ Leo/ 猫/ 芙琳/ 以太/ 博丽/ 魔理沙/ 洛丽塔/ 羽小团/ 娄良/ Rosi/ 叶/ 以北/ 吃不胖/ 的/ 小猫/ Linaingriditttttx/ 胡杨/ 孙阿童/ 林美珍/ 赫蘿/ Taiga/ 宫曼曼/ YoonyiccZW711/ 笙箫/ KIKI/ ./ Liu/ 另一只/ 袜子/ 荒野/ 大/ 嫖客/ 少女/ 诗/ 芸豆/ 豆豆/ 豆璐璐噜/ 棹/ 歌梦里/ 有/ 只/ 独角兽/ Oo/ 澄子/ oO/ 雷梅/ 苔丝/ CherryZhao/ 李萬/ 一琴脂/ 鹿斑/ 比/ Chris/ 姬/ -/ 云烟/ hyoram/ 蔗蔗蔗/ 柚子/ RubySheena/ 孟德尔/ kaka/ 小/ 师妹/ 桢视/ 明大/ 豆苗/ 少女/ 开膛手/ 陈诗/ 茗 下一步绘制图云，用jupyter遇到了很多坑。。 # -*- coding: utf-8 -*- import matplotlib.pyplot as plt from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator # 直接从文件读取数据 text = &apos;&apos;&apos;阿蕾/杨/面陈/10/杨/顺顺/霧/橤/真/顺顺/真/谢椿明/刀刀/水枪/大帝/倾浅/Listening/小/火龙/包子/琛/杨笋/笋/蜉蝣/十元/靡靡之音/Real/机智/张/陈梓/小/童鞋/花甲/窗里/窗外/刘梓乔/璇/璇/97Olivia/菊香/小姐姐/牛奶/小夏目/周依宁/万/阿/咸一/蓑/烟雨任/平生/来/都/来/了/就/像/周一/Mc/蛋蛋/秉剑侯/李大梦/LeeDiss/锐雯雨/音眞白/半仙/幺/幺/Natsuki/是/只/蠢/兔纸/夏/冰莹/guuweihai/阿舞/肖柚妮/墨脱/要/开芷/珞/舒西婷/Childe0Q/被/压扁/的/海螺/snow/ /arc/灰灰/灰灰/灰/plus/小兔子/菲/呀/士多啤梨/羊/咩/咩/李小/可可/谁/来/拽/我/的/尾巴/飞鸽/之舞/小美/樱雪/绫/samazshiyao/王漠/里/Slivan/喵/小虾/SUSAN/苏/上官/兰颜/这个/杀手/不/太冷/看朱成碧/纷思君/情绪/我系/小忌廉/一只/兔/June/我/就/想/改名/而已/温柔/的/大猫/Leo/猫/芙琳/以太/博丽/魔理沙/洛丽塔/羽小团/娄良/Rosi/叶/以北/吃不胖/的/小猫/Linaingriditttttx/胡杨/孙阿童/林美珍/赫蘿/Taiga/宫曼曼/YoonyiccZW711/笙箫/KIKI/./Liu/另一只/袜子/荒野/大/嫖客/少女/诗/芸豆/豆豆/豆璐璐噜/棹/歌梦里/有/只/独角兽/Oo/澄子/oO/雷梅/苔丝/CherryZhao/李萬/一琴脂/鹿斑/比/Chris/姬/-/云烟/hyoram/蔗蔗蔗/柚子/RubySheena/孟德尔/kaka/小/师妹/桢视/明大/豆苗/少女/开膛手/陈诗/茗&apos;&apos;&apos; backgroud_Image = plt.imread(&apos;girl.jpg&apos;) wc = WordCloud( background_color = &apos;white&apos;, # 设置背景颜色 mask = backgroud_Image, # 设置背景图片 max_words = 2000, # 设置最大现实的字数 stopwords = STOPWORDS, # 设置停用词 font_path = &apos;C:/Users/Windows/fonts/msyh.ttf&apos;,# 设置字体格式，如不设置显示不了中文 max_font_size = 300, # 设置字体最大值 random_state = 50, # 设置有多少种随机生成状态，即有多少种配色方案 ) wc.generate(text) image_colors = ImageColorGenerator(backgroud_Image) #wc.recolor(color_func = image_colors) plt.imshow(wc) plt.axis(&apos;off&apos;) plt.show() 来看看图云吧 其实这个并没有什么卵用，知识自己无聊时玩玩的。下面才是我想要的 0 陈诗茗 https://www.zhihu.com/people/chen-shi-ming-69 1 李大梦Lee https://www.zhihu.com/people/li-da-meng-58-44 2 snow arc https://www.zhihu.com/people/xiaoxueli 3 夏冰莹 https://www.zhihu.com/people/xia-bingying 4 Sheena https://www.zhihu.com/people/zhang-chu-yun-84 5 喵小虾 https://www.zhihu.com/people/maoxiaoxia233 6 李大梦Lee https://www.zhihu.com/people/li-da-meng-58-44 7 李大梦Lee https://www.zhihu.com/people/li-da-meng-58-44 8 以太 https://www.zhihu.com/people/elapse08 9 zshiyao https://www.zhihu.com/people/duo-rou-wan-zi-89 10 SUSAN苏 https://www.zhihu.com/people/susansu-66 11 温柔的大猫Leo https://www.zhihu.com/people/li-yue-90-56 12 琴脂 https://www.zhihu.com/people/qin-zhi-49 13 王漠里 https://www.zhihu.com/people/wang-mo-li-66 14 花甲 https://www.zhihu.com/people/hua-jia-1-71 15 雷梅苔丝 https://www.zhihu.com/people/lei-mei-tai-si-15 16 Olivia菊香小姐姐 https://www.zhihu.com/people/olivia-60-10 17 芷珞 https://www.zhihu.com/people/zhi-luo-90-6 18 Mc蛋蛋 https://www.zhihu.com/people/lee2egg 19 少女诗 https://www.zhihu.com/people/shao-nu-shi-75 20 ingrid https://www.zhihu.com/people/da-da-yao-guai 21 博丽魔理沙 https://www.zhihu.com/people/nan-xiao-niao-94-5 22 赫蘿Taiga https://www.zhihu.com/people/he-luo-taiga 23 kaka小师妹 https://www.zhihu.com/people/kakasis 24 芸豆豆豆豆 https://www.zhihu.com/people/yun-dou-dou-dou-dou 25 林美珍 https://www.zhihu.com/people/lin-mei-zhen 26 喵小虾 https://www.zhihu.com/people/maoxiaoxia233 27 这个杀手不太冷 https://www.zhihu.com/people/wei-jun-jie-9 28 喵小虾 https://www.zhihu.com/people/maoxiaoxia233 29 Rosi https://www.zhihu.com/people/rosi-91 ... ... ... 111 洛丽塔 https://www.zhihu.com/people/hua-hua-gu-niang-5 112 洛丽塔 https://www.zhihu.com/people/hua-hua-gu-niang-5 113 洛丽塔 https://www.zhihu.com/people/hua-hua-gu-niang-5 114 洛丽塔 https://www.zhihu.com/people/hua-hua-gu-niang-5 115 Diss锐雯 https://www.zhihu.com/people/DSRiven 116 水枪大帝 https://www.zhihu.com/people/shuiqiangge 117 樱雪绫sama https://www.zhihu.com/people/lin-xuan-ting-1 118 李小可可 https://www.zhihu.com/people/li-gao-xing-2 119 士多啤梨羊咩咩 https://www.zhihu.com/people/shi-duo-pi-li-yan... 120 李萬一 https://www.zhihu.com/people/moire 121 万阿咸 https://www.zhihu.com/people/wan-a-xian-58 122 笙箫 https://www.zhihu.com/people/sheng-xiao-36 123 谢椿明 https://www.zhihu.com/people/xie-chun-ming-16 124 孙阿童 https://www.zhihu.com/people/sun-a-tong 125 宫曼曼 https://www.zhihu.com/people/gong-nuo-6 126 荒野大嫖客 https://www.zhihu.com/people/ji-da-fa-37 127 我就想改名而已 https://www.zhihu.com/people/wowjessica 128 就像周一 https://www.zhihu.com/people/yin-qing-chu-kai 129 胡杨 https://www.zhihu.com/people/hu-yang-49-22 130 杨笋笋 https://www.zhihu.com/people/yang-sun-sun-98 131 蜉蝣 https://www.zhihu.com/people/yuan-xia-66 132 羽小团 https://www.zhihu.com/people/xiao-yu-bao-er 133 杨笋笋 https://www.zhihu.com/people/yang-sun-sun-98 134 Lina https://www.zhihu.com/people/li-nuo-84-28 135 另一只袜子 https://www.zhihu.com/people/151231 136 刘梓乔 https://www.zhihu.com/people/liu-zi-qiao-42 137 guuweihai https://www.zhihu.com/people/guuweihai 138 陈10 https://www.zhihu.com/people/chen-10-80 139 ZW711 https://www.zhihu.com/people/zw711 140 看朱成碧纷思君 https://www.zhihu.com/people/kan-zhu-cheng-bi-... 下周二晚上8点，我会在趣直播聊一聊Python爬虫，如果你感兴趣，欢迎你来参加。 趣直播 - 知识直播平台","tags":[{"name":"知乎","slug":"知乎","permalink":"https://zhangslob.github.io/tags/知乎/"},{"name":"妹子","slug":"妹子","permalink":"https://zhangslob.github.io/tags/妹子/"}]},{"title":"Leetcode_4.Median of Two Sorted Arrays","date":"2017-05-19T15:07:32.000Z","path":"2017/05/19/Leetcode-4-Median-of-Two-Sorted-Arrays/","text":"这是崔斯特的第十一篇原创文章 参考Median of Two Sorted Arrays 1、题目There are two sorted arrays nums1 and nums2 of size m and n respectively. Find the median of the two sorted arrays. The overall run time complexity should be O(log (m+n)). Example 1: nums1 = [1, 3] nums2 = [2] The median is 2.0 Example 2: nums1 = [1, 2] nums2 = [3, 4] The median is (2 + 3)/2 = 2.5 2、思路蛋疼的说，我有看不懂题目了，尤其是O(log (m+n))，啥玩意。没办法，去网上搜索，看看前辈们的想法。 翻译如下： 给你两个排序数组，容量为m的数组A，容量为n的数组B。求出两个数组的中位数（啥玩意？），硬性要求时间复杂度O(log (m+n)). 1：太汗颜了，median到底是个啥，查一下： 中位数是在一组数据中居于中间的数(特别注意的地方是：这组数据之前已经经过升序排列！！！)，即在这组数据中，有一半的数据比它大，有一半的数据比它小。如果这组数据包含偶数个数字，中值是位于中间的两个数的平均值。 2：好吧，中位数是这么个玩意，那么理论上首先我们需要先将两个数组合为一，再求这个新合并的数组的中位数。 3：但是，已经限定死了时间复杂度为log（m+n），原来LeetCode的题目也思路不开放嘛。 4：问题可以转化成两个有序序列找第num大的数，由于时间复杂度已经限定死了，只能采用类似二分的思想，每个步骤去掉一半数据元素。 出现了一个词语：时间复杂度，这是个啥？ 完全不懂，换个思路来吧，不去看题目了，直接看第四点： 问题可以转化成两个有序序列找第num大的数，由于时间复杂度已经限定死了，只能采用类似二分的思想，每个步骤去掉一半数据元素。 二分，又是二分，赶紧去复习下。 二分查找就是将查找的键和子数组的中间键作比较，如果被查找的键小于中间键，就在左子数组继续查找；如果大于中间键，就在右子数组中查找，否则中间键就是要找的元素。 这个好像还可以看得懂，嘿嘿。我还发现了Python源代码（百度这样说的）： def bin_search(data_list, val): low = 0 # 最小数下标 high = len(data_list) - 1 # 最大数下标 while low &lt;= high: mid = (low + high) // 2 # 中间数下标 if data_list[mid] == val: # 如果中间数下标等于val, 返回 return mid elif data_list[mid] &gt; val: # 如果val在中间数左边, 移动high下标 high = mid - 1 else: # 如果val在中间数右边, 移动low下标 low = mid + 1 return # val不存在, 返回None ret = bin_search(list(range(1, 10)), 3) print(ret) 大概明白他的意思了。 3、解法很多解法都提到：如果我们可以在两个数列中求出第K小的元素，便可以解决该问题 解题思路：这道题要求两个已经排好序的数列的中位数。中位数的定义：如果数列有偶数个数，那么中位数为中间两个数的平均值；如果数列有奇数个数，那么中位数为中间的那个数。比如{1，2，3，4，5}的中位数为3。{1，2，3，4，5，6}的中位数为（3+4）/ 2 = 3.5。那么这题最直接的思路就是将两个数列合并在一起，然后排序，然后找到中位数就行了。可是这样最快也要O((m+n)log(m+n))的时间复杂度，而题目要求O(log(m+n))的时间复杂度。这道题其实考察的是二分查找，是《算法导论》的一道课后习题，难度还是比较大的。 首先我们来看如何找到两个数列的第k小个数，即程序中getKth(A, B , k)函数的实现。用一个例子来说明这个问题：A = {1，3，5，7}；B = {2，4，6，8，9，10}；如果要求第7个小的数，A数列的元素个数为4，B数列的元素个数为6；k/2 = 7/2 = 3，而A中的第3个数A[2]=5；B中的第3个数B[2]=6；而A[2]&lt;B[2]；则A[0]，A[1]，A[2]中必然不可能有第7个小的数。因为A[2]&lt;B[2]，所以比A[2]小的数最多可能为A[0], A[1], B[0], B[1]这四个数，也就是说A[2]最多可能是第5个大的数，由于我们要求的是getKth(A, B, 7)；现在就变成了求getKth(A’, B, 4)；即A’ = {7}；B不变，求这两个数列的第4个小的数，因为A[0]，A[1]，A[2]中没有解，所以我们直接删掉它们就可以了。这个可以使用递归来实现。 class Solution: # @return a float # @line20 must multiply 0.5 for return a float else it will return an int def getKth(self, A, B, k): lenA = len(A); lenB = len(B) if lenA &gt; lenB: return self.getKth(B, A, k) if lenA == 0: return B[k - 1] if k == 1: return min(A[0], B[0]) pa = min(k/2, lenA); pb = k - pa if A[pa - 1] &lt;= B[pb - 1]: return self.getKth(A[pa:], B, pb) else: return self.getKth(A, B[pb:], pa) def findMedianSortedArrays(self, A, B): lenA = len(A); lenB = len(B) if (lenA + lenB) % 2 == 1: return self.getKth(A, B, (lenA + lenB)/2 + 1) else: return (self.getKth(A, B, (lenA + lenB)/2) + self.getKth(A, B, (lenA + lenB)/2 + 1)) * 0.5 在我提交了代码之后，发现超过50.24 % 我找出最快的解法，来学习下： class Solution(object): def findMedianSortedArrays(self, a, b): &quot;&quot;&quot; :type nums1: List[int] :type nums2: List[int] :rtype: float &quot;&quot;&quot; c = a+b c.sort() m = len(c) / 2 mm = len(c) % 2 if mm &gt; 0 : return c[m] return (c[m-1]+c[m])/2.# + (c[m-1]+c[m])%2 第二名的： class Solution(object): def findMedianSortedArrays(self, nums1, nums2): &quot;&quot;&quot; :type nums1: List[int] :type nums2: List[int] :rtype: float &quot;&quot;&quot; nums3 = nums1 + nums2 nums3.sort() l = len(nums3) if l%2 == 1: return nums3[l/2] else: return (float(nums3[l/2]) + float(nums3[l/2-1]))/2 每做一题，都会被打击好多次，但是算法是一定要学习的。","tags":[]},{"title":"Leetcode_2. Add Two Numbers","date":"2017-05-14T08:42:56.000Z","path":"2017/05/14/Leetcode-2-Add-Two-Numbers/","text":"这是崔斯特的第十篇原创文章 好久没写博客了，定个小目标，2天一篇，哈哈。 1、题目You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list. You may assume the two numbers do not contain any leading zero, except the number 0 itself. Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4) Output: 7 -&gt; 0 -&gt; 8 2、思路其实我并看不懂题目说得啥意思，如果说243+564，结果是807才是，但是很明显题目不会这么简单。怎么办？ 好像只能去看看别人的理解了，发现： 1.因为存储是反过来的，即数字342存成2-&gt;4-&gt;3，所以要注意进位是向后的； 2.链表l1或l2为空时，直接返回，这是边界条件，省掉多余的操作； 3.链表l1和l2长度可能不同，因此要注意处理某个链表剩余的高位； 4.2个数相加，可能会产生最高位的进位，因此要注意在完成以上1－3的操作后，判断进位是否为0，不为0则需要增加结点存储最高位的进位。 给个链接http://blog.csdn.net/zhouworld16/article/details/14045855 原来是倒着相加，342+465=807，结果倒序，正好是708，题目应该是这样理解的吧，哈哈。 思路本题的思路很简单，按照小学数学中学习的加法原理从末尾到首位，对每一位对齐相加即可。技巧在于如何处理不同长度的数字，以及进位和最高位的判断。这里对于不同长度的数字，我们通过将较短的数字补0来保证每一位都能相加。递归写法的思路比较直接，即判断该轮递归中两个ListNode是否为null。 全部为null时，返回进位值 有一个为null时，返回不为null的那个ListNode和进位相加的值 都不为null时，返回 两个ListNode和进位相加的值 来源一位大佬https://segmentfault.com/a/1190000002986101 原来是小学内容小学数学中学习的加法，使用递归写法。 递归，就是在运行的过程中调用自己。什么鬼啊，自己使用自己吗？ 目前我找到的对递归最恰当的比喻，就是查词典。 我们使用的词典，本身就是递归，为了解释一个词，需要使用更多的词。 当你查一个词，发现这个词的解释中某个词仍然不懂，于是你开始查这第二个词，可惜，第二个词里仍然有不懂的词，于是查第三个词，这样查下去，直到有一个词的解释是你完全能看懂的，那么递归走到了尽头，然后你开始后退，逐个明白之前查过的每一个词，最终，你明白了最开始那个词的意思。。。 好像明白了一点点~ 3、解法 #（Python）版本1 class Solution: def addTwoNumbers(self, l1, l2): addends = l1, l2 dummy = end = ListNode(0) carry = 0 while addends or carry: carry += sum(a.val for a in addends) addends = [a.next for a in addends if a.next] end.next = end = ListNode(carry % 10) carry /= 10 return dummy.next 版本2 class Solution: # @return a ListNode def addTwoNumbers(self, l1, l2): carry = 0 sum = ListNode(0) s = sum while l1 is not None or l2 is not None or carry: s.val = carry if l1: s.val += l1.val l1 = l1.next if l2: s.val += l2.val l2 = l2.next carry = s.val / 10 s.val = s.val % 10 if l1 or l2 or carry: s.next = ListNode(0) s = s.next return sum 好吧，承认版本1看不懂。至于版本2 ListNode 是一个元祖，然后。。。s.val是什么意思啊？又不懂。回去看看，原来是已经定义了ListNode # Definition for singly-linked list. # class ListNode(object): # def __init__(self, x): # self.val = x # self.next = None 也就是说有2个类，Solution 和 ListNode 今天先这样了，我需要去学习一下什么是“类”，也就是Python的面向对象编程","tags":[{"name":"leetcode","slug":"leetcode","permalink":"https://zhangslob.github.io/tags/leetcode/"},{"name":"刷题","slug":"刷题","permalink":"https://zhangslob.github.io/tags/刷题/"}]},{"title":"使用Python计算文章中的字词频率丨学习笔记和反思","date":"2017-03-28T10:58:18.000Z","path":"2017/03/28/使用Python计算文章中的字词频率丨学习笔记和反思/","text":"这是崔斯特的第九篇原创文章 来源：天善智能-商业智能和大数据在线社区，用心创造价值https://edu.hellobi.com/course/159/play/lesson/2531 丘祐玮https://ask.hellobi.com/people/DavidChiu人人都爱数据科学家！Python数据科学精华实战课程 环境：Anaconda3 建议使用Anaconda，下载源文件后再阅读本文：https://github.com/zhangslob/DanmuFenxi 选择经典演讲稿，奥巴马2009年9月8日开学演讲。。https://wenku.baidu.com/view/ad77bc1caf45b307e8719758.html THE PRESIDENT: Hello, everybody! Thank you. Thank you. Thank you, everybody. All right, everybody go ahead and have a seat. How is everybody doing today? (Applause.) How about Tim Spicer? (Applause.) I am here with students at Wakefield High School in Arlington, Virginia. And we’ve got students tuning in from all across America, from kindergarten through 12th grade. And I am just so glad that all could join us today. And I want to thank Wakefield for being such an outstanding host. Give yourselves a big round of applause. (Applause.)… 1、World Count(Version 1)把数据命名为speech_text，首先需要对英文进行分词。英文中主要是空格，使用split()函数 # coding: utf-8 # In[1]: speech_text=&apos;&apos;&apos;#长文本使用&apos;&apos;&apos;..&apos;&apos;&apos; THE PRESIDENT: Hello, everybody! Thank you. Thank you. Thank you, everybody. All right, everybody go ahead and have a seat. How is everybody doing today? (Applause.) How about Tim Spicer? (Applause.) I am here with students at Wakefield High School in Arlington, Virginia. And we&apos;ve got students tuning in from all across America, from kindergarten through 12th grade. And I am just so glad that all could join us today. And I want to thank Wakefield for being such an outstanding host. Give yourselves a big round of applause. (Applause.) ...#省略文字 &apos;&apos;&apos; # In[2]: speech=speech_text.split() # In[3]: speech 下一步，计算speech中词语出现的次数 # In[4]: dic={} for word in speech: if word not in dic: dic[word] = 1 else: dic[word] = dic[word] + 1 # In[5]: dic 通过 items() 函数以列表返回可遍历的(键, 值) 元组数组。 下一步，对词语进行排序 # In[7]: import operator swd=sorted(dic.items(),key=operator.itemgetter(1),reverse=True)#从大到小排序 # In[9]: swd 发现其中“to”、“the”等单词是常见词，借用nltk我们可以把这些词语去掉 from nltk.corpus import stopwords stop_words = stopwords.words(&apos;English&apos;) 虽说Anaconda已经安装了NLTK，但是我自己操作时stopwords貌似没有，出错请参考https://www.douban.com/note/534906136/ 看看英文中的去停词，下一步，遍历，打印出不含有去停词 for k,v in swd2: if k not in stop_words: print(k,v) 发现出现了很多“–”，回去原文中观察，发现确实有很多， 那么问题来了，为什么出现这么多“–”。萌新求解！ 2、World Count(Version 2)from collections import Counter c=Counter(speech2) 使用Python 的collections模块更简洁，详细见http://www.jb51.net/article/48771.htm 同样可以使用stop_word，还可以使用most_common打印出前几个 for sw in stop_words: del c[sw] 3、反思上一篇文章https://zhuanlan.zhihu.com/p/25983014写的比较粗糙，很多人要求把“观众” “礼物”筛选出来，那我来试试。 stop = [&apos;！&apos;,&apos;*&apos;,&apos;观众&apos;,&apos;礼物&apos;,&apos;:&apos;,&apos;？&apos;,&apos;。&apos;,&apos;，&apos;,&apos;~&apos;,&apos;1&apos;] 去停词只有这些、可以根据实际情况添删。 看来观众很喜欢说“xx学院发来贺电~~”","tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://zhangslob.github.io/tags/学习笔记/"},{"name":"文字处理","slug":"文字处理","permalink":"https://zhangslob.github.io/tags/文字处理/"}]},{"title":"昨天看球时，球迷都说了啥——弹幕抓取与分析","date":"2017-03-24T12:41:58.000Z","path":"2017/03/24/昨天看球时，球迷都说了啥——弹幕抓取与分析/","text":"这是崔斯特的第八篇原创文章 数据来源：http://star.longzhu.com/teamchina 本次弹幕记录（开始时间: 2017-03-23-19:43:34，结束21:29:33)，共记录20788条数据。 使用OBS弹幕助手记录http://www.obsapp.com/apps/obsdanmu/ 1、分析 文件中含有时间记录，观众ID和送礼记录，其次是弹幕内容，所以决定对前两列内容不分析。 首先需要对文本分词，这里采用jieba分词 https://github.com/fxsjy/jieba/ 去除空格，使用strip()函数， 去掉换行符”\\n” line = line.strip(&apos;\\n&apos;) 把分析结果写入新的文档’text.txt’，Python join() 方法用于将序列中的元素以指定的字符连接生成一个新的字符串 text = &apos;&apos; with open(&apos;danmu.txt&apos;,encoding=&apos;utf-8&apos;) as fin: for line in fin.readlines(): line = line.strip(&apos;\\n&apos;) text += &apos;/&apos;.join(jieba.cut(line)) text += &apos; &apos; fout = open(&apos;text.txt&apos;,&apos;wb&apos;)#以二进制写模式写入 pickle.dump(text,fout) fout.close() 这样就完成了分词过程，结果如下： 2、绘制图云# 直接从文件读取数据 fr = open(&apos;text.txt&apos;,&apos;rb&apos;) text = pickle.load(fr) 使用word_cloud，具体用法https://github.com/amueller/word_cloud backgroud_Image = plt.imread(&apos;girl.jpg&apos;) wc = WordCloud( background_color = &apos;white&apos;, # 设置背景颜色 mask = backgroud_Image, # 设置背景图片 max_words = 2000, # 设置最大现实的字数 stopwords = STOPWORDS, # 设置停用词 font_path = &apos;C:/Users/Windows/fonts/msyh.ttf&apos;,# 设置字体格式，如不设置显示不了中文 max_font_size = 300, # 设置字体最大值 random_state = 50, # 设置有多少种随机生成状态，即有多少种配色方案 ) 使用matplotlib绘图http://matplotlib.org/2.0.0/index.html wc.generate(text) image_colors = ImageColorGenerator(backgroud_Image) #wc.recolor(color_func = image_colors) plt.imshow(wc) plt.axis(&apos;off&apos;) plt.show() OK，这样就完成了，附上结果 有没有你发过的弹幕呢？ 可自形修改数据，得到更好看图片。 能力有限，分析很少，如果你想进行更深入分析，请找我要文件。 对英雄联盟感兴趣的小伙伴可以看看这篇，对游戏直播弹幕的分析。 https://zhangslob.github.io/2017/03/24/%E5%88%A9%E7%94%A8Python%E5%AF%B9%E7%9B%B4%E6%92%AD%E5%BC%B9%E5%B9%95%E7%9A%84%E5%88%86%E6%9E%90/ github：https://github.com/zhangslob/DanmuFenxi","tags":[{"name":"足球","slug":"足球","permalink":"https://zhangslob.github.io/tags/足球/"},{"name":"数据分析","slug":"数据分析","permalink":"https://zhangslob.github.io/tags/数据分析/"}]},{"title":"利用Python对直播弹幕的分析","date":"2017-03-24T12:12:58.000Z","path":"2017/03/24/利用Python对直播弹幕的分析/","text":"这是崔斯特的第七篇原创文章 弹幕（ barrage），中文流行词语，原意指用大量或少量火炮提供密集炮击。而弹幕，顾名思义是指子弹多而形成的幕布， 大量吐槽评论从屏幕飘过时效果看上去像是飞行射击游戏里的弹幕。 今天就来说说游戏直播中， 弹幕都有哪些。 一、准备利用 danmu 弹幕接口对斗鱼主播赏金术士直播间的弹幕进行抓取，抓取时间约2 小时，共计 2534 条弹幕。赏金直播间： https://www.douyu.com/846805 二、 分析1、 弹幕词云词云， 由词汇组成类似云的彩色图形。 使用的是 Python 的模块 wordcloud。 通过 jieba 分词对弹幕中文分词， 使用 wordcloud 对结果构造词云， 最终结果为： 可以看到，高频率词语有： 外甥、无敌、厉害、可以、无限、火力、什么、垃圾、赏金。 2、关键词TextRank算法可以用来从文本中提取关键词和摘要 关键词： 不 0.010922664205428556 外甥 0.010484629632807344 玩 0.008177160003721682 无限 0.0058741805660283575 没 0.005665342357801469 说 0.005548941560115147 大 0.005541099430280024 主播 0.005498954448927515 出 0.0054948076800822405 看 0.0051528084835430555 可以看出来，作为汉字常用字，‘不’、‘玩’、‘没’、‘说’、‘大’、‘出’、‘看’这7个字出现频率高，这不奇怪。但是，‘外甥’、‘无限’、‘主播’就和英雄联盟主播赏金术士有责很大关系了。 外甥是赏金双排的一位选手、扮演着搞笑、逗乐的角色。‘无限’则是“无限火力”，一个特定模式，颇受玩家喜爱。主播可能是赏金，也有可能说别的主播。 关键短语： 对面不会 垃圾主播 不出 没带 不大 赏金玩 对面德玛 外甥说 大不 外甥大 摘要： 7337 0.0006968086282134394 真的有护眼模式666 3059 0.0006968086282134393 护眼模式为什么这么绿 10503 0.00047342729603724247 找儿子，爱好护眼 从摘要中看到三条弹幕中，都含有“护眼”二字，这是为什么呢？ lol护眼一词，其实主要来源于英雄联盟的直播平台，随着叫的人多了，这个词便火热了起来。起初是有人带节奏，说打护眼斗鱼可以进入护眼模式。一般在直播lol的主播使用的英雄死掉后，界面会呈现暗灰色的，亮度降低有利于防护眼睛，从而就有了lol护眼，当然意思就是嘲讽主播很菜的意思。 三、总结：由于自己所收集的数据过少、而且仅保存了一位主播的弹幕，造成结果不具有通用性，可以通过对各大直播平台的热门主播弹幕的爬取，获得观众的心理变化和网络风气。 以及主播有没有过气一说~ github：https://github.com/zhangslob/DanmuFenxi","tags":[{"name":"直播","slug":"直播","permalink":"https://zhangslob.github.io/tags/直播/"},{"name":"文本分析","slug":"文本分析","permalink":"https://zhangslob.github.io/tags/文本分析/"}]},{"title":"如何优雅的“轮带逛”初级篇——获取单张图片","date":"2017-03-20T12:35:00.000Z","path":"2017/03/20/如何优雅的“轮带逛”初级篇——获取单张图片/","text":"这是崔斯特的第六篇原创文章 轮子哥护体 首先上收藏夹 https://www.zhihu.com/collection/78172986?page=1 由@vega13创建，内容挺多的。例如， 等等，看的老夫脸都红了 写了一个简单爬取图片的程序。记录下过程。手动 @轮子哥 1、分析网页收藏夹只收藏了问题的一个答案，初步想法是获取当前页面的图片 因为上一次原因，直接去网页源代码 &lt;img src=&quot;https://pic4.zhimg.com/de5ecb16bcb912e99a83f647eb96c5bb_200x112.jpg&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1080&quot; class=&quot;origin_image inline-img zh-lightbox-thumb&quot; data-original=&quot;https://pic4.zhimg.com/de5ecb16bcb912e99a83f647eb96c5bb_r.jpg&quot;&gt; &lt;img data-rawwidth=&quot;1280&quot; data-rawheight=&quot;1836&quot; src=&quot;https://pic2.zhimg.com/v2-61ba67d910104f99acdb805a3568ab05_200x112.jpg&quot; class=&quot;origin_image inline-img zh-lightbox-thumb&quot; data-original=&quot;https://pic2.zhimg.com/v2-61ba67d910104f99acdb805a3568ab05_r.jpg&quot;&gt; 在&lt;img&gt;标签下，src和data-original都含有图片链接，经验证data-original是大图，那就把每个问题的图片链接找到了，接下来就很简单了。 2、代码就18行的代码。简单吧~ import requests,urllib from lxml import etree def get_img(url): headers = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36&apos;} r= requests.get(url,headers=headers).text s = etree.HTML(r) #print(r) link = s.xpath(&apos;//img/@data-original&apos;) for i in link: print(i) name = i.split(&apos;/&apos;)[-1]#图片名称 urllib.request.urlretrieve(i,name) if __name__ == &apos;__main__&apos;: for i in range(1,43): url = &apos;https://www.zhihu.com/collection/78172986?page=&apos; + str(i) get_img(url) 但是这样存在一个问题 link = s.xpath(&apos;//img/@data-original&apos;) 这里的link只有每个回答的第一张图片，更多的图片藏在文本中，除了正则不知道还有没有更好的办法 只有200多张图片。结尾有百度云 下载了几分钟出现错误，有一张图片下载不了，知友们有什么好办法吗？ 三、“轮带逛”高级篇既然有初级篇，肯定还有高级篇。 其实这个收藏夹中每一个问题下面都含有很多回答，收藏夹只是选取其中一个，也就是被轮子哥点赞的那个，那么还有那些没有被点赞的呢？ 下一期讲一讲怎么获取所有图片链接。 放一张图片，卡死我程序的 —————————————最后的小广告—————————————– 有朋友竟然叫我去作一期直播，讲一讲Python。 打算根据自己的经历分享一些经验，主要是关于Python入门的，想听听可以私信我。 时间是周二晚9点~ 百度云下载 链接：http://pan.baidu.com/s/1dFOPbUx 密码：abrl","tags":[{"name":"知乎爬虫","slug":"知乎爬虫","permalink":"https://zhangslob.github.io/tags/知乎爬虫/"},{"name":"轮带逛","slug":"轮带逛","permalink":"https://zhangslob.github.io/tags/轮带逛/"}]},{"title":"Python爬虫实战——免费图片 - Pixabay","date":"2017-03-19T09:24:58.000Z","path":"2017/03/19/Python爬虫实战——免费图片-Pixabay/","text":"这是崔斯特的第五篇原创文章 Pixabay，一个挺不错的高清无码图片网站，可以免费下载。 https://pixabay.com/ 一些介绍 超过 900000 高质量照片、 插图和矢量图形。可免费用于商业用途。没有所需的归属。 Pixabay是一家高质量图片分享网站。最初，该网站由Hans Braxmeier和Simon Steinberger在德国发展起来。2013年2月，网站拥有由影师和其社区的插画家提供的大约7万张免费的照片和矢量图形。该公司于2010年12月在德国乌尔姆成立。 2012年3月，Pixabay开始从一个私人图像搜集网站转变成一个互动的网上社区，该网站支持20种语言。同年5月，网站推出公共应用程序编程接口，从而使第三方用户和网站开发人员搜索其图像数据库。网站还与Flickr，YouTube和维基共享资源。 Pixabay用户无需注册就可以获得免费版权的高质量图像。根据知识共享契约CC0相关的肖像权，用户在该网站通过上传图片就默认放弃图片版权，从而使图片广泛流通。网站允许任何人使用，修改图片 - 即便是在商业应用 - 不要求许可并且未认可。 Pixabay为了确保高品质图片标准，用户上传的所有图片将由网站工作人员手动审批。大约27％的用户会说英语，20％的用户会说西班牙语，11％的用户会说葡萄牙语，7％的用户会说德语和5％的用户会说法语。其用户主要是博客、图形设计师、作家、记者和广告商。 今天的目标就是爬取小编精选的图片 https://pixabay.com/zh/editors_choice/?media_type=photo&amp;pagi=1 一、分析我们需要写3个函数 一个Download(url)，用来下载图片 一个用来获取小编精选一共有的165页FullUrl() 最后用来调用main() 下面开始一个个写吧~ https://pixabay.com/zh/editors_choice/?media_type=photo&amp;pagi=1 打开网页，F12，查看图片链接所在的标签 可以看到图片链接都在&lt;img&gt;标签下，但是我自己发现前几张和后几张的属性是不一样的，提取出&lt;img&gt;中“src”就可以了，使用的是xpath import requests from lxml import etree header = {&apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36&apos;} url = &apos;https://pixabay.com/zh/editors_choice/&apos; r = requests.get(url,headers=header).text s = etree.HTML(r) print(s.xpath(&apos;//img/@src&apos;)) 结果是 前面都是正确的图片链接，可是后面出现了’/static/img/blank.gif’，这个是什么鬼，查看网页源代码，搜索 可以发现确实有这一段字符串，我自己在这一点上花了很多时间。感谢@李宏杰的帮助，https://www.zhihu.com/question/57188290 浏览器中的代码是JavaScript修改过的, 你直接用requests请求然后打印出来看就会发现 &lt;div class=&quot;item&quot; data-w=&quot;640&quot; data-h=&quot;426&quot;&gt; &lt;a href=&quot;/zh/%E8%9B%8B%E7%B3%95-%E4%B8%80%E5%9D%97%E8%9B%8B%E7%B3%95-%E9%A3%9F%E8%B0%B1-%E4%B8%80%E7%89%87-%E7%B3%96%E6%9E%9C-%E6%8F%92%E5%9B%BE-%E7%83%98%E7%83%A4-%E7%94%9C%E7%82%B9-%E9%A3%9F%E5%93%81-1971556/&quot;&gt; &lt;img src=&quot;/static/img/blank.gif&quot; data-lazy-srcset=&quot;https://cdn.pixabay.com/photo/2017/01/11/11/33/cake-1971556__340.jpg 1x, https://cdn.pixabay.com/photo/2017/01/11/11/33/cake-1971556__480.jpg 2x&quot; data-lazy=&quot;https://cdn.pixabay.com/photo/2017/01/11/11/33/cake-1971556__340.jpg&quot; alt=&quot;&quot;&gt; &lt;/a&gt; &lt;div&gt; requests返回的数据中可以看到，“data-lazy”总含有我们需要的数据，修改代码 发现现在返回的数据是我们需要的，打开一张图片查看 下面的图片要清晰很多，我们只需要把__340换成_960_720即可 小编精选一共有165页，我们需要获取下一页URL https://pixabay.com/zh/editors_choice/?media_type=photo&amp;pagi=2 https://pixabay.com/zh/editors_choice/?media_type=photo&amp;pagi=3 。。。 规律很简单 full_link = [] for i in range(1,165): #print(i) full_link.append( &apos;https://pixabay.com/zh/editors_choice/?media_type=photo&amp;pagi=&apos;+ str(i)) 到现在，准备工作做好了，思路可能不是很清楚，请谅解~ 二、代码import requests from lxml import etree import time import urllib def Download(url): header = { &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36&apos;} r = s.get(url, headers=header).text s = etree.HTML(r) r = s.xpath(&apos;//img/@data-lazy&apos;) for i in r: imglist = i.replace(&apos;__340&apos;, &apos;_960_720&apos;) name = imglist.split(&apos;/&apos;)[-1]#图片名称 urllib.request.urlretrieve(imglist,name) time.sleep(1) def FullUrl(): full_link = [] for i in range(2,165): #print(i) full_link.append( &apos;https://pixabay.com/zh/editors_choice/?media_type=photo&amp;pagi=&apos;+ str(i)) #print(full) return full_link if __name__ == &apos;__main__&apos;: urls = FullUrl() for url in urls: Download(url) 爬取图片的工作就完成了，粗略的计算6600张，每一张下载需要5秒钟，一分钟60秒、一小时60分钟，天呐，需要9个小时才能爬取全部的图片。想一想还是算了吧，整站爬取还是要使用Scrapy+mongodb。 &gt;&gt;&gt; 165*40 6600 &gt;&gt;&gt; from __future__ import division &gt;&gt;&gt; 6600*5/60/60 9.166666666666666 下载了700多张，108M，也算是留着看看吧。 一会上传到Github上 三、结语昨天学习了崔庆才老师的爬虫，感觉真的学习到了好多，对Python爬虫提高很有帮助，还有，原来他就是静觅，刚开始学习爬虫就在看他的博客，没想到他现在又在出爬虫教程，打算跟着学习。 分享内容： 1. 分析知乎Ajax请求及爬取逻辑 2. 用Scrapy实现递归爬取 3. 爬取结果存储到MongoDB 废话不多说，自己看看就知道了。 静觅丨崔庆才的个人博客http://cuiqingcai.com/ 微课录播 | 03月17日 爬取知乎所有用户详细信息https://edu.hellobi.com/course/163 最后的小广告 有朋友竟然叫我去作一期直播，讲一讲Python。 打算根据自己的经历分享一些经验，主要是关于Python入门的，想听听可以私信我。 时间是周二晚9点~ Hello World！ Try to be a Pythoner！","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zhangslob.github.io/tags/爬虫/"},{"name":"图片下载","slug":"图片下载","permalink":"https://zhangslob.github.io/tags/图片下载/"}]},{"title":"Python练习第九题，爬取贴吧图片","date":"2017-03-14T11:56:07.000Z","path":"2017/03/14/Python练习第九题，爬取贴吧图片/","text":"这是崔斯特的第四篇原创文章 一、问题：用 Python爬取妹子图片 :) http://tieba.baidu.com/p/2166231880 二、分析贴吧网页源码打开网页http://tieba.baidu.com/p/2166231880，F12 发现图片链接都在&lt;img&gt;标签中 &lt;cc&gt; &lt;div...&gt; &lt;img...&gt; &lt;img...&gt; 测试发现，src中的链接就是图片链接。那么就很简单，只需要把&lt;img&gt;中的src的链接拿出来即可。 三、写代码环境：Python3，Pycharm 使用requests和xpath，最近才学了xpath，发现超级好用，比bs4简洁，有兴趣看看这个https://zhuanlan.zhihu.com/p/25572729 import requests from lxml import etree url = &apos;http://tieba.baidu.com/p/2166231880&apos; header = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36&apos;} r = requests.get(url,headers=header).content s = etree.HTML(r) print(s.xpath(&apos;//div/img/@src&apos;)) 发现链接都已经拿到手，下一步就是下载了~ 下载图片的语句： import urllib.request path = &apos;......&apos; #下载链接 jpg_link ＝ &apos;......&apos; #图片链接 request.urlretrieve(jpg_link, path) 加在一起，大功告成。 四、总结经测试，贴吧里面其他网页如：http://tieba.baidu.com/p/1165861759 本代码都可以下载，顺便说一说问题。 1、图片名称使用图片链接中的名称，包含大量数字和字母，可以优化。 2、可以看到，下载文件中包含了一个表情，查看那是用户所发，说明筛选出了问题。 3、帖子数量多，翻页过后，需要在代码中加入获取下一页链接。 除此之外，还有什么问题呢？ 源码请见：https://github.com/zhangslob/TiebaImg","tags":[{"name":"福利","slug":"福利","permalink":"https://zhangslob.github.io/tags/福利/"},{"name":"美女图片","slug":"美女图片","permalink":"https://zhangslob.github.io/tags/美女图片/"}]},{"title":"Python练习第七题，我要倒过来看","date":"2017-03-06T10:47:46.000Z","path":"2017/03/06/Python练习第七题，我要倒过来看/","text":"这是崔斯特的第三篇原创文章 一、ChallengeUsing the Python language, have the function FirstReverse(str) take the str parameter being passed and return the string in reversed（颠倒的） order. For example: if the input string is “Hello World and Coders” then your program should return the string sredoC dna dlroW olleH.题目意思是，给定字符串，返回原来的倒序。例如给出的是“Hello World and Coders”，返回“sredoC dna dlroW olleH.” Sample Test Cases Input:”coderbyte” Output:”etybredoc” Input:”I Love Code” Output:”edoC evoL I” Hint Think of how you can loop through a string or array of characters backwards to produce a new string. def FirstReverse(str): # code goes here return str # keep this function call here print FirstReverse(raw_input()) 二、解法:切片环境：Python3.5 A simple way to reverse a string would be to create a new string and fill it with the characters from the original string, but backwards. To do this, we need to loop through the original string starting from the end, and every iteration of the loop we move to the previous character in the string. Here is an example: def FirstReverse(str): return str[::-1] print (FirstReverse(input())) 非常简洁str[::-1]就可以完成目标。 三、切片详解1、取字符串中第几个字符 &gt;&gt;&gt; &apos;hello&apos;[0]#表示输出字符串中第一个字符 &apos;h&apos; &gt;&gt;&gt; &apos;hello&apos;[-1]#表示输出字符串中最后一个字符 &apos;o&apos; 2、字符串分割 &gt;&gt;&gt; &apos;hello&apos;[1:3] &apos;el&apos; 第一个参数表示原来字符串中的下表第二个参数表示分割后剩下的字符串的第一个字符 在 原来字符串中的下标 注意，Python从0开始计数 3、几种特殊情况 &gt;&gt;&gt; &apos;hello&apos;[:3]#从第一个字符开始截取，直到最后 &apos;hel&apos; &gt;&gt;&gt; &apos;hello&apos;[0:]#从第一个字符开始截取，截取到最后 &apos;hello&apos; &gt;&gt;&gt; &apos;hello&apos;[:] &apos;hello&apos; 4、步长截取 &gt;&gt;&gt; &apos;abcde&apos;[::2]#表示从第一个字符开始截取，间隔2个字符取一个。 &apos;ace&apos; &gt;&gt;&gt; &apos;abcde&apos;[::-2] &apos;eca&apos; &gt;&gt;&gt; &apos;abcde&apos;[::-1] &apos;edcba&apos; 推荐阅读： 官方文档https://docs.python.org/3/tutorial/introduction.html#strings 廖雪峰的教程 http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431756919644a792ee4ead724ef7afab3f7f771b04f5000 更多解法： def FirstReverse(str): # reversed(str) turns the string into an iterator object (similar to an array) # and reverses the order of the characters # then we join it with an empty string producing a final string for us return &apos;&apos;.join(reversed(str)) print(FirstReverse(input())) 使用了什么语法？评论中见。","tags":[{"name":"Python语法","slug":"Python语法","permalink":"https://zhangslob.github.io/tags/Python语法/"},{"name":"切片介绍","slug":"切片介绍","permalink":"https://zhangslob.github.io/tags/切片介绍/"}]},{"title":"教你免费搭建个人博客，Hexo&Github","date":"2017-02-28T12:01:50.000Z","path":"2017/02/28/教你免费搭建个人博客，Hexo-Github/","text":"这是崔斯特的第二篇原创文章 说在前面： 为什么自己搭建博客，知乎不行吗？可以看看刘未鹏 | Mind Hacks，前些天发布了某篇文章应该是被人举报了，结果知乎就删了。有自己的博客自由，自在。更多请看为什么要自建博客？https://www.zhihu.com/question/19916345 说在前面： 为什么自己搭建博客，知乎不行吗？可以看看刘未鹏 | Mind Hacks，前些天发布了某篇文章应该是被人举报了，结果知乎就删了。有自己的博客自由，自在。更多请看为什么要自建博客？https://www.zhihu.com/question/19916345 我用了多久才完成博客的搭建？不瞒您说，我花了有3天时间。对着别人的“5分钟 搭建免费个人博客”花了3天才完成，中间遇到了无数困难。查看很多资料，所以当你遇到困难，别放弃，仔细看文档或资料。 为了发布这篇教程，重新注册了Github，崔斯特的博客https://zhangslob.github.io/这个是我自己的博客，崔斯特测试所用博客https://zhihuya.github.io/这个是我一边写这篇教程一边搭建的。所以如果你也和我一样，喜欢自由，喜欢捣腾，那就来吧。 系统：windows 7 64位，编辑器：sublime text3，控制台：cmder搭建博客使用hexo+Github 什么是 Hexo？https://hexo.io/zh-cn/docs/ Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 一、配置Github首先注册、登录 https://github.com/ 记住自己的Username（很重要） 然后右上角选择 Create a new repository https://github.com/new Repository name （填自己的名字） yourname.github.io(yourname与你的注册用户名一致,这个就是你博客的域名了) 例如，我的域名是github.com/zhihuya，就填入zhihuya.github.io。成功后出现下面的画面 二、环境安装（node、git）1、安装 Node.js https://nodejs.org/en/ 2、安装 Git https://github.com/waylau/git-for-win Git教程 https://github.com/waylau/git-for-win廖雪峰老师的教程，非常好。 3、安装完成后，在开始菜单里找到“Git”-&gt;“Git Bash”，名称和邮箱是Github上的 4、安装 Hexo。所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。 $ npm install -g hexo-cli （使用的cmder，超级好用~~。等待时间可能有点长） 好了到这一步我们环境全部安装好了。 三、设置在电脑F盘（自己随意）目录下新建文件夹 test，进入test，按住Shift键点击鼠标右键 因为我有安装Cmder，没有安装的点击“在此处打开命令窗口”，输入 hexo init blog 稍微等待下，速度有点慢。成功提示 INFO Start blogging with Hexo! 因为你初始化hexo 之后source目录下自带一篇hello world文章, 所以直接执行下方命令 $ hexo generate # 启动本地服务器 $ hexo server # 在浏览器输入 http://localhost:4000/就可以看见网页和模板了 INFO Start processing INFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 访问http://localhost:4000/，便可以看到网站初步的模样，不要激动，我们还要把网页发布到Github上去。 重新打开CMD，输入： ssh-keygen -t rsa -C &quot;Github的注册邮箱地址&quot; 一路Enter过来就好，得到信息： Your public key has been saved in /c/Users/user/.ssh/id_rsa.pub. 找到该文件，打开（sublime text），Ctrl + a复制里面的所有内容，然后进入Sign in to GitHub：https://github.com/settings/ssh New SSH key ——Title：blog —— Key：输入刚才复制的—— Add SSH key 四、配置博客在blog目录下，用sublime打开_config.yml文件，修改参数信息 特别提醒，在每个参数的：后都要加一个空格 修改网站相关信息 title: 崔斯特测试所用博客 subtitle: 副标题 description: 网页描述 author: 崔斯特 language: zh-CN timezone: Asia/Shanghai 配置部署（我的是zhihuya，修改成自己的） deploy: type: git repo: https://github.com/zhihuya/zhihuya.github.io.git branch: master 五、发表文章在CMD中输入 $ hexo new &quot;崔斯特测试文章&quot; INFO Created: F:\\test\\blog\\source\\_posts\\崔斯特测试文章.md 找到该文章，打开，使用Markdown语法，该语法介绍可以查看https://zhangslob.github.io/2017/02/26/%E5%88%A9%E7%94%A8HEXO%E6%90%AD%E5%BB%BA%E7%9A%84%E5%8D%9A%E5%AE%A2/ --- title: 崔斯特测试文章 date: 2017-02-28 13:03:44 tags: --- 这是一篇测试文章，欢迎关注作者博客[1]: https://zhangslob.github.io/ 保存，然后执行下列步骤： F:\\test\\blog $ hexo clean INFO Deleted database. INFO Deleted public folder. F:\\test\\blog $ hexo generate INFO Start processing INFO Files loaded in 1.48 s #省略 INFO 29 files generated in 4.27 s F:\\test\\blog $ hexo server INFO Start processing INFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 这个时候，打开http://localhost:4000/，发现刚才的文章已经成功了 最后一步，发布到网上，执行： F:\\test\\blog $ hexo deploy INFO Deploying: git INFO Clearing .deploy_git folder... INFO Copying files from public folder... #省略 其中会跳出Github登录，直接登录，如果没有问题输入zhihuya（换成你的）.github.io/ 崔斯特测试所用博客https://zhihuya.github.io/ 然后就可以看到已经发布了 六、总结发布文章的步骤： 1、hexo new 创建文章 2、Markdown语法编辑文章 3、部署（所有打开CMD都是在blog目录下） hexo clean #清除缓存 网页正常情况下可以忽略此条命令 hexo generate #生成 hexo server #启动服务预览，非必要，可本地浏览网页 hexo deploy #部署发布 简写Tips： hexo n “我的博客” == hexo new “我的博客” #新建文章 hexo p == hexo publish hexo g == hexo generate#生成 hexo s == hexo server #启动服务预览 hexo d == hexo deploy#部署 如果在执行 hexo deploy 后,出现 error deployer not found:github 的错误，执行： npm install hexo-deployer-git --save 出错是正常的，出错了自己先百度或google，实在不知道的可以询问我。 托管的话不仅有github可以用，还有个国内的https://coding.net/可选 到这里已经完成了博客的搭建，但是还有很多需要设置和调整的。这是我的博客，也许你会发现，我和刚才搭建的不一样，因为我修改了博客主题，简洁、优美。 主题介绍https://github.com/litten/hexo-theme-yilia 欢迎大家关注，定会有更多精彩 知乎https://www.zhihu.com/people/cuishite 博客https://zhangslob.github.io/","tags":[{"name":"个人博客","slug":"个人博客","permalink":"https://zhangslob.github.io/tags/个人博客/"},{"name":"Hexo&Github","slug":"Hexo-Github","permalink":"https://zhangslob.github.io/tags/Hexo-Github/"}]},{"title":"利用HEXO搭建的博客及Markdown语法介绍","date":"2017-02-26T09:20:57.000Z","path":"2017/02/26/利用HEXO搭建的博客/","text":"这是崔斯特的第一篇原创文章 Markdown的一些语法 花了不少时间。终于把自己的第一个博客搭建成功了。但是遇到新问题，发表文章需要使用Markdown语法，下面就来说说。 Markdown的文档介绍http://www.appinn.com/markdown/看看简单介绍就可以了，以后有需求再去学习。 1、标题和引用。Markdown 支持两种标题的语法，Setext 和 atx 形式。Setext 形式是用底线的形式，利用 = （最高阶标题）和 - （第二阶标题），Atx 形式在行首插入 1 到 6 个 # ，对应到标题 1 到 6 阶。 区块引用则使用 email 形式的 ‘&gt;’ 角括号。 如你所见，左边是编辑区，右边可以直接看到结果。编辑器是MarkdownPad，下载链接是http://markdownpad.com/download.html 2、修辞和强调。 3、链接和图片。（如果你有下载MarkdownPad这个编辑器，会更加方便哦） 4、加入代码,使用单引号。 这些应该可以帮助我完成一般书写，好的呢，第一篇文章就完成了。 欢迎评论。。 人生苦短，我学Python (๑• . •๑) 2017/2/27 19:11:11","tags":[{"name":"黑魔法","slug":"黑魔法","permalink":"https://zhangslob.github.io/tags/黑魔法/"},{"name":"个人博客","slug":"个人博客","permalink":"https://zhangslob.github.io/tags/个人博客/"},{"name":"Markdown语法","slug":"Markdown语法","permalink":"https://zhangslob.github.io/tags/Markdown语法/"}]}]